- en: Chapter 4. Administration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。管理
- en: 'In this chapter, we will see the following recipes related to MongoDB administration:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到以下与MongoDB管理相关的配方：
- en: Renaming a collection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重命名集合
- en: Viewing collection stats
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看集合统计信息
- en: Viewing database stats
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看数据库统计信息
- en: Manually padding a document
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动填充文档
- en: The mongostat and mongotop utilities
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mongostat和mongotop实用程序
- en: Getting current executing operations and killing them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取当前正在执行的操作并终止它们
- en: Using profiler to profile operations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分析器对操作进行分析
- en: Setting up users in Mongo
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中设置用户
- en: Interprocess security in Mongo
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mongo中的进程间安全性
- en: Modifying collection behavior using the collMod command
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用collMod命令修改集合行为
- en: Setting up MongoDB as a Windows service
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将MongoDB设置为Windows服务
- en: Replica set configurations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本集配置
- en: Stepping down as primary from the replica set
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从副本集中降级为主要
- en: Exploring the local database of a replica set
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索副本集的本地数据库
- en: Understanding and analyzing oplogs
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和分析oplogs
- en: Building tagged Replica sets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建带标签的副本集
- en: Configuring the default shard for non-sharded collections
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为非分片集合配置默认分片
- en: Manual split and migration of chunks
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动拆分和迁移块
- en: Domain-driven sharding using tags
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标签进行领域驱动分片
- en: Exploring the config database in a sharded setup
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分片设置中探索配置数据库
- en: Introduction
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter we will cover some of the tools and practices for administering
    MongoDB. The following recipes will help you collect statistics from your database,
    administer user access, analyze oplogs and look into some aspects of working with
    replica sets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些用于管理MongoDB的工具和实践。以下配方将帮助您从数据库中收集统计信息，管理用户访问权限，分析oplogs，并了解与副本集工作的一些方面。
- en: Renaming a collection
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重命名集合
- en: Have you ever come across a scenario where you have named a table in a relational
    database and at a later point of time felt that the name could have been better?
    Or perhaps the organization you work for was late in realizing that the table
    names are really getting messy and enforce some standards on the names? Relational
    databases do have some proprietary ways to rename the tables and a database admin
    would do that for you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否曾经遇到过这样的情况：在关系数据库中命名了一个表，后来觉得名字可能会更好？或者也许您所在的组织迟迟没有意识到表名真的很混乱，并对名称强制执行一些标准？关系数据库确实有一些专有的方法来重命名表，数据库管理员会为您做这件事。
- en: This raises a question though. In Mongo world, where collections are synonymous
    to tables, is there a way to rename a collection to some other name after it is
    created? In this recipe, we will explore this feature of Mongo where we rename
    an existing collection with some data in it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这也带来了一个问题。在Mongo世界中，集合等同于表，创建后是否有办法将集合重命名为其他名称？在这个配方中，我们将探索Mongo的这个特性，即在集合中有数据的情况下重命名现有集合。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We would need to run a MongoDB instance to perform this collection renaming
    experiment. Refer to the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* for information on how to start the server. The operations we will perform
    would be from mongo shell.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要运行一个MongoDB实例来执行这个集合重命名实验。参考[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*一节，了解如何启动服务器的信息。我们将要执行的操作将来自mongo
    shell。
- en: How to do it…
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Once the server is started and assuming it is listening for client connections
    on default port `27017`, execute the following command to connect to it from the
    shell:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务器启动，并假设它在默认端口`27017`上监听客户端连接，从shell执行以下命令连接到它：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once connected, using the default test database. Let''s create a collection
    with some test data. The collection we will use is named:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接后，使用默认的测试数据库。让我们创建一个带有一些测试数据的集合。我们将使用的集合名称是：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The test data will now be created (we may verify the data by querying the collection
    `sloppyNamedCollection`).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将创建测试数据（我们可以通过查询集合`sloppyNamedCollection`来验证数据）。
- en: 'Rename the collection `neatNamedCollection` as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集合`neatNamedCollection`重命名为：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Verify that the collection `sloppyNamedCollection` is no longer present by
    executing:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证`slappyNamedCollection`集合是否不再存在：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, query the `neatNamedCollection` collection to verify that the data
    originally in `sloppyNamedCollection` is indeed present in it. Simply execute
    the following on the mongo shell:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，查询`neatNamedCollection`集合，验证最初在`sloppyNamedCollection`中的数据确实存在其中。只需在mongo
    shell上执行以下命令：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works…
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Renaming a collection is pretty simple. It is accomplished with the `renameCollection`
    method, which takes two arguments. Generally, the function signature is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名集合非常简单。它是通过`renameCollection`方法实现的，该方法接受两个参数。通常，函数签名如下：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first argument is the name to which the collection is to be renamed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是要将集合重命名为的名称。
- en: 'The second parameter that we didn''t use is a Boolean value that tells the
    command whether to drop the target collection if it exists. This value defaults
    to false, which means do not drop the target but give an error. This is a sensible
    default, otherwise the results would be ghastly if we accidently gave a collection
    name that exists and didn''t wish to drop it. However, if you know what you are
    doing and want the target to be dropped while renaming the collection, pass the
    second parameter as true. The name of this parameter is `dropTarget`. In our case,
    the call would have been:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有使用的第二个参数是一个布尔值，告诉命令是否删除目标集合（如果存在）。这个值默认为false，这意味着不要删除目标，而是报错。这是一个明智的默认值，否则如果我们意外给出一个存在的集合名称并且不希望删除它，结果会很可怕。但是，如果你知道自己在做什么，并且希望在重命名集合时删除目标，将第二个参数传递为true。这个参数的名称是`dropTarget`。在我们的情况下，调用应该是：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As an exercise, try creating the `sloppyNamedCollection` again and rename it
    without the second parameter (or false as the value). You should see mongo complaining
    that the target namespace exists. Then, again rename with the second parameter
    as true, and now the renaming operation executes successfully.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试再次创建`sloppyNamedCollection`并将其重命名为没有第二个参数（或false作为值）。您应该看到mongo抱怨目标命名空间已存在。然后，再次使用第二个参数重命名为true，现在重命名操作执行成功。
- en: 'Note that the rename operation will keep the original and the newly renamed
    collection in the same database. This `renameCollection` method is not enough
    to move/rename the collection across another database. In such cases, we need
    to run the `renameCollection` command that looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重命名操作将保留原始的和新重命名的集合在同一个数据库中。这个`renameCollection`方法不足以将集合移动/重命名到另一个数据库。在这种情况下，我们需要运行类似于以下命令的`renameCollection`命令：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Suppose we want to rename the collection `sloppyNamedCollection` to `neatNamedCollection`
    as well as move it from `test` database to `newDatabase`, we can do so by executing
    the following command. Note the switch `dropTarget: true` used is meant to remove
    the existing target collection (`newDatabase.neatNamedCollection`) if it exists.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们想要将集合`sloppyNamedCollection`重命名为`neatNamedCollection`，并将其从`test`数据库移动到`newDatabase`，我们可以通过执行以下命令来执行此操作。请注意，使用的`dropTarget:
    true`开关旨在删除现有的目标集合（`newDatabase.neatNamedCollection`）（如果存在）。'
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Also, the rename collection operation doesn't work on sharded collections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，重命名集合操作不适用于分片集合。
- en: Viewing collection stats
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看集合统计信息
- en: Perhaps one of the interesting statistics from an administrative purpose when
    it comes to the usage of storage, the number of documents in collection possibly
    to estimate the future space, and memory requirements based on the growth of the
    data is to get a high level statistics of the collection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 也许在管理目的上，关于存储使用情况的一个有趣的统计数据是集合中文档的数量，可能可以根据数据的增长来估算未来的空间和内存需求，以获得集合的高级统计信息。
- en: Getting ready
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: To find the stats of the collection we need to have a server up and running
    and a single node is what should be okay. Refer to the *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找集合的统计信息，我们需要运行一个服务器，并且一个单节点应该是可以的。有关如何启动服务器的信息，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们将要操作的数据需要导入到数据库中。导入数据的步骤在[第2章](ch02.html
    "第2章。命令行操作和索引")的*创建测试数据*中给出。完成这些步骤后，我们就可以继续进行本教程了。
- en: How to do it…
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We would be using `postalCodes` collection for viewing the stats.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`postalCodes`集合来查看统计信息。
- en: 'Open the mongo shell and connect to the running MongoDB instance. In case you
    have started the mongo on default port, execute the following:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开mongo shell并连接到正在运行的MongoDB实例。如果您在默认端口上启动了mongo，请执行以下操作：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With the data imported, create an index on the `pincode` field if one doesn''t
    exist:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据后，如果`pincode`字段上不存在索引，则在该字段上创建一个索引：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'On the mongo terminal, execute the following:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在mongo终端上执行以下操作：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Observe the output and execute the following on the shell:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察输出并在shell上执行以下操作：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, observe the output.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次观察输出。
- en: We will now see what these values printed out mean to us in the following section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看这些打印出的值对我们意味着什么。
- en: How it works…
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: If we observe the output for both these commands, we see that the second one
    has all the figures in KB whereas the first one is in bytes. The parameter provided
    is known as scale and all the figures indicating size are divided by this scale.
    In this case, since we gave the value as `1024`, we get all the values in KB whereas
    if `1024 * 1024` is passed as the value of scale (the size shown will be in MB).
    For our analysis, we will use the one that shows the sizes in KB.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察这两个命令的输出，我们会发现第二个命令中的所有数字都是以KB为单位，而第一个命令中的数字是以字节为单位。提供的参数称为比例，所有指示大小的数字都会除以这个比例。在这种情况下，由于我们给出的值是`1024`，我们得到的所有值都是以KB为单位，而如果将`1024
    * 1024`作为比例的值（显示的大小将以MB为单位）。对于我们的分析，我们将使用以KB显示大小的值。
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following table shows the meaning of the important fields:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了重要字段的含义：
- en: '| Field | Description |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Field | Description |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `ns` | The fully qualified name of the collection with a format `<database>.<collection
    name>`. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `ns` | 以`<database>.<collection name>`格式的集合的完全限定名称。|'
- en: '| `count` | The number of documents in the collection. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `count` | 集合中的文档数量。|'
- en: '| `size` | The actual storage size occupied by the documents in the collection.
    Addition, deletion, and updates to documents in the collection can change this
    figure. The scale parameter affects this field''s value and in our case this value
    is in KB as `1024` is the scale. This number does include padding, if any. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `size` | 集合中文档占用的实际存储空间大小。对集合中文档的添加、删除和更新可能会改变此数字。比例参数会影响此字段的值，在我们的情况下，此值以KB为单位，因为`1024`是比例。此数字包括填充（如果有）。'
- en: '| `avgObjSize` | This is the average size of the document in the collection.
    It is simply the size field divided by the count of documents in the collection
    (the preceding two fields). The scale parameter affects this field''s value and
    in our case this value is in KB as `1024` is the scale. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `avgObjSize` | 这是集合中文档的平均大小。它只是大小字段除以集合中文档的计数（前两个字段）。比例参数会影响此字段的值，在我们的情况下，此值以KB为单位，因为`1024`是比例。|'
- en: '| `storageSize` | Mongo preallocates the space on the disk to ensure that the
    documents in the collection are kept on continuous locations to provide better
    performance in disk access. This preallocation fills up the files with zeros and
    then starts allocating space to these documents inserted. This field tells the
    size on the storage used by this collection. This figure will generally be much
    more than the actual size of the collection. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `storageSize` | Mongo在磁盘上预先分配空间，以确保集合中的文档保持在连续的位置，以提供更好的磁盘访问性能。这种预分配会用零填充文件，然后开始为插入的文档分配空间。该字段告诉此集合使用的存储空间大小。这个数字通常会比集合的实际大小大得多。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `numExtents` | As we saw, mongo pre allocates continuous disk space to the
    collections for performance purpose. However as the collection grows, new space
    needs to be allocated. This field gives the number of such continuous chunk allocation.
    This continuous chunk is called an extent. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `numExtents` | 正如我们所看到的，Mongo为了性能目的而预先分配了连续的磁盘空间给集合。然而，随着集合的增长，需要分配新的空间。该字段给出了这种连续块分配的数量。这个连续的块称为一个区段。
    |'
- en: '| `nindexes` | This field gives the number of indexes present on the collection.
    This value would be `1` even if we do not create an index on the collection as
    mongo implicitly creates an index on the field `_id`. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `nindexes` | 该字段给出了集合上存在的索引的数量。即使我们没有在集合上创建索引，该值也将为`1`，因为Mongo会在字段`_id`上隐式创建一个索引。
    |'
- en: '| `lastExtentSize` | The size of the last extent allocated. The scale parameter
    affects this field''s value and in our case this value is in KB as `1024` is the
    scale. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `lastExtentSize` | 分配的最后一个区段的大小。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `paddingFactor` | This parameter has been deprecated since version 3.0.0
    and is hardcoded to `1` for backward compatibility reasons. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `paddingFactor` | 自3.0.0版本起，此参数已被弃用，并且由于向后兼容性原因已硬编码为`1`。 |'
- en: '| `totalIndexSize` | Indexes take up space to store too. This field gives the
    total size taken up by the indexes on the disk. The scale parameter affects this
    field''s value and in our case this value is in KB as `1024` is the scale. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `totalIndexSize` | 索引也占用存储空间。该字段给出了磁盘上索引占用的总大小。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。
    |'
- en: '| `indexSizes` | This field is a document with the key as the name of the index
    and value as the size of the index in question. In our case, we had created an
    index explicitly on the `pincode` field; thus, we see the name of the index as
    the key and the size of the index on disk as the value. The total of these values
    of all the index is same as the value given previously, `totalIndexSize`. The
    scale parameter affects this field''s value and in our case this value is in KB
    as `1024` is the scale. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `indexSizes` | 该字段是一个文档，其键是索引的名称，值是所讨论的索引的大小。在我们的情况下，我们在`pincode`字段上显式创建了一个索引；因此，我们看到索引的名称作为键，磁盘上索引的大小作为值。所有这些索引的值的总和与先前给出的值`totalIndexSize`相同。比例参数影响此字段的值，在我们的情况下，此值以KB为单位，因为比例为`1024`。|'
- en: Documents are placed on the storage device in continuous locations. If a document
    is updated, resulting in an increase in size, Mongo will have to relocate this
    document. This operation turns out to be expensive affecting the performance of
    such update operations. Starting with Mongo 3.0.0, two data allocation strategies
    are used. One is *The power of 2*, where documents are allocated space in power
    of 2 (for example, 32, 64, 128, and so on). The other is *No Padding*, where collections
    do not expect document sizes to be altered.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 文档被放置在存储设备上的连续位置。如果文档被更新，导致大小增加，Mongo将不得不重新定位这个文档。这个操作会变得昂贵，影响这样的更新操作的性能。从Mongo
    3.0.0开始，使用了两种数据分配策略。一种是*2的幂*，其中文档以2的幂分配空间（例如，32、64、128等）。另一种是*无填充*，其中集合不希望文档大小被改变。
- en: See also
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: In this recipe, we discussed viewing stats of a collection. See the next recipe
    to view the stats at a database level.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们讨论了查看集合的统计信息。查看下一个配方以在数据库级别查看统计信息。
- en: Viewing database stats
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看数据库统计信息
- en: In the previous recipe, we saw how to view some important statistics of a collection
    from an administrative perspective. In this recipe, we get an even higher picture,
    getting those (or most of those) statistics at the database level.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个配方中，我们看到了如何从管理角度查看集合的一些重要统计信息。在这个配方中，我们得到了一个更高的视角，获得了数据库级别的这些（或大部分）统计信息。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To find the stats of the database, we need to have a server up and running and
    a single node is what should be okay. Refer to the recipe *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for information on how to start the server.
    The data on which we would be operating needs to be imported in the database.
    The steps to import the data are given in the recipe *Creating Test Data* in [Chapter
    2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*. Once these steps are completed, we are all set to go
    ahead with this recipe. Refer to the previous recipe if you need to see how to
    view stats at the collection level.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找数据库的统计信息，我们需要运行一个服务器，一个单节点应该是可以的。有关如何启动服务器的信息，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*。我们将要操作的数据需要导入到数据库中。有关如何导入数据的步骤，请参阅[第2章](ch02.html
    "第2章。命令行操作和索引")中的配方*创建测试数据*，*命令行操作和索引*。完成这些步骤后，我们就可以继续进行这个配方了。如果需要查看如何在集合级别查看统计信息，请参阅上一个配方。
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We will use the `test` database for the purpose of this recipe. It already has
    a `postalCodes` collection in it.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`test`数据库来完成此配方的目的。它已经在其中有一个`postalCodes`集合。
- en: Connect to the server using the mongo shell by typing in the following command
    from the operating system terminal. It is assumed that the server is listening
    to port `27017`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用mongo shell连接到服务器，通过在操作系统终端中输入以下命令。假设服务器正在监听端口`27017`。
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'On the shell, execute the following command and observe the output:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shell上，执行以下命令并观察输出：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the shell, again execute the following but this time around we add the scale
    parameter. Observe the output:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shell上，再次执行以下命令，但这次我们添加了scale参数。观察输出：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works…
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `scale` parameter, which is a parameter to the `stats` function, divides
    the number of bytes with the given scale value. In this case, it is `1024` and
    hence all the values will be in KB. We analyze the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale`参数是`stats`函数的一个参数，它将字节数除以给定的scale值。在这种情况下，它是`1024`，因此所有值将以KB为单位。我们分析以下输出：'
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table shows the meaning of the important fields:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了重要字段的含义：
- en: '| Field | Description |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 描述 |'
- en: '| --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `db` | This is the name of the database whose stats are being viewed. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `db` | 这是正在查看统计信息的数据库的名称。 |'
- en: '| `collections` | This is the total number of collections in the database.
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `collections` | 这是数据库中集合的总数。 |'
- en: '| `objects` | This is the count of documents across all collections in the
    database. If we find the stats of a collection using `db.<collection>.stats()`,
    we get the count of documents in the collection. This attribute is the sum of
    counts of all the collections in the database. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `objects` | 这是数据库中所有集合中文档的计数。如果我们使用`db.<collection>.stats()`查找集合的统计信息，我们会得到集合中文档的计数。这个属性是数据库中所有集合计数的总和。
    |'
- en: '| `avgObjectSize` | This is simply the size in bytes of all the objects in
    all the collections in the database divided by the count of the documents across
    all the collections. This value is not affected by the scale provided, although
    this is a `size` field. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `avgObjectSize` | 这只是数据库中所有集合中所有对象的字节大小除以所有集合中文档的计数。这个值不受提供的scale影响，尽管这是一个`size`字段。
    |'
- en: '| `dataSize` | This is the total size of the data held across all the collections
    in the database. This value is affected by the scale provided. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `dataSize` | 这是数据库中所有集合中保存的数据的总大小。这个值受提供的scale影响。 |'
- en: '| `storageSize` | This is the total amount of storage allocated to collections
    in this database for storing documents. This value is affected by the scale provided.
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `storageSize` | 这是为存储文档而分配给该数据库中集合的总存储量。这个值受提供的scale影响。 |'
- en: '| `numExtents` | This is the count of all the number of extents in the database
    across all the collections. This is basically the number of extents (logical containers)
    in the collection stats for collections in this database. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `numExtents` | 这是数据库中所有集合的extent数量的总数。这基本上是该数据库中集合统计信息中extent（逻辑容器）的数量。 |'
- en: '| `indexes` | This is the sum of number of indexes across all collections in
    the database |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `indexes` | 这是数据库中所有集合的索引数量的总和。 |'
- en: '| `indexSize` | This is the size in bytes for all the indexes of all the collections
    in the database. This value is affected by the scale provided. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `indexSize` | 这是数据库中所有集合的所有索引的字节大小。这个值受提供的scale影响。 |'
- en: '| `fileSize` | This is a sum of the size of all the database files you should
    find on the filesystem for this database. The files would be named `test.0`, `test.1`,
    and so on for test database. This value is affected by the scale provided. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `fileSize` | 这是应该在文件系统中找到的该数据库的所有数据库文件的大小总和。文件的名称将是`test.0`，`test.1`等等。这个值受提供的scale影响。
    |'
- en: '| `nsSizeMB` | This is the size of the file in MB for the `.ns` file of the
    database. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `nsSizeMB` | 这是数据库的`.ns`文件的大小（以MB为单位）。 |'
- en: '| `extentFreeList.num` | This is the number of free extends in freelist. You
    can look at extent as an internal data structure of MongoDB. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `extentFreeList.num` | 这是空闲列表中空闲extent的数量。你可以将extent看作是MongoDB的内部数据结构。 |'
- en: '| `extentFreeList.totalSize` | Size of the extents on the freelist. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `extentFreeList.totalSize` | 空闲列表上extent的大小。 |'
- en: For more information on these, you can refer to books such as *Instant MongoDB*
    by *Packt Publishing* ([http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant](http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，你可以参考《Instant MongoDB》这样的书籍，由*Packt Publishing*出版（[http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant](http://www.packtpub.com/big-data-and-business-inteliigence/instant-mongodb-instant)）。
- en: How it works…
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Let's start by looking at the `collections` field. If you look carefully at
    the number and execute the show collections command on the mongo shell, you will
    find one extra collection in the stats as compared to those by executing the command.
    The difference is for one collection, which is hidden. Its name is `system.namespaces`
    collection. You may do a `db.system.namespaces.find()` to view its contents.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`collections`字段开始。如果你仔细观察数字，并在mongo shell上执行`show collections`命令，你会发现与执行命令时相比，统计信息中多了一个隐藏的集合。这个差异是因为有一个隐藏的集合，它的名称是`system.namespaces`。你可以执行`db.system.namespaces.find()`来查看它的内容。
- en: Getting back to the output of stats operation on the database, the objects field
    in the result has an interesting value too. If we find the count of documents
    in the `postalCodes` collection, we see it is `39732`. The count shown here is
    `39738`, which means there are six more documents. These six documents come from
    the `system.namespaces` and `system.indexes` collection. Executing a count query
    on these two collections will confirm it. Note that the `test` database doesn't
    contain any other collection apart from `postalCodes`. The figures would change
    if the database contains more collections with documents in it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回到数据库上的统计操作的输出，结果中的对象字段也有一个有趣的值。如果我们在`postalCodes`集合中找到文档的数量，我们会发现它是`39732`。这里显示的数量是`39738`，这意味着还有六个文档。这六个文档来自`system.namespaces`和`system.indexes`集合。在这两个集合上执行计数查询将予以确认。请注意，`test`数据库除了`postalCodes`之外不包含任何其他集合。如果数据库包含更多包含文档的集合，这些数字将会改变。
- en: Another thing to note is the value of the `avgObjectSize` and there is something
    weird in this value. Unlike this very field in the collection's stats, which is
    affected by the value of the scale provided, in database stats this value is always
    in bytes. This is pretty confusing and I am not really sure why this is not scaled
    according to the provided scale.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意`avgObjectSize`的值，这个值有点奇怪。与集合统计信息中的这个字段不同，该字段受所提供的比例值的影响，在数据库统计信息中，该值始终以字节为单位。这很令人困惑，我不太确定为什么这个值不根据提供的比例进行缩放。
- en: Manually padding a document
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动填充文档
- en: Without getting too much into the internals of the storage, MongoDB uses memory
    mapped files, which means that the data is stored in files exactly as how it would
    be in memory and it would use the low level OS services to map these pages to
    memory. The documents are stored in continuous locations in mongo data files and
    problem arises when the document grows and no longer fits in the space. In such
    scenarios, mongo rewrites the document towards the end of the collection with
    the updated data and clearing up the space where it was originally placed (note
    that this space is not released to OS as free space).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在不深入存储的内部细节的情况下，MongoDB使用内存映射文件，这意味着数据存储在文件中，就像存储在内存中一样，并且它会使用低级别的操作系统服务将这些页面映射到内存中。文档存储在mongo数据文件中的连续位置，当文档增长并且不再适合空间时会出现问题。在这种情况下，mongo会将文档重写到集合的末尾，并清理原来放置的空间（请注意，这个空间不会作为空闲空间释放给操作系统）。
- en: This is not a big problem for applications that don't expect the documents to
    grow in size. However, this is a big performance hit for those who foresee this
    growth in the document size over a period of time and potentially a lot of such
    document movements. With the release of MongoDB 3.0, the *Power of 2* method became
    the default size allocation strategy. As the name suggests, this method stores
    documents in space allocated in powers of 2\. This provides additional padding
    to the documents as well as better reuse of free space caused by relocation or
    deletion of documents.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不希望文档增长的应用程序来说，这不是一个大问题。然而，对于那些预期文档在一段时间内增长并且可能有很多这样的文档移动的人来说，这是一个很大的性能损失。随着MongoDB
    3.0的发布，*Power of 2*方法成为了默认的大小分配策略。顾名思义，这种方法将文档存储在以2的幂分配的空间中。这不仅为文档提供了额外的填充，还更好地重用了由于文档的重定位或删除而导致的空闲空间。
- en: That said, if you still wish to introduce manual padding in your strategy, read
    on.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果你仍然希望在你的策略中引入手动填充，继续阅读。
- en: Getting ready
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Nothing is needed for this recipe unless you plan to try out this simple technique,
    in which case you would need a single instance up and running. Refer to the recipe
    *Installing single node MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start the server.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱不需要任何东西，除非你打算尝试这个简单的技术，如果是这样，你需要一个正在运行的单个实例。有关如何启动服务器的信息，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的食谱*安装单节点MongoDB*，*安装和启动服务器*。
- en: How to do it…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: The idea of this technique is to add some dummy data to the document to be inserted.
    This dummy data's size in addition to other data in the document is approximately
    same as the anticipated size of the document.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的想法是向要插入的文档添加一些虚拟数据。这些虚拟数据的大小加上文档中的其他数据大致等于文档的预期大小。
- en: For example, if the average size of the document is estimated to be around 1200
    bytes over a period of time and there is 300 bytes of data present in the document
    while inserting it, we will add a dummy field of size around 900 bytes so that
    the total document size sums up to 1200 bytes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果文件的平均大小在一段时间内估计为1200字节，而在插入文件时存在300字节的数据，我们将添加一个大小约为900字节的虚拟字段，以使总文件大小达到1200字节。
- en: Once the document is inserted, we unset this dummy field, which leaves a hole
    in the file between the two consecutive documents. This empty space would then
    be used when the document grows over a period of time minimizing the document
    movements. The empty space may also be used by another document. The more foolproof
    way is to remove the padding only when you are using the space. However, any document
    growing beyond the anticipated average growth will have to be copied by the server
    to the end of the collection. Needless to say, documents not growing to the anticipated
    size will tend to waste disk space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦插入文档，我们取消这个虚拟字段，这样在两个连续文档之间留下一个空隙。当文档随着时间的推移增长时，这个空白空间将被使用，最大限度地减少文档的移动。这个空白空间也可能被另一个文档使用。更加可靠的方法是只有在使用空间时才删除填充。然而，任何超出预期平均增长的文档都将被服务器复制到集合的末尾。不用说，没有达到预期大小的文档将倾向于浪费磁盘空间。
- en: The applications can come up with some intelligent strategy to perhaps the adjust
    the size of the padding field based on say some particular field of the document
    to take care of these shortcomings but that is something up to the application
    developers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以提出一些智能策略，也许根据文档的某个特定字段调整填充字段的大小，以解决这些缺陷，但这取决于应用程序开发人员。
- en: 'Let''s now see a sample of this approach:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下这种方法的示例：
- en: 'We define a small function that will add a field called `padField` with an
    array of string values to the document. Its code is as follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个小函数，它将向文档添加一个名为`padField`的字段，并将字符串值的数组添加到文档中。其代码如下：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It will add an array called `padField` and add 20 times a string called `Dummy`.
    There is no restriction on what type you add to the document and how many times
    it is added as long as it consumes the space you desire. The preceding code is
    just a sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它将添加一个名为`padField`的数组，并添加20次名为`Dummy`的字符串。对于您添加到文档中的类型和添加的次数没有限制，只要它占用您所需的空间。上述代码只是一个示例。
- en: 'The next step is to insert a document. We will define another function called
    `insert` to do that:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是插入一个文档。我们将定义另一个名为`insert`的函数来执行：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will now put this into action by inserting a document in the collection
    `testCol` as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将通过在集合`testCol`中插入一个文档来将其付诸实践：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You may query the `testCol` using the following query and check if the document
    inserted exists or not:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下查询查询`testCol`，并检查插入的文档是否存在：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that on querying you would not find the `padField` in it. However, the
    space once occupied by the array stays between the subsequently inserted documents
    even if the field was unset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在查询时，您将在其中找不到`padField`。但是，即使未设置该字段，数组占用的空间仍将保留在随后插入的文档之间。
- en: How it works…
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The `insert` function is self-explanatory and has comments in it to tell what
    it does. An obvious question is how do we believe if this indeed what we intent
    to do. For this purpose, we shall do a small activity as follow. We will work
    on a `manualPadTest` collection for this purpose. From the mongo shell, execute
    the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`insert`函数是不言自明的，并且其中有注释来说明它的作用。一个明显的问题是，我们如何相信这确实是我们打算做的事情。为此，我们将进行一个小活动如下。我们将在`manualPadTest`集合上进行这个目的。从mongo
    shell执行以下操作：'
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Take a note of the `avgObjSize` field in the stats. Next, execute the following
    from the mongo shell:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计信息中注意`avgObjSize`字段。接下来，从mongo shell执行以下操作：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Take a note of the `avgObjSize` field in the stats. This figure is much larger
    than the one we saw earlier with a regular insert without padding. The `paddingFactor`
    as we see in both cases still is one, but the latter case has more buffer for
    the document to grow.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计信息中注意`avgObjSize`字段。这个数字比我们之前看到的普通插入的数字要大得多。`paddingFactor`在这两种情况下仍然是1，但后一种情况为文档提供了更多的缓冲区。
- en: One catch in the `insert` function we used in this recipe is that the insert
    into the collection and the update document operations are not atomic.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中我们使用的`insert`函数中，插入到集合和更新文档操作不是原子的。
- en: The mongostat and mongotop utilities
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mongostat和mongotop实用程序
- en: Most of you might find these names similar to two popular Unix commands, `iostat`
    and `top`. For MongoDB, `mongostat` and `mongotop` are two utilities which does
    pretty much the same job as these two Unix commands do and there is no prize for
    guessing that these are used to monitor the mongo instance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人可能会发现这些名称与两个流行的Unix命令`iostat`和`top`相似。对于MongoDB，`mongostat`和`mongotop`是两个实用程序，它们的工作与这两个Unix命令几乎相同，毫无疑问，它们用于监视mongo实例。
- en: Getting ready
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we would be simulating some operations on a standalone mongo
    instance by running a script that would attempt to keep your server busy, and
    then in another terminal we will run these utilities to monitor the `db` instance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将通过运行一个脚本来模拟独立mongo实例上的一些操作，该脚本将尝试使您的服务器保持繁忙，然后在另一个终端中，我们将运行这些实用程序来监视`db`实例。
- en: You need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to download the script `KeepServerBusy.js`
    from Packt site and keep it handy for execution on local drive. Also, it is assumed
    that the `bin` directory of your mongo installation is present in the path variable
    of your operating system. If not, then these commands need to be executed with
    the absolute path of the executable from the shell. These two utilities `mongostat`
    and `mongotop` comes standard with the mongo installation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要启动一个独立的服务器来监听任何端口以进行客户端连接；在这种情况下，我们将坚持使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要从Packt网站下载脚本`KeepServerBusy.js`并将其放在本地驱动器上以备执行。还假定您的mongo安装的`bin`目录存在于操作系统的路径变量中。如果没有，那么这些命令需要在shell中使用可执行文件的绝对路径来执行。这两个实用程序`mongostat`和`mongotop`是与mongo安装一起提供的标准工具。
- en: How to do it…
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Start the MongoDB server, and let it listen to the default port for connections.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动MongoDB服务器，并让它监听默认端口以进行连接。
- en: 'In a separate terminal, execute the provided JavaScript `KeepServerBusy.js`
    as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端中，执行提供的JavaScript `KeepServerBusy.js`如下：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Open a new OS terminal and execute the following command:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的操作系统终端并执行以下命令：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获一段时间的输出内容，然后按下*Ctrl* + *C*停止命令捕获更多的统计信息。保持终端打开或将统计信息复制到另一个文件中。
- en: 'Now, execute the following command from the terminal:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从终端执行以下命令：
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Capture the output content for some time and then hit *Ctrl* + *C* to stop the
    command from capturing more stats. Keep the terminal open or copy the stats to
    another file.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获输出内容一段时间，然后按*Ctrl* + *C*停止命令捕获更多统计信息。保持终端打开或将统计信息复制到另一个文件中。
- en: Hit *Ctrl* + *C* in the shell where the provided JavaScript `KeepServerBusy.js`
    was executed to stop the operation that keeps the server busy.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行提供的JavaScript `KeepServerBusy.js`的shell中按*Ctrl* + *C*停止使服务器保持繁忙的操作。
- en: How it works…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Let's see what we have captured from these two utilities.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们从这两个实用程序中捕获到了什么。
- en: 'We start by analyzing `mongostat`. On my laptop, the capture using `mongostat`
    looks like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先分析`mongostat`。在我的笔记本电脑上，使用`mongostat`进行捕获如下：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You may choose to look at what the script `KeepServerBusy.js` is doing to keep
    the server busy. All it does is insert 1000 documents in collection `monitoringTest`,
    then update them one by one to set a new key in it, executes a find and iterates
    through all of them, and finally deletes them one by one and is basically a write
    intensive operation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择查看脚本`KeepServerBusy.js`是如何使服务器保持繁忙的。它所做的就是在`monitoringTest`集合中插入1000个文档，然后逐个更新它们以设置一个新的键，执行查找并遍历所有文档，最后逐个删除它们，基本上是一个写入密集型操作。
- en: The output does look ugly with content wrapping, but let's analyze the fields
    one by one and see what the fields to keep an eye on.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来很丑陋，但让我们逐个分析字段，看看需要关注的字段。
- en: '| Column(s) | Description |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 列 | 描述 |'
- en: '| --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `insert`, `query`, `update`, `delete` | The first four columns are the number
    of `insert`, `query`, `update` and `delete` operation per second. It is per second
    as the time frame these figures are captured are separated by one second, which
    is indicated by the last column. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `insert`，`query`，`update`，`delete` | 前四列是每秒`insert`，`query`，`update`和`delete`操作的次数。这是每秒的，因为捕获这些数字的时间间隔相隔一秒，这由最后一列表示。'
- en: '| `getmore` | When the cursor runs out of data for the query, it executes a
    `getmore` operation on the server to get more results for the query executed earlier.
    This column shows the number of `getmore` operations executed in this given time
    frame of 1 second. In our case, there are not many `getmore` operations that are
    executed. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `getmore` | 当游标对查询的数据用尽时，它会在服务器上执行`getmore`操作，以获取之前执行的查询的更多结果。此列显示在此给定的1秒时间范围内执行的`getmore`操作的次数。在我们的情况下，并没有执行太多`getmore`操作。'
- en: '| `commands` | This is the number of commands executed on the server in the
    given time frame of 1 second. In our case, it wasn''t much and was only one. The
    number after a `&#124;` is `0` in our case, as this was in standalone mode. Try
    executing `mongostat` connecting to a replica set primary and secondary. You should
    see slightly different figures there. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `commands` | 这是在给定的1秒时间范围内在服务器上执行的命令数量。在我们的情况下，并不多，只有一个。在我们的情况下，`&#124;`后面的数字是`0`，因为这是独立模式。尝试连接到副本集主服务器和次服务器执行`mongostat`。你应该在那里看到稍微不同的数字。
    |'
- en: '| `flushes` | This is the number of times data was flushed to disk in the interval
    of 1 second. (`fsync` in case of `MMAPv1` storage engine, and checkpoints triggered
    between polling interval in case of `WiredTiger` storage engine) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `flushes` | 这是在1秒间隔内将数据刷新到磁盘的次数。（在`MMAPv1`存储引擎的情况下是`fsync`，在`WiredTiger`存储引擎的情况下是在轮询间隔之间触发的检查点）
    |'
- en: '| `mapped`, `virtual`, and `resident memory` | Mapped memory is the amount
    of memory mapped by the Mongo process to the database. This will typically be
    same as the size of the database. Virtual memory on other hand is the memory allocated
    to the entire `mongod` process. This will be more than twice the size of mapped
    memory especially when journaling is enabled. Finally, resident memory is the
    actual of physical memory used by mongo. All these figures are given in MB. The
    total amount of physical memory might be a lot more than what is being used by
    Mongo, but that is really not a concern unless a lot of page faults occur (which
    does happen in the previously mentioned output). |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `mapped`，`virtual`和`resident memory` | 映射内存是Mongo进程映射到数据库的内存量。这通常与数据库的大小相同。另一方面，虚拟内存是分配给整个`mongod`进程的内存。当启用日志记录时，这将是映射内存大小的两倍以上。最后，常驻内存是Mongo实际使用的物理内存。所有这些数字以MB为单位给出。物理内存的总量可能比Mongo使用的内存多得多，但除非发生大量页面错误（在先前提到的输出中确实会发生），否则这并不是一个问题。'
- en: '| `faults` | These are the number of page faults occurring per second. These
    numbers should be as less as possible. It indicates the number of times mongo
    had to go to disk to obtain the document/index that was missing in the main memory.
    This problem is not as big a problem when using SSD for persistent storage as
    it is when using spinning disk drives. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `faults` | 这些是每秒发生的页面错误次数。这些数字应尽可能少。它表示Mongo需要多少次去磁盘获取在主内存中缺失的文档/索引。当使用SSD作为持久存储时，这个问题不像使用旋转磁盘驱动器时那么严重。'
- en: '| `locked` | Since version 2.2, all write operations to a collection lock the
    database in which the collection is and does not acquire a global level lock.
    This field shows the database that was locked for a majority of the time in the
    given time interval. In our case, the `test` database is locked for a majority
    of time. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| `locked` | 自2.2版本以来，对集合的所有写操作都会锁定包含该集合的数据库，并且不会获取全局级别的锁。此字段显示在给定的时间间隔内大部分时间被锁定的数据库。在我们的情况下，`test`数据库大部分时间被锁定。'
- en: '| `idx miss %` | This field gives the number of times a particular index was
    needed and was not present in memory. This causes a page fault and the disk needs
    to be accessed to get the index. Another disk access might be needed to get the
    document as well. This figure too should be low. A high percentage of index miss
    is something that would need attention. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `idx miss %` | 此字段给出了需要特定索引但在内存中不存在的次数。这会导致页面错误，并且需要访问磁盘以获取索引。可能还需要另一次磁盘访问以获取文档。这个数字也应该很低。高百分比的索引缺失是需要关注的问题。'
- en: '| `qr` &#124; `qw` | These are the queued up reads and writes that are waiting
    for getting a chance to be executed. If this number goes up, it shows that the
    database is getting overwhelmed by the volume of read and writes than it could
    handle. If the values are too high, keep an eye on page faults and database lock
    percents in order to get more insights on increased queue counts. If the data
    set is too large, sharding the collection can improve the performance significantly.
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `qr` &#124; `qw` | 这些是等待执行的读取和写入的排队数。如果这个数字增加，表明数据库受到了读取和写入量的压倒。如果值太高，要密切关注页面错误和数据库锁定百分比，以便更深入地了解排队计数的增加。如果数据集太大，分片集合可以显著提高性能。
    |'
- en: '| `ar` &#124; `aw` | This is the number of active readers and writers (clients).
    Not something to worry of even for a large number as far as other stats we saw
    previously are under control. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `ar` &#124; `aw` | 这是活动读者和写者（客户端）的数量。只要其他我们之前看到的统计数据都在控制之下，即使数量很大，也不用担心。
    |'
- en: '| `netIn` and `netOut` | The network traffic in and out of the mongo server
    in the given time frame. Figure is measured in bits. For example, 271k means 271
    kilobits. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `netIn`和`netOut` | 在给定时间范围内，mongo服务器的网络流量进出。数字以位为单位。例如，271k表示271千位。 |'
- en: '| `conn` | This indicates the number of open connections. Something to keep
    a watch on to see if this doesn''t keep getting higher. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `conn` | 这表示打开的连接数。要密切关注，看看是否会不断增加。 |'
- en: '| `time` | This is the time interval when this sample was captured. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `time` | 这是捕获此样本时的时间间隔。 |'
- en: There are some more fields seen if `mongostat` is connected to a replica set
    primary or secondary. As an assignment, once the stats or a standalone instance
    are collected, start a replica set server and execute the same script to keep
    the server busy. Use `mongostat` to connect to a primary and secondary instance
    and see different stats captured.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`mongostat`连接到副本集的主服务器或从服务器，会看到一些更多的字段。作为一个任务，一旦收集到统计数据或独立实例，启动一个副本集服务器并执行相同的脚本以使服务器保持繁忙。使用`mongostat`连接到主服务器和从服务器实例，并查看不同的统计数据。
- en: 'Apart from `mongostat`, we also used the `mongotop` utility to capture the
    stats. Let''s see its output and make some sense out of it:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`mongostat`，我们还使用了`mongotop`实用程序来捕获统计数据。让我们看看它的输出并理解一些：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There is not much to see in this stat. We see the total time a database was
    busy reading or writing in the given slice of 1 second. The value given in the
    total would be sum of the read and the write time. If we actually compare the
    `mongotop` and `mongostat` for the same time slice, the percentage of time duration
    for which the write was taking place would be very close to the figure given in
    the percentage time that the database was locked in the `mongostat` output.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个统计数据中没有太多可看的。我们看到数据库在给定的1秒时间片段内忙于读取或写入的总时间。总时间中给定的值将是读取和写入时间的总和。如果我们实际上比较相同时间片段的`mongotop`和`mongostat`，那么写入正在进行的时间所占的百分比将非常接近`mongostat`输出中数据库被锁定的百分比。
- en: 'The command `mongotop` accepts a parameter on the command line as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongotop`命令接受命令行上的参数，如下所示：'
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this case, the interval after which the stats will be printed out will be
    5 seconds as opposed to the default value of 1 second.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，打印统计数据的时间间隔将是5秒，而不是默认值1秒。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Starting with MongoDB 3.0, both `mongotop` and `mongostat` utilities allow output
    in JSON format using `--json` option. This can be very useful if you were to use
    custom monitoring or metrics collection scripts, which would rely on these utilities.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.0开始，`mongotop`和`mongostat`实用程序都允许使用`--json`选项以JSON格式输出。如果您要使用自定义监视或度量收集脚本，这可能非常有用，这些脚本将依赖于这些实用程序。
- en: See also
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: In the recipe *Getting current executing operations and killing them*, we will
    see how to get the current executing operations from the shell and kill them if
    needed
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*获取当前执行操作并终止它们*的示例中，我们将看到如何从shell获取当前执行的操作，并在需要时终止它们。
- en: In the recipe *Using profiler to profile operations*, we will see how to use
    the inbuilt profiling feature of Mongo to log operation execution times.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*使用分析器来分析操作*的示例中，我们将看到如何使用Mongo的内置分析功能来记录操作执行时间。
- en: Getting current executing operations and killing them
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取当前执行操作并终止它们
- en: In this recipe, we will see how to view the current running operations and kill
    some operations that are running for a long time.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到如何查看当前运行的操作，并终止一些长时间运行的操作。
- en: Getting ready
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will simulate some operations on a standalone mongo instance. We need to
    start a standalone server listening to any port for client connections; in this
    case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start two shells connected to the server started. One
    shell would be used for background index creation and another would be used to
    monitor the current operation and then kill it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在独立的mongo实例上模拟一些操作。我们需要启动一个独立服务器，以便监听任何端口以进行客户端连接；在这种情况下，我们将使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要启动两个连接到已启动服务器的shell。一个shell将用于后台索引创建，另一个将用于监视当前操作，然后终止它。
- en: How to do it…
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We would not be able to simulate the actual long running operation in our test
    environment. We will try to create an index and hope it takes long to create.
    Depending on your target hardware configuration, the operation may take some time.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法在测试环境中模拟实际长时间运行的操作。我们将尝试创建一个索引，并希望它需要很长时间来创建。根据您的目标硬件配置，该操作可能需要一些时间。
- en: 'To start with this test, let''s execute the following on the mongo shell:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始这个测试，让我们在mongo shell上执行以下操作：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding insertion might take some time to insert 10 million documents.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的插入可能需要一些时间来插入1000万个文档。
- en: Once the documents are inserted, we will execute an operation that would create
    the index in background. If you would like to know more about index creation,
    refer to the recipe *Creating a background and foreground index in the shell*
    in [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"), *Command-line
    Operations and Indexes*, but it is not a prerequisite for this recipe.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦文档被插入，我们将执行一个操作，该操作将在后台创建索引。如果您想了解更多关于索引创建的信息，请参考[第2章](ch02.html "第2章。命令行操作和索引")中的*在shell中创建后台和前台索引*，但这不是本教程的先决条件。
- en: Create a background index on the field `i` in the document. This index creation
    operation is what we will be viewing from the `currentOp` operation and is what
    we will attempt to kill from using the kill operation. Execute the following in
    one shell to initiate the background index creation operation. This takes fairly
    long time and on my laptop it took well over 100 seconds.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文档中的字段`i`上创建一个后台索引。这个索引创建操作是我们将从`currentOp`操作中查看的，也是我们将尝试使用终止操作来终止的操作。在一个shell中执行以下操作来启动后台索引创建操作。这需要相当长的时间，在我的笔记本电脑上花了100多秒。
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the second shell, execute the following command to get the current executing
    operations:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二个shell中，执行以下命令以获取当前正在执行的操作：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Take a note of the progress of the operations and find the one that is necessary
    for index creation. In our case, it was the only in progress on test machine.
    It will be an operation on `system.indexes` and the operation will be insert.
    The keys to lookout for in the output document are `ns` and `op`, respectively.
    We need to note the first field of this operation, `opid`. In this case, it is
    `11587458`. The sample output of the command is given in next section.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意操作的进度，并找到必要的索引创建操作。在我们的情况下，这是测试机器上唯一正在进行的操作。它将是一个在`system.indexes`上的操作，操作将是插入。在输出文档中要注意的关键是`ns`和`op`。我们需要注意这个操作的第一个字段，`opid`。在这种情况下，它是`11587458`。命令的示例输出在下一节中给出。
- en: 'Kill the operation from the shell using the following command, using the `opid`
    (operation ID) we got earlier:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从shell中终止操作，使用我们之前得到的`opid`（操作ID）：
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works…
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We will split our explanation into two sections, the first about the current
    operation details and second about killing the operation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的解释分成两部分，第一部分是关于当前操作的详细信息，第二部分是关于终止操作。
- en: In our case, index creation process is the long-running operation that we intend
    to kill. We create a big collection with about 10 million documents and initiate
    a background index creation process.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，索引创建过程是我们打算终止的长时间运行的操作。我们创建了一个大约有1000万个文档的大集合，并启动了一个后台索引创建过程。
- en: 'On executing the `db.currentOp()` operation, we get a document as the result
    with a field `inprog` whose value is an array of other documents each representing
    a currently running operation. It is common to get a big list of documents on
    a busy system. Here is a document taken for the index creation operation:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行`db.currentOp()`操作时，我们会得到一个文档作为结果，其中包含一个`inprog`字段，其值是另一个文档的数组，每个文档代表一个当前正在运行的操作。在繁忙的系统上通常会得到一个大型文档列表。这是一个用于索引创建操作的文档：
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will see what these fields mean in the following table:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下表中看到这些字段的含义：
- en: '| Field | Description |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: 字段 | 描述
- en: '| --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `opid` | This is a unique operation ID identifying the operation. This is
    the ID to be used to kill an operation. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: opid | 这是一个唯一的操作ID，用于标识操作。这是要用来终止操作的ID。
- en: '| `active` | The Boolean value indicating whether the operation has started
    or not, it is false only if it is waiting for acquiring the lock to execute the
    operation. The value will be true once it starts even if at a point of time where
    it has yielded the lock and is not executing. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `active` | 布尔值，指示操作是否已经开始，如果它正在等待获取锁来执行操作，则为false。一旦它开始，即使在某个时刻它已经释放了锁并且没有在执行，值也将为true。'
- en: '| `secs_running` | Gives the time in seconds the operation is executing for.
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `secs_running` | 给出操作执行的时间，单位为秒。'
- en: '| `op` | This is the type of the operation. In the case of index creation,
    it is inserted into a system collection of indexes. Possible values are `insert`,
    `query`, `getmore`, `update`, `remove`, and `command`. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| `op` | 这是操作的类型。在索引创建的情况下，它被插入到索引的系统集合中。可能的值包括`insert`，`query`，`getmore`，`update`，`remove`和`command`。'
- en: '| `ns` | This is the fully qualified namespace for the target. It would be
    in the form `<database name>.<collection name>`. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `ns` | 这是目标的完全限定命名空间。它将以`<数据库名称>.<集合名称>`的形式出现。'
- en: '| `insert` | This is the document that would be inserted in the collection.
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `insert` | 这是将插入到集合中的文档。'
- en: '| `query` | This is a field that would be present for other operations, other
    than `insert`, `getmore`, and `command`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: 查询 | 这是一个字段，除了`insert`，`getmore`和`command`之外的其他操作中都会出现。
- en: '| `client` | The ip address/hostname and the port of the client who initiated
    the operation. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `client` | 启动操作的客户端的IP地址/主机名和端口。'
- en: '| `desc` | This is the description of the client, mostly the client connection
    name. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `desc` | 这是客户端的描述，主要是客户端连接名称。'
- en: '| `connectionId` | This is the identifier of the client connection from which
    the request originated. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `connectionId` | 这是请求来源的客户端连接的标识符。'
- en: '| `locks` | This is a document containing the locks held for this operation.
    The document shows the type and mode of locks held for the operation being analyzed.
    The possible modes are as follows:**R** represents Shared (S) lock.**W** represents
    Exclusive (X) lock.**r** represents Intent Shared (IS) lock.**w** represents Intent
    Exclusive (IX) lock. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `locks` | 这是一个包含为此操作持有的锁的文档。该文档显示了用于分析的操作所持有的锁的类型和模式。可能的模式如下：**R**表示共享（S）锁。**W**表示排他（X）锁。**r**表示意向共享（IS）锁。**w**表示意向排他（IX）锁。'
- en: '| `waitingForLock` | This field indicates if the operation is waiting for a
    lock to be acquired. For instance, if the preceding index creation was not a background
    process, other operations on this database would queue up for the lock to be acquired.
    This flag for those operations would then be true. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `waitingForLock` | 此字段指示操作是否正在等待获取锁。例如，如果前面的索引创建不是后台进程，那么此数据库上的其他操作将排队等待获取锁。那些操作的标志将为true。'
- en: '| `msg` | This is a human-readable message for the operation. In this case,
    we do see the percentage of operation complete as this is an index creation operation.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `msg` | 这是操作的人类可读消息。在这种情况下，我们可以看到操作完成的百分比，因为这是一个索引创建操作。'
- en: '| `progress` | The state of the operation, the total gives the total number
    of documents in the collection and done gives the number indexed so far. In this
    case, the collection already had some more documents over 10 million documents.
    The percentage completion is computed from these figures. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `progress` | 操作的状态，total给出了集合中文档的总数，done给出了到目前为止已索引的数量。在这种情况下，集合已经有超过1000万个文档。完成百分比是从这些数字计算出来的。'
- en: '| `numYields` | This is the number of times the process has yielded the lock
    to allow other operations to execute. Since this is the background index creation
    process, this number will keep on increasing as the server yields it frequently
    to let other operations execute. Had it been a foreground process, the lock would
    never be yielded till the operation completes. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '`numYields` | 这是进程放弃锁的次数，以允许其他操作执行。由于这是后台索引创建过程，这个数字会不断增加，因为服务器经常放弃它，以便让其他操作执行。如果是前台进程，锁将一直保持到操作完成。'
- en: '| `lockStats` | This document has more nested documents giving the stats for
    the total time this operation has held the read or write lock and also the time
    it waited to acquire the lock. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `lockStats` | 这个文档有更多的嵌套文档，给出了此操作持有读取或写入锁的总时间，以及等待获取锁的时间。'
- en: Note
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In case you have a replica set, there would be more lot of getmore operations
    on the oplog on primary from secondary.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个副本集，主服务器上的oplog将有更多的getmore操作，来自从服务器。
- en: 'To see the system operations being executed too, we need to pass a true value
    as the parameter to the `currentOp` function call as follows:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看正在执行的系统操作，我们需要将true值作为参数传递给`currentOp`函数调用，如下所示：
- en: '[PRE35]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we will see how to kill the user initiated operation using the `killOp`
    function. The operation is simply called as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何使用`killOp`函数终止用户发起的操作。操作可以简单地如下所示：
- en: '[PRE36]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In our case, the index creation process had the process ID 11587458 and thus
    it will be killed as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，索引创建过程的进程ID为11587458，因此将如下终止它：
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On killing any operation, irrespective of whether the given operation ID exists
    or not, we see the following message on the console:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 无论给定的操作ID是否存在，终止任何操作，我们都会在控制台上看到以下消息：
- en: '[PRE38]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Thus, seeing this message doesn't mean that the operation was killed. It just
    means that the operation if it exists will be attempted to be killed.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，看到这条消息并不意味着操作已被终止。这只是意味着如果存在该操作，将尝试终止该操作。
- en: 'If some operation cannot be killed immediately and if the `killOp` command
    is issued for it, the field `killPending` in the `currentOp` will start appearing
    for the given operation. For example, execute the following query on the shell:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果某些操作无法立即终止，并且为其发出了`killOp`命令，则`currentOp`中的`killPending`字段将开始出现给定操作。例如，在shell上执行以下查询：
- en: '[PRE39]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will not return and the thread executing the query will sleep for 100 seconds.
    This is an operation that cannot be killed using `killOp`. Try executing the command
    `currentOp` from another shell (do not press *Tab* for auto completion, your shell
    may just hang), get the operation ID, and then kill it using the `killOp`. You
    should see that the process still would be running if you execute the `currentOp`
    command, but the document for the process details will now contain a new key `killPending`
    stating that the kill for this operation is requested but pending.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会返回，并且执行查询的线程将休眠100秒。这是一个无法使用`killOp`终止的操作。尝试从另一个shell执行`currentOp`命令（不要按*Tab*进行自动完成，否则您的shell可能会挂起），获取操作ID，然后使用`killOp`终止它。如果执行`currentOp`命令，您应该看到该进程仍在运行，但是进程详细信息的文档现在将包含一个新的`killPending`键，指出该操作的终止已被请求但是挂起。
- en: Using profiler to profile operations
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分析器来分析操作
- en: In this recipe, we will look at mongo's inbuilt profiler that would be used
    to profile the operations executed on the mongo server. It is a utility that is
    used to log all or slow operations that could be used for analysis of the performance
    of the server.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将看一下mongo内置的分析器，用于分析在mongo服务器上执行的操作。这是一个用于记录所有或慢操作的实用程序，可用于分析服务器的性能。
- en: Getting ready
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will perform some operations on a standalone mongo instance
    and profile them. We need to start a standalone server listening to any port for
    client connections; in this case, we will stick to the default `27017`. If you
    are not aware how to start a standalone server, refer to *Installing single node
    MongoDB* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*. We also need to start a shell that would
    be used to perform querying, enabling profiling, and viewing the profiling operation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将在独立的mongo实例上执行一些操作并对其进行分析。我们需要启动一个独立的服务器，以便监听任何端口以进行客户端连接；在这种情况下，我们将使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要启动一个shell，用于执行查询，启用分析和查看分析操作。
- en: How to do it…
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Once the server is started and the shell is connected to it, execute the following
    to get the current profiling level:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务器启动并且shell连接到它，执行以下内容以获取当前的分析级别：
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The default level should be `0` (no profiling, if we have not set it earlier).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们之前没有设置默认级别，那么默认级别应该是`0`（不进行分析）。
- en: 'Let''s set the profiling level to `1` (log slow operations only) and log all
    the operations slower than `50` ms. Execute the following on the shell:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将分析级别设置为`1`（仅记录慢操作），并记录所有慢于`50`毫秒的操作。在shell上执行以下操作：
- en: '[PRE41]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let''s execute an insert operation into a collection, and then execute
    a couple of queries:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们执行一个插入操作到一个收集中，然后执行一些查询：
- en: '[PRE42]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, execute the query on the following collection:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在以下收集上执行查询：
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works…
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Profiling is something that would not be enabled by default. If you are happy
    about the performance of the database, there is no reason one would enable the
    profiler. It is only when one feels there is some room for improvement and wants
    to target some expensive operations taking place. An important question is what
    classifies an operation to be slow? The answer is, it depends from application
    to application. In mongo, slow means any operation above 100 ms. However, while
    setting the profiling level, you may choose the threshold value.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 分析通常不会默认启用。如果您对数据库的性能感到满意，没有理由启用分析器。只有当有改进的空间并且想要针对一些昂贵的操作时才会启用。一个重要的问题是什么样的操作被分类为慢操作？答案是，这取决于应用程序。在mongo中，慢操作指的是任何超过100毫秒的操作。然而，在设置分析级别时，您可以选择阈值。
- en: 'There are three possible values for profiling levels:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种可能的分析级别：
- en: '`0`: Disable profiling'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`：禁用分析'
- en: '`1`: Enable profiling for slow operations, where the threshold value for an
    operation to be classified as slow is provided with the call while setting the
    profiling level'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`：启用慢操作的分析，调用时提供操作被分类为慢操作的阈值'
- en: '`2`: Profile all operations'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`：分析所有操作'
- en: While profiling all operations might not be a very good idea and might not be
    commonly used as we shall soon see, setting the value to `1` and a threshold provided
    to it is a good way to monitor slow operations.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分析所有操作可能不是一个很好的主意，也可能不常用，但将值设置为`1`并提供一个阈值是监视慢操作的好方法。
- en: If we look at the steps that we executed, we see that we can get the current
    profiling level by executing the operation `db.getProfilingLevel()`. To get more
    information, for example, what value is set as a threshold for the slow operations,
    we can use `db.getProfilingStatus()`. This returns a document with the profiling
    level and the threshold value for slow operations.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下我们执行的步骤，我们可以通过执行操作`db.getProfilingLevel()`来获取当前的分析级别。要获取更多信息，例如慢操作的阈值是多少，可以使用`db.getProfilingStatus()`。这将返回一个包含分析级别和慢操作阈值的文档。
- en: For setting the profiling level, we call the `db.setProfilingLevel()` method.
    In our case, we set it for logging all operations taking more than `50` ms as
    `db.setProfilingLevel(1, 50)`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置分析级别，我们调用`db.setProfilingLevel()`方法。在我们的情况下，我们设置为记录所有操作花费超过`50`毫秒的时间为`db.setProfilingLevel(1,
    50)`。
- en: To disable profiling, simply execute `db.setProfilingLevel(0)`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用分析，只需执行`db.setProfilingLevel(0)`。
- en: Next we executed three operations, one to insert a document, one to find all
    documents, and finally a find that calls sleep with a value of `70` ms to slow
    it down.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们执行了三个操作，一个是插入文档，一个是查找所有文档，最后一个是调用`sleep`并设置值为`70`毫秒以减慢速度的查找。
- en: The final step was to see these profiled operations that are logged in the `system.profile`
    collection. We execute a find to see the operations logged. For my execution,
    the insert and the final `find` operation with the sleep were logged.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是查看记录在`system.profile`收集中的这些被分析的操作。我们执行一个查找以查看记录的操作。对于我的执行，插入和最终的`find`操作与`sleep`一起被记录。
- en: 'Obviously, this profiling has some overhead but it is negligible. Hence, we
    would not enable it by default but only when we want to profile slow operations.
    Also, another question would be, *Will this profiling collection increase over
    a period of time?* The answer is *No*, as this is a capped collection. Capped
    collections are fixed size collections that preserve insertion orders and act
    as a circular queue filling in the new documents, discarding the oldest when it
    gets full. A query on `system.namespaces` should show the stats. The query execution
    would show the following for the `system.profile` collection:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种分析会带来一些开销，但可以忽略不计。因此，我们不会默认启用它，只有在我们想要分析慢操作时才会启用。另一个问题是，“这种分析收集会随时间增加吗？”答案是“不会”，因为这是一个有上限的收集。有上限的收集是固定大小的收集，保留插入顺序，并充当循环队列，在新文档填满时丢弃最旧的文档。对`system.namespaces`的查询应该显示统计信息。对`system.profile`收集的查询执行将显示以下内容：
- en: '[PRE44]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As we can see, the size of the collection is 1 MB, which is incredibly small.
    Setting the profiling level to `2` thus would easily overwrite the data on busy
    systems. One may also choose to explicitly create a collection with the name `system.profile`
    as a capped collection and of any size they prefer should they choose to retain
    more operations in it. To create a capped collection explicitly, you can execute
    the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这个收集的大小是1MB，非常小。因此，将分析级别设置为`2`会很容易覆盖繁忙系统上的数据。如果希望保留更多操作，也可以选择显式创建一个名为`system.profile`的有上限的收集，并设置任何所需的大小。要显式创建一个有上限的收集，可以执行以下操作：
- en: '[PRE45]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Obviously, the size chosen is arbitrary and you are free to allocate any size
    to this collection based on how frequently the data gets filled and how much of
    profiling data you want to keep before it gets overwritten.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所选择的大小是任意的，您可以根据数据填充的频率和希望在覆盖之前保留多少分析数据来分配任何大小给这个收集。
- en: As this is a capped collection and insertion order is preserved, a query with
    the `sort order {$natural:-1}` would be perfectly fine and very efficient to find
    the operations in the reverse order of the execution time.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个有上限的收集，并且保留了插入顺序，使用`sort order {$natural:-1}`的查询将非常适用且非常有效，可以按照执行时间的相反顺序找到操作。
- en: 'We would finally take a look at the document that got inserted in the `system.profile`
    collection and see what all operations it has logged:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终将查看插入到`system.profile`集合中的文档，并查看它记录了哪些操作：
- en: '[PRE46]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As we can see in the document, there are indeed some interesting stats. Let's
    look at some of them in the following table. Some of these fields are identical
    to the fields we see when we execute the `db.currentOp()` operation from the shell
    and we then discussed in the previous recipe.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在文档中所看到的，确实有一些有趣的统计数据。让我们在下表中看一些。其中一些字段与我们从shell执行`db.currentOp()`操作时看到的字段相同，并且我们在上一个示例中已经讨论过。
- en: '| Field | Description |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 描述 |'
- en: '| --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `op` | This is the operation that got executed; in this case, it was a find
    and thus it is query in this case. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `op` | 执行的操作；在这种情况下，是一个查找操作，因此在这种情况下是查询。 |'
- en: '| `ns` | This is the fully qualified name of the collection on which the operation
    was performed. It would be of the format `<database>.<collection name>`. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `ns` | 这是操作执行的集合的完全限定名称。它的格式将是`<数据库>.<集合名称>`。 |'
- en: '| `query` | It shows the query that got executed on the server. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| `query` | 显示在服务器上执行的查询。 |'
- en: '| `nscanned` | This has a similar meaning to explain plan. It is the total
    number of documents and index entries scanned. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| `nscanned` | 这与解释计划有相似的含义。它是扫描的文档和索引条目的总数。 |'
- en: '| `numYields` | This is the number of times the lock was yielded when the operation
    was executed. Higher yields could indicate that the query required a lot of disk
    access. This could be a good indication of re-looking at the index or optimizing
    the query itself. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| `numYields` | 操作执行时锁被放弃的次数。更高的放弃次数可能表明查询需要大量的磁盘访问。这可能是重新查看索引或优化查询本身的良好指标。
    |'
- en: '| `lockStats` | Some interesting stats for the time taken to acquire the lock
    and the time for which the lock was held. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| `lockStats` | 获取锁所花费的时间和持有锁的时间的一些有趣的统计数据。 |'
- en: '| `nreturned` | The number of documents returned. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| `nreturned` | 返回的文档数量。 |'
- en: '| `responseLength` | The length of the response in bytes. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| `responseLength` | 响应的长度（以字节为单位）。 |'
- en: '| `millis` | Most important of all, the time taken in milliseconds to execute
    the operation. This can be a good starting point to catch slow queries. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| `millis` | 最重要的是，执行操作所花费的毫秒数。这可以是捕捉慢查询的良好起点。 |'
- en: '| `ts` | This is the time when the operation was executed. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| `ts` | 这是操作执行的时间。 |'
- en: '| `client` | This is the hostname/IP address of the client who executed the
    operation. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| `client` | 执行操作的客户端的主机名/IP地址。 |'
- en: Setting up users in Mongo
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo中设置用户
- en: 'Security is one of the cornerstones of any enterprise-level system. Not always
    would you find a system in a completely safe and secure environment to allow unauthenticated
    user access to it. Apart from test environments, almost every production environment
    requires proper access rights and perhaps audit of the system access too. Mongo
    security has multiple aspects:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 安全是任何企业级系统的基石之一。并非总是可以在完全安全的环境中找到系统，以允许未经身份验证的用户访问它。除了测试环境外，几乎每个生产环境都需要适当的访问权限，也许还需要对系统访问进行审计。Mongo安全有多个方面：
- en: Access rights for the end users accessing the system. There would be multiple
    roles such as admin, read-only users, and read and write non-administrative users.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终用户访问系统的访问权限。将会有多个角色，如管理员、只读用户和读写非管理员用户。
- en: Authentication of the nodes that are added to the replica set. In a replica
    set, one should only be allowed to add authenticated systems. The integrity of
    the system would be compromised if any unauthenticated node is added to the replica
    set.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本集中添加的节点的身份验证。在副本集中，只允许添加经过身份验证的系统。如果向副本集添加未经身份验证的节点，系统的完整性将受到损害。
- en: Encryption of the data that is transmitted across the wire between the nodes
    of the replica sets or even the client and the server (or the mongos process in
    case of sharded setup).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加密在副本集的节点之间或甚至客户端和服务器（或分片设置中的mongos进程）之间传输的数据。
- en: In this and the next recipe, we would be looking at how to address the first
    and the second point given here. The third point of encrypting the data being
    transmitted on the wire is not supported by default by the community edition of
    mongo and would need a rebuild of mongo database with the `ssl` option enabled.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个和下一个示例中，我们将看看如何解决这里给出的第一和第二点。默认情况下，社区版的mongo不支持在传输的数据上加密，需要使用`ssl`选项重新构建mongo数据库。
- en: Getting ready
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will set up users for a standalone mongo instance. We need
    to start a standalone server listening to any port for client connections; in
    this case, we will stick to the default `27017`. If you are not aware how to start
    a standalone server, refer to *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*. We also need to start a shell that would be used for this admin operation.
    For a replica set, we will only be connected to a primary and perform these operations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将为独立的mongo实例设置用户。我们需要启动一个独立服务器，监听任何端口以进行客户端连接；在这种情况下，我们将使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要启动一个用于此管理操作的shell。对于副本集，我们只会连接到主服务器并执行这些操作。
- en: How to do it…
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We will add an admin user, a read-only user for a test database, and a read-write
    user for test database in this recipe.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将为测试数据库添加一个管理员用户、一个只读用户和一个读写用户。
- en: 'It is assumed that at this point:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，假设：
- en: The server is up and running, and we are connected to it from the shell.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器正在运行，并且我们从shell连接到它。
- en: The server is started without any special command-line argument other than those
    mentioned in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for *Starting a single node instance using
    command-line options* recipe. We thus have full access to the server for any user.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器在没有特殊命令行参数的情况下启动，除了[第1章](ch01.html "第1章。安装和启动服务器")中提到的那些，*安装和启动服务器*的*使用命令行选项启动单节点实例*配方。因此，我们对任何用户都有对服务器的完全访问权限。
- en: 'Perform the following steps:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: The first step we will do is to create an admin user. All the commands assume
    that you are using MongoDB 3.0 and above.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将要做的第一步是创建一个管理员用户。所有命令都假定您正在使用MongoDB 3.0及以上版本。
- en: 'First, we start by creating the admin user in admin database as follows:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从admin数据库开始创建管理员用户如下：
- en: '[PRE47]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We will add the `read_user` and `write_user` to test database. To add the users,
    execute the following from the mongo shell:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加`read_user`和`write_user`到测试数据库。要添加用户，请从mongo shell执行以下操作：
- en: '[PRE48]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now shut down the mongo server and the close the shell too. Restart the mongo
    server but with the `--auth` option on the command line:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在关闭mongo服务器并关闭shell。在命令行上重新启动mongo服务器，但使用`--auth`选项：
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: If your mongod instance is using `/etc/mongod.conf`, then add the line `auth
    = true` in the configuration file and restart the mongod service.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的mongod实例使用`/etc/mongod.conf`，则在配置文件中添加`auth = true`一行，并重新启动mongod服务。
- en: 'Now connect to the server from the newly opened mongo shell and execute the
    following:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从新打开的mongo shell连接到服务器并执行以下操作：
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The collection `testAuth` need not exist, but you should see an error that we
    are not authorized to query the collection.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`testAuth`集合不需要存在，但是您应该会看到一个错误，即我们未被授权查询该集合。'
- en: 'We will now log in from the shell using a `read_user` as follows:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用`read_user`从shell登录如下：
- en: '[PRE51]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will now execute the same `find` operation as follows. It should not give
    an error and it might not return any results depending on whether the collection
    exists or not:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将执行相同的`find`操作如下。它不应该出现错误，根据集合是否存在，可能不会返回任何结果：
- en: '[PRE52]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, we will try to insert a document as follows. We should get an error that
    you are not authorized to insert data in this collection.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将尝试插入一个文档如下。我们应该会收到一个错误，表示您未被授权在此集合中插入数据。
- en: '[PRE53]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We will now log out and log in again, but with a write user as follows. Note
    the difference in the way we login this time around as against the previous instance.
    We are providing a document as the parameter to the `auth` function, where as
    in previous case we passed two parameters for the username and password:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将注销并再次登录，但是使用write用户如下。请注意，这次我们登录的方式与以前不同。我们为`auth`函数提供了一个文档作为参数，而在以前的情况下，我们为用户名和密码传递了两个参数：
- en: '[PRE54]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, execute the following on the shell. You should get the unauthorized error:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在shell上执行以下操作。您应该会收到未经授权的错误：
- en: '[PRE55]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We will now switch to `admin` database. We are currently connected to the server
    using a `write_user` that has read-write permissions on the `test` database. From
    the mongo shell, try to do the following:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将切换到`admin`数据库。我们当前使用具有`test`数据库上读写权限的`write_user`连接到服务器。从mongo shell尝试执行以下操作：
- en: '[PRE56]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Close the mongo shell or open a new shell as follows from the operating system''s
    console. This should take us directly to admin database:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭mongo shell或从操作系统控制台打开一个新的shell如下。这应该会直接将我们带到admin数据库：
- en: '[PRE57]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now execute the following on the shell. It should show us the collections in
    the admin database:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在shell上执行以下操作。它应该会显示我们在admin数据库中的集合：
- en: '[PRE58]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Try and execute the following operation:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试并执行以下操作：
- en: '[PRE59]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: How it works…
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We executed a lot of steps and now we will take a closer look at them.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行了很多步骤，现在我们将仔细研究它们。
- en: 'Initially, the server is started without `--auth` option and hence no security
    is enforced by default. We create an admin user with the `db.createUser` method.
    The signature of the method to create the user is `createUser(user, writeConcern)`.
    The first parameter is the user, which actually is a JSON document and second
    is the write concern to use for user creation. The JSON document for the user
    has the following format:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，服务器在没有`--auth`选项的情况下启动，因此默认情况下不会强制执行任何安全性。我们使用`db.createUser`方法创建了一个具有`db.createUser`方法的管理员用户。创建用户的方法签名是`createUser(user,
    writeConcern)`。第一个参数是用户，实际上是一个JSON文档，第二个是用于用户创建的写关注。用户的JSON文档具有以下格式：
- en: '[PRE60]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The roles provided here can be provided as follows, assuming that the current
    database when the user is created is test on the shell:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的角色可以按如下方式提供，假设在创建用户时的当前数据库是shell上的测试：
- en: '[PRE61]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This gives the user being created read access to the reports `db` and `readWrite`
    access to the `test` database. Let''s see the complete user creation call of the
    `test` user:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建的用户对报告`db`具有读取访问权限，并对`test`数据库具有`readWrite`访问权限。让我们看看`test`用户的完整用户创建调用：
- en: '[PRE62]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The write concern, which is an optional parameter, can be provided as the JSON
    document. Some examples values are `{w:1}`, `{w:'majority'}`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 写关注，这是一个可选参数，可以作为JSON文档提供。一些示例值是`{w:1}`，`{w:'majority'}`。
- en: Coming back to the admin user creation, we created the user in step 2 using
    the `createUser` method and gave three inbuilt roles to this user in the `admin`
    database.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 回到管理员用户创建，我们在第2步中使用`createUser`方法创建了用户，并在`admin`数据库中为该用户提供了三个内置角色。
- en: In step 3, we created the `read` and `read-write` users in `test` database using
    the same `createUser` method.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们使用相同的`createUser`方法在`test`数据库中创建了`read`和`read-write`用户。
- en: We shut down the MongoDB server after the `admin`, `read`, and `read-write`
    user creation and restarted it with the `--auth` option.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在`admin`，`read`和`read-write`用户创建后关闭MongoDB服务器，并使用`--auth`选项重新启动它。
- en: On starting the server again, we will connect to it from the shell in step 8,
    but unauthenticated. Here, we try to execute a `find` query on a collection in
    test database, which fails as we are unauthenticated. This indicates that the
    server now requires appropriate credentials to execute operations on it. In step
    8 and 9, we log in using the `read_user` and first execute a `find` operation
    (which succeeds), and then an insert that doesn't as the user has read privileges
    only. The way to authenticate a user by invoking from the shell `db.auth(<user
    name>, <password>)` and `db.logout()`, which will logout the current logged in
    user.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动服务器后，我们将在第8步中从shell连接到它，但未经身份验证。在这里，我们尝试在test数据库中的集合上执行`find`查询，但由于我们未经身份验证，操作失败。这表明服务器现在需要适当的凭据才能执行操作。在第8和9步中，我们使用`read_user`登录，首先执行`find`操作（成功），然后执行一个插入操作（失败），因为用户只有读取权限。通过从shell调用`db.auth(<user
    name>, <password>)`和`db.logout()`来验证用户的方式，这将注销当前登录的用户。
- en: In steps 10 to 12, we demonstrate that we can perform `insert` operations using
    `write_user` but admin operations like `db.serverStatus()` cannot be executed.
    This is because these operations execute an `admin command` on the server, which
    a non-admin user and not permitted to invoke these. Similarly, when we change
    the database to admin, the `write_user`, which is from `test` database, is not
    permitted to perform any operations like getting a list of collections or any
    operation to query a collection in `admin` database.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤10到12中，我们演示了我们可以使用`write_user`执行`insert`操作，但是像`db.serverStatus()`这样的管理员操作无法执行。这是因为这些操作在服务器上执行`admin
    command`，非管理员用户不允许调用这些操作。同样，当我们将数据库更改为admin时，来自`test`数据库的`write_user`不被允许执行任何操作，比如获取集合列表或查询`admin`数据库中的集合。
- en: In Step 14, we log in to the shell using the `admin` user to the `admin` database.
    Previously, we logged in to database using the `auth` method; in this case, we
    used the `-u` and `-p` options for providing the username and the password. We
    also provided the name of the database to connect to, which is admin in this case.
    Here, we are able to view the collections on the admin database and also execute
    admin operations like getting the server status. Executing the `db.serverStatus`
    call is possible as the user is given the `clusterAdmin` role.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14步中，我们使用`admin`用户登录到`admin`数据库的shell中。之前，我们使用`auth`方法登录到数据库；在这种情况下，我们使用`-u`和`-p`选项来提供用户名和密码。我们还提供要连接的数据库的名称，在这种情况下是admin。在这里，我们能够查看admin数据库中的集合，并执行像获取服务器状态这样的管理员操作。执行`db.serverStatus`调用是可能的，因为用户被赋予了`clusterAdmin`角色。
- en: One final thing to note, apart from writing to a collection, a user with write
    privileges can also create indexes on the collection in which he has write access.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要注意的一点是，除了向集合写入数据之外，具有写入权限的用户还可以在具有写入访问权限的集合上创建索引。
- en: There's more…
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we saw how we can create different users and what permissions
    they have restricting some set of operations. In the following recipe, we will
    see how we can have authentication done at process level. That is, how can one
    mongo instance authenticate itself for being added to a replica set.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们看到了如何创建不同的用户以及他们具有的权限，限制了一些操作。在接下来的示例中，我们将看到如何在进程级别进行身份验证。也就是说，一个mongo实例如何对自己进行身份验证，以便被添加到副本集中。
- en: See also
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'MongoDB comes with a lot of built-in user roles with various privileges associated
    to each of them. Refer to the following URL to get details of various in built
    roles: [http://docs.mongodb.org/manual/reference/built-in-roles/](http://docs.mongodb.org/manual/reference/built-in-roles/).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB带有许多内置用户角色，每个角色都有各种权限。请参考以下网址以获取各种内置角色的详细信息：[http://docs.mongodb.org/manual/reference/built-in-roles/](http://docs.mongodb.org/manual/reference/built-in-roles/)。
- en: 'MongoDB also supports custom user roles. Refer to the following URL for knowing
    more about defining custom user roles: [http://docs.mongodb.org/manual/core/authorization/#user-defined-roles](http://docs.mongodb.org/manual/core/authorization/#user-defined-roles).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB还支持自定义用户角色。请参考以下网址了解如何定义自定义用户角色的更多信息：[http://docs.mongodb.org/manual/core/authorization/#user-defined-roles](http://docs.mongodb.org/manual/core/authorization/#user-defined-roles)。
- en: Interprocess security in Mongo
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mongo中的进程间安全性
- en: In the previous recipe, we saw how authentication can be enforced for user to
    be logged in before allowing any operations on Mongo. In this recipe, we will
    look at interprocess security. By the term interprocess security, we don't mean
    to encrypt the communication but only to ensure that the node being added to a
    replica set is authenticated before being added to the replica set.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们看到了如何强制用户在允许对Mongo进行任何操作之前登录进行身份验证。在这个示例中，我们将研究进程间安全性。通过进程间安全性，我们并不是指加密通信，而是确保在将节点添加到副本集之前对其进行身份验证。
- en: Getting ready
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will start multiple mongo instances as part of a replica
    set and thus you might have to refer to the recipe *Starting multiple instances
    as part of a replica set* from [Chapter 1](ch01.html "Chapter 1. Installing and
    Starting the Server"), *Installing and Starting the Server* if you are not aware
    of how to start a replica set. Apart from that, in this recipe, all we would be
    looking at how to generate key file to be used and the behavior when an unauthenticated
    node is added to the replica set.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将作为副本集的一部分启动多个mongo实例，因此您可能需要参考[第1章](ch01.html "第1章。安装和启动服务器")中的*作为副本集的一部分启动多个实例*这个示例，如果您不知道如何启动副本集。除此之外，在这个示例中，我们将看到如何生成用于使用的密钥文件以及在未经身份验证的节点被添加到副本集时的行为。
- en: How to do it…
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: To set the ground, we would be starting three instances, each listening to port
    `27000`, `27001`, and `27002`, respectively. The first two would be started by
    providing it a path to the key file and the third wouldn't be. Later, we will
    try to add these three instances to the same replica set.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 为了奠定基础，我们将启动三个实例，分别监听端口`27000`、`27001`和`27002`。前两个将通过提供密钥文件的路径来启动，第三个则不会。稍后，我们将尝试将这三个实例添加到同一个副本集中。
- en: 'Let''s generate key the key file first. There is nothing spectacular about
    generating the key file. This is as simple as having a file with 6 to 1024 characters
    from the `base64` character set. On Linux filesystem, you may choose to generate
    pseudo random bytes using `openssl` and encode them to `base64`. The following
    command will generate 500 random bytes and those bytes will then be `base64` encoded
    and written to the file `keyfile`:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先生成密钥文件。生成密钥文件并没有什么特别之处。这就像有一个包含来自`base64`字符集的6到1024个字符的文件一样简单。在Linux文件系统上，您可以选择使用`openssl`生成伪随机字节，并将其编码为`base64`。以下命令将生成500个随机字节，然后将这些字节编码为`base64`并写入文件`keyfile`：
- en: '[PRE63]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'On a Unix filesystem, the key file should not have permissions for world and
    group. Thus, we should do the following after it is created:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unix文件系统上，密钥文件不应该对世界和组有权限。因此，在创建后，我们应该执行以下操作：
- en: '[PRE64]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Not giving write permission to the creator ensures that we don''t overwrite
    the contents accidently. On Windows platform, however, `openssl` doesn''t come
    out of the box and thus you have to download it, the archive extracted, and the
    `bin` folder added to the OS path variable. For Windows, we can download it from
    the following URL: [http://gnuwin32.sourceforge.net/packages/openssl.htm](http://gnuwin32.sourceforge.net/packages/openssl.htm).'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不给创建者写权限可以确保我们不会意外地覆盖内容。然而，在Windows平台上，`openssl`并不是开箱即用的，因此您需要下载它，解压缩存档，并将`bin`文件夹添加到操作系统的路径变量中。对于Windows，我们可以从以下网址下载：[http://gnuwin32.sourceforge.net/packages/openssl.htm](http://gnuwin32.sourceforge.net/packages/openssl.htm)。
- en: 'You may even choose not to generate the key file using the approach mentioned
    here (using `openssl`) and can take an easy way out by just typing in plain text
    in the key file from any text editor or your choice. However, note that the characters
    `\r`, `\n`, and spaces are stripped off by mongo and the remainder text is considered
    as the key. For example, we may create a file with the following content added
    to the key file. Again, the file will be named `keyfile` with the following content:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您甚至可以选择不使用这里提到的方法（使用`openssl`）生成密钥文件，并且可以通过在任何文本编辑器或您选择的地方输入纯文本来简化。但是，请注意，mongo会剥离字符`\r`、`\n`和空格，并将剩余文本视为密钥。例如，我们可以创建一个文件，其中包含以下内容添加到密钥文件。同样，文件将被命名为`keyfile`，内容如下：
- en: '[PRE65]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Using any approach mentioned here, we must not have a `keyfile` in place that
    would be used for next steps of the recipe.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这里提到的任何方法，我们都不应该有一个`keyfile`，它将用于后续的步骤。
- en: We will now secure the mongo processes by starting the mongo instance as follows.
    I will start the following on windows, and my key file ID is named `keyfile` and
    is placed on `c:\MongoDB`. The data path is `c:\MongoDB\data\c1, c:\MongoDB\data\c2`,
    and `c:\MongoDB\data\c3` for the three instances, respectively.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将通过以下方式启动mongo进程来保护mongo进程。我将在Windows上启动以下内容，我的密钥文件ID命名为`keyfile`，放在`c:\MongoDB`上。数据路径分别为`c:\MongoDB\data\c1`、`c:\MongoDB\data\c2`和`c:\MongoDB\data\c3`。
- en: 'Start the first instance listening to port `27000` as follows:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动第一个实例，监听端口`27000`如下：
- en: '[PRE66]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Similarly, start the second server listening to port `27001` as follows:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，启动第二个服务器，监听端口`27001`如下：
- en: '[PRE67]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The third instance would be started but without the `--auth` and the `--keyFile`
    option listening to port `27002` as follows:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个实例将启动，但不带`--auth`和`--keyFile`选项，监听端口`27002`如下：
- en: '[PRE68]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We then start a mongo shell and connect it to port `27000`, which is the first
    instance started. From the mongo shell, we type:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们启动一个mongo shell，并连接到端口`27000`，这是第一个启动的实例。从mongo shell中，我们输入：
- en: '[PRE69]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'In few seconds, the replica set would be initiated with just one instance in
    it. We will now try to add two new instances to this replica set. First, add the
    one listening on port `27001` as follows (you will need to add the appropriate
    hostname, `Amol-PC` is the hostname in my case):'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几秒钟后，副本集将被初始化，只有一个实例在其中。现在我们将尝试向这个副本集添加两个新实例。首先，按照以下方式添加监听端口`27001`的实例（您需要添加适当的主机名，`Amol-PC`是我的主机名）：
- en: '[PRE70]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We will execute `rs.status()` command to see the status of our replica set.
    In the command's output, we should see our newly added instance.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将执行`rs.status()`命令来查看我们副本集的状态。在命令的输出中，我们应该看到我们新添加的实例。
- en: 'We will now finally try and add an instance that was started without the `--auth`
    and the `--keyFile` option as follows:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将尝试最终添加一个实例，该实例是在没有`--auth`和`--keyFile`选项的情况下启动的，如下所示：
- en: '[PRE71]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This should add the instance to the replica set, but using `rs.status()` will
    show the status of the instance as UNKNOWN. The server logs for the instance running
    on `27002` too should show some authentication errors.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该将实例添加到副本集中，但使用`rs.status()`将显示实例状态为UNKNOWN。运行在`27002`上的服务器日志也应该显示一些身份验证错误。
- en: 'We would finally have to restart this instance; however, this time we provide
    the `--auth` and the `--keyFile` option as follows:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须重新启动这个实例；然而，这一次我们提供`--auth`和`--keyFile`选项如下：
- en: '[PRE72]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Once the server is started, connect to it from the shell again and type in `rs.status()`
    in few moments, it should come up as a secondary instance.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务器启动，再次从shell连接到它，并在几分钟内输入`rs.status()`，它应该会显示为一个辅助实例。
- en: There's more…
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In this recipe, we saw interprocess security for preventing unauthenticated
    nodes from being added to the mongo replica set. We still haven't secured the
    transport by encrypting the data that is being sent over the wire. In the *Appendix*,
    we will show how to build the mongo server from the source and how to enable encryption
    of the contents over the wire.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们看到了用于防止未经身份验证的节点被添加到mongo副本集的进程间安全性。我们仍然没有通过加密在传输过程中发送的数据来保护传输。在*附录*中，我们将展示如何从源代码构建mongo服务器以及如何启用传输内容的加密。
- en: Modifying collection behavior using the collMod command
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`collMod`命令修改集合行为
- en: This is a command that would be executed to change the behavior of a collection
    in mongo. It could be thought of as a *collection modify* operation (officially,
    it is not mentioned anywhere though).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于更改mongo中集合行为的命令。它可以被认为是一个*集合修改*操作（尽管官方没有明确提到）。
- en: For a part of this recipe, knowledge of TTL indexes is required.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方的一部分，需要了解TTL索引的知识。
- en: Getting ready
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will execute the `collMod` operation on a collection. We
    need to start a standalone server listening to any port for client connections;
    in this case, we will stick to the default `27017`. If you are not aware how to
    start a standalone server, refer to *Installing single node MongoDB* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*. We also need to start a shell that would be used for this
    administration. It is highly recommended to take a look at the recipes *Expiring
    documents after a fixed interval using the TTL index* and *Expiring documents
    at a given time using the TTL index* in [Chapter 2](ch02.html "Chapter 2. Command-line
    Operations and Indexes"), *Command-line Operations and Indexes* if you are not
    aware of them.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将在一个集合上执行`collMod`操作。我们需要启动一个独立的服务器来监听任何端口以进行客户端连接；在这种情况下，我们将坚持使用默认的`27017`端口。如果您不知道如何启动独立服务器，请参考[第1章](ch01.html
    "第1章 安装和启动服务器")中的*安装单节点MongoDB*，*安装和启动服务器*。我们还需要启动一个用于此管理的shell。如果您不知道它们，强烈建议您查看[第2章](ch02.html
    "第2章 命令行操作和索引")中的*在固定间隔后使文档过期使用TTL索引*和*使用TTL索引在给定时间使文档过期*这两个配方。
- en: How it works…
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'This operation can be used to do a couple of things:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作可以用来做一些事情：
- en: 'Assuming we have a collection with TTL index, as we saw in [Chapter 2](ch02.html
    "Chapter 2. Command-line Operations and Indexes"), *Command-line Operations*,
    let us see the list indexes by executing the following:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个带有TTL索引的集合，就像我们在[第2章](ch02.html "第2章 命令行操作和索引")中看到的那样，让我们通过执行以下操作来查看索引列表：
- en: '[PRE73]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'To change the expiry to `800` ms from `300` ms, execute the following:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将到期时间从`300`毫秒更改为`800`毫秒，请执行以下操作：
- en: '[PRE74]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: How it works…
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'The `collMod` command always has the following format: `{collMod : <name of
    the collection>, <collmod operation>}`.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`collMod`命令始终具有以下格式：`{collMod：<集合名称>，<collmod操作>}`。'
- en: 'We use the index operation using `collMod` to modify the TTL index. If a TTL
    index is already created and the time to live needs to be changed after creation,
    we use the `collMod` command. This operation-specific field to the command is
    as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`collMod`进行索引操作来修改TTL索引。如果TTL索引已经创建，并且需要在创建后更改生存时间，我们使用`collMod`命令。该命令的操作特定字段如下：
- en: '[PRE75]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The `keyPattern` is the field, of the collection, on which the TTL index is
    created and the `expireAfterSeconds` will contain the new time to be changed to.
    On successful execution, we should see the following in the shell:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '`keyPattern`是创建TTL索引的集合上的字段，`expireAfterSeconds`将包含要更改的新时间。成功执行后，我们应该在shell中看到以下内容：'
- en: '[PRE76]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Setting up MongoDB as a windows service
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将MongoDB设置为Windows服务
- en: Windows services are long-running applications that run in background, similar
    to daemon threads. Databases are good candidates for such type of services, whereby
    they would start and stop when the host machines starts and stops (you may, however,
    choose to manually start/stop a service). Many database vendors provide a feature
    to start the database as a service when installed on the server. MongoDB lets
    you do that as well and this is what we will see in this recipe.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: Windows服务是在后台运行的长时间运行的应用程序，类似于守护线程。数据库是这种类型服务的良好候选者，它们会在主机启动和停止时启动和停止（但您也可以选择手动启动/停止服务）。许多数据库供应商在服务器上安装时提供了将数据库作为服务启动的功能。MongoDB也可以做到这一点，这就是我们将在这个配方中看到的。
- en: Getting ready
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Installing single node MongoDB with options from the config
    file* in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for getting information on how to start a
    MongoDB server using an external configuration file. Since mongo is run as a service
    in this case, it cannot be provided with command-like arguments and configuring
    it from configuration file is the only alternative. Refer to the prerequisites
    of the *Installing single node MongoDB* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, which is all
    we would need for this recipe.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[第1章](ch01.html "第1章 安装和启动服务器")中的配方*使用配置文件从配置文件安装单节点MongoDB*，获取有关如何使用外部配置文件启动MongoDB服务器的信息。由于在这种情况下mongo作为服务运行，因此无法提供类似命令的参数，并且从配置文件配置是唯一的选择。参考[第1章](ch01.html
    "第1章 安装和启动服务器")中*安装单节点MongoDB*配方的先决条件，这是我们这个配方所需要的一切。
- en: How to do it…
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will first create a config file with three configuration values the `port`,
    `dbpath`, and the `logpath` file. We name the file `mongo.conf` and keep it at
    location `c:\conf\mongo.conf` with the following three entries in it (you may
    choose any path for config file location, database and logs):'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将创建一个带有三个配置值`port`、`dbpath`和`logpath`文件的配置文件。我们将文件命名为`mongo.conf`，并将其保存在位置`c:\conf\mongo.conf`，其中包含以下三个条目（您可以选择任何路径作为配置文件位置、数据库和日志）：
- en: '[PRE77]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Execute the following from the windows terminal, which you may need to execute
    as an administrator. On Windows 7, the following steps were executed:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Windows终端执行以下操作，可能需要以管理员身份执行。在Windows 7中，执行了以下步骤：
- en: Press the Windows key on your keyboard.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在键盘上按Windows键。
- en: In the Search programs and files space, type `cmd`.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“搜索程序和文件”空间中，键入“cmd”。
- en: In the programs, the command prompt program will be seen; right-click on it
    and select **Run as administrator**.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在程序中，将看到命令提示符程序；右键单击它并选择**以管理员身份运行**。
- en: 'In the shell, execute the following:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shell中执行以下操作：
- en: '[PRE78]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The log printed out on the console should confirm that the service is installed
    properly.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台上打印的日志应该确认服务已正确安装。
- en: 'The service can be started as follows from the console:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过以下方式从控制台启动服务：
- en: '[PRE79]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The service can be stopped as follows:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过以下方式停止服务：
- en: '[PRE80]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Type in `services.msc` in the Run window (Windows button + *R*). In the management
    console, search for MongoDB service. We should see it as follows:![How to do it…](img/B04831_04_01.jpg)
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行窗口中键入`services.msc`（Windows键+*R*）。在管理控制台中，搜索MongoDB服务。我们应该看到它如下所示：![如何操作…](img/B04831_04_01.jpg)
- en: The service is automatic, that is, it will be started when the operating system
    starts. It can be changed to manual by right-clicking on it and selecting **Properties**.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该服务是自动的，也就是说，当操作系统启动时会启动它。可以通过右键单击它并选择**属性**来更改为手动。
- en: 'To remove a service, we need to execute the following from the command prompt:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要删除服务，需要从命令提示符执行以下操作：
- en: '[PRE81]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'There are more options available that can be used to configure the name of
    the service, display name, description, and the user account used to run the service.
    These can be provided as command-line arguments. Execute the following to see
    the possible options and take a look at the **Windows Service Control Manager**
    options:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有更多可用的选项，可用于配置服务的名称、显示名称、描述以及运行服务的用户帐户。这些可以作为命令行参数提供。执行以下操作以查看可能的选项，并查看**Windows服务控制管理器**选项：
- en: '[PRE82]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Replica set configurations
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集配置
- en: We have had a good discussion on what replica set is in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* in the recipe *Starting multiple instances as part of a replica set*,
    and we saw how to start a simple replica set. In the recipe *Interprocess security
    in Mongo* in this chapter, we saw how to start a replica set with interprocess
    authentication. To be honest, that is pretty much what we do in setting up a standard
    replica set. There are a few configurations that one must know and should be aware
    of how it affects the replica set's behavior. Note that we still are not discussing
    tag aware replication in this recipe and it would be taken up later in this chapter
    as a separate recipe *Building tagged replica sets*.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.html "第1章。安装和启动服务器")中对副本集进行了深入讨论，*安装和启动服务器*中的配方*作为副本集的一部分启动多个实例*，我们看到了如何启动一个简单的副本集。在本章的*Mongo中的进程间安全性*中，我们看到了如何启动具有进程间身份验证的副本集。老实说，这基本上就是我们在设置标准副本集时所做的。有一些配置是必须了解的，并且应该了解它们如何影响副本集的行为。请注意，我们在本配方中仍未讨论标签感知复制，并且它将在本章的另一个配方*构建带标签的副本集*中单独讨论。
- en: Getting ready
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[第1章](ch01.html "第1章。安装和启动服务器")中的*安装和启动服务器*中的配方*作为副本集的一部分启动多个实例*，了解先决条件并了解副本集的基础知识。按照配方中的说明，在计算机上设置一个简单的三节点副本集。
- en: Before we go ahead with the configurations, we will see what elections are in
    a replica set and how they work from a high level. This is good to know about
    elections because some of the configuration options affect the voting process
    in the elections.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行配置之前，我们将了解副本集中的选举是什么，以及它们在高层次上是如何工作的。了解选举是很有必要的，因为一些配置选项会影响选举中的投票过程。
- en: Elections in a replica set
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 副本集中的选举
- en: Mongo replica set has a single primary instance and multiple secondary instances.
    All database writes happen only through the primary instance and are replicated
    to the secondary instances. Read operations can happen from secondary instances
    depending on the read preference. Refer to the *Understanding ReadPreference for
    querying* in the [Appendix](apa.html "Appendix A. Concepts for Reference") to
    know what read preference is. If, however, the primary goes down or is not reachable
    for some reason, the replica set becomes unavailable for writes. MongoDB replica
    set has a feature to automatically failover to a secondary, by promoting it to
    a primary and make the set available to clients for both read and write operations.
    The replica set remains unavailable for that brief moment till a new primary comes
    up.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo副本集有一个主要实例和多个辅助实例。所有数据库写操作只发生在主要实例上，并且会被复制到辅助实例上。读操作可以根据读取偏好从辅助实例中进行。请参考[附录](apa.html
    "附录A.查询的概念")中的*了解查询的读取偏好*，了解读取偏好是什么。然而，如果主要实例宕机或由于某种原因无法访问，副本集将无法进行写操作。MongoDB副本集具有自动故障转移到辅助实例的功能，将其提升为主要实例，并使集合对客户端可用进行读写操作。在这一瞬间，副本集将暂时不可用，直到新的主要实例出现。
- en: It all sounds good but the question is, who decides upon who the new primary
    instance would be? The process of choosing a new primary happens through an election.
    Whenever any secondary detects that it cannot reach out to a primary, it asks
    all replica set nodes in the instance to elect itself as the new primary.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切听起来都很好，但问题是，谁决定新的主要实例是谁？选择新主要实例的过程是通过选举来进行的。每当任何辅助节点检测到无法联系主节点时，它会要求实例中的所有副本集节点选举自己为新的主节点。
- en: 'All other nodes in the replica set who receive this request for election of
    primary will perform certain checks before they vote a Yes to the secondary requesting
    an election:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 复制集中的所有其他节点在接收主节点选举请求之前将执行某些检查，然后才会对请求重新选举的次要节点投票赞成：
- en: They would first check if the existing primary is reachable. This is necessary
    because the secondary requesting the re-election is not able to reach the primary
    possibly because of a network partition in which case it should not be allowed
    to become a primary. In such case the instance receiving the request will vote
    a No.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，他们会检查现有的主节点是否可访问。这是必要的，因为请求重新选举的次要节点可能无法访问主节点，可能是因为网络分区，如果是这种情况，它不应该被允许成为主节点。在这种情况下，接收请求的实例将投票否定。
- en: Secondly, the instance would check the state of replication of itself with the
    secondary requesting the election. If it finds that the requesting secondary is
    behind itself in the replicated data, it would vote a No.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，实例将检查自身的复制状态与请求选举的次要节点的复制状态。如果发现请求的次要节点在复制数据方面落后于自己，它将投票否定。
- en: Finally, the primary is not reachable, but some instance with priority higher
    than the secondary requesting the re-election, is reachable from it. This is possible
    if the secondary requesting the re-election can't reach out to the secondary with
    higher priority possibly due to a network partition. In this scenario the instance
    receiving the request for election would vote a No.
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，主节点无法访问，但具有比请求重新选举的次要节点更高优先级的实例可以访问它。如果请求重新选举的次要节点无法访问具有更高优先级的次要节点，可能是由于网络分区，此时接收选举请求的实例将投票否定。
- en: The preceding checks are pretty much what would be happening (not necessarily
    in the order mentioned previously) during the re-election; if these checks pass,
    the instance votes a Yes.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的检查基本上是在重新选举期间会发生的事情（不一定按照之前提到的顺序），如果这些检查通过，实例就会投票赞成。
- en: The election is void even if a single instance votes No. However, if none of
    the instances have voted a No, then the secondary that requests the election would
    become a new primary if it receives a Yes from majority of instances. If the election
    becomes void, there would be a re-election with the same secondary or any other
    instance requesting an election with the aforementioned process till a new primary
    is elected.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有一个实例投票否定，选举也将无效。但是，如果没有一个实例投票否定，那么请求选举的次要节点将成为新的主节点，如果它从大多数实例那里得到了赞成。如果选举无效，将会进行重新选举，直到选出新的主节点为止，这个过程将与之前相同的次要节点或任何其他请求选举的实例进行。
- en: Now that we have an idea about the elections in replica set and the terminologies,
    let's look at some of the replica set configurations. Few of these options are
    related to votes and we start by looking at these options first.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对复制集中的选举和术语有了一些了解，让我们来看一些复制集配置。其中一些选项与投票有关，我们首先来看看这些选项。
- en: Basic configuration for a replica set
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复制集的基本配置
- en: 'From the first chapter when we set up a replica set, we had a configuration
    similar to the following one. The basic replica set configuration for a three
    member set is as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们设置复制集的第一章开始，我们的配置与以下类似。三个成员集的基本复制集配置如下：
- en: '[PRE83]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We would not be repeating the entire configuration in all the steps in the
    following sections. All the flags we would be mentioning would be added to the
    document of a particular member in the members array. In the preceding example,
    if node with `_id` as `2` is to be made arbiter, we would be having the following
    configuration for it in the configuration document shown previously:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在以下步骤中重复整个配置。我们将提到的所有标志都将添加到成员数组中特定成员的文档中。在上面的例子中，如果具有`_id`为`2`的节点要成为仲裁者，我们将在先前显示的配置文档中为其添加以下配置：
- en: '[PRE84]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Generally, the steps to reconfigure an existing replica set are as follows:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，重新配置现有复制集的步骤如下：
- en: Assign the configuration document to a variable. If the replica set is already
    configured, it can be obtained using the `rs.conf()` call from the shell.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将配置文档分配给一个变量。如果复制集已经配置，可以使用shell中的`rs.conf()`调用来获取它。
- en: '[PRE85]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The members field in the document is an array of documents for each individual
    member of a replica set. To add a new property to a particular member, we do the
    following. For instance, if we want to add the `votes` key and set its value to
    `2` for the third member of the replica set (index 2 in the array), we would do
    the following:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档中的成员字段是复制集中每个成员的文档数组。要为特定成员添加新属性，我们要做以下操作。例如，如果我们想要为复制集的第三个成员（数组中的索引2）添加`votes`键并将其值设置为`2`，我们将执行以下操作：
- en: '[PRE86]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Just changing the JSON document won''t change the replica set. We need to reconfigure
    it if the replica set is already in place, as follows:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅仅改变JSON文档不会改变复制集。如果复制集已经存在，我们需要重新配置它，如下所示：
- en: '[PRE87]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'If the configuration is done for the first time, we would call the following:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是首次进行配置，我们将调用以下命令：
- en: '[PRE88]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: For all the steps given next, you need to follow the preceding steps to reconfigure
    or initiate the replica set unless some other steps are mentioned explicitly.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的所有步骤中，除非明确提到其他步骤，否则您需要遵循前面的步骤来重新配置或启动复制集。
- en: How to do it…
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: In this recipe, we will look at some of the possible configurations that can
    be done in a replica set. The explanation will be minimal with all the explanation
    done in the next section, as usual.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将看一些可能在复制集中进行的配置。解释将是最小的，所有解释都将在下一节中进行，与往常一样。
- en: 'The first configuration is `arbiterOnly` option. It is used to configure a
    replica set member as a member that holds no data but only has rights to vote.
    The following key need to be added to the configuration of the member who would
    be made an arbiter:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个配置是`arbiterOnly`选项。它用于将复制集成员配置为不持有数据，只具有投票权的成员。需要将以下键添加到将成为仲裁者的成员的配置中：
- en: '[PRE89]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'One thing to remember regarding this configuration is that once a replica set
    is initiated, no existing member can be changed to an arbiter from a non-arbiter
    node and vice versa. We can, however, add arbiter to an existing replica set using
    the helper function `rs.addArb(<hostname>:<port>)`. For example, add an arbiter
    listening to port `27004` to an existing replica set. The following was done on
    my machine to add an arbiter:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于此配置的一点需要记住的是，一旦初始化了副本集，就无法将现有成员从非仲裁节点更改为仲裁节点，反之亦然。但是，我们可以使用助手函数`rs.addArb(<hostname>:<port>)`向现有副本集添加仲裁者。例如，向现有副本集添加一个侦听端口`27004`的仲裁者。在我的机器上执行以下操作以添加仲裁者：
- en: '[PRE90]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: When the server is started to listen to port `27004` and `rs.status()` is executed
    from the mongo shell, we should see that the `state` and the `strState` for this
    member is `7` and `ARBITER`, respectively.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当服务器启动以侦听端口`27004`并从mongo shell执行`rs.status()`时，我们应该看到该成员的`state`和`strState`分别为`7`和`ARBITER`。
- en: 'The next option `votes` affects the number of votes a member has in the election.
    By default, all members have one vote each, this option can be used to change
    the number of votes a particular member has. It can be set as follows:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个选项`votes`影响成员在选举中的投票数。默认情况下，所有成员每人有一票，此选项可用于更改特定成员的投票数。可以设置如下：
- en: '[PRE91]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Votes of existing members of a replica set can be changed and the replica set
    can be reconfigured using the `rs.reconfig()` helper.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用`rs.reconfig()`助手更改副本集现有成员的选票，并重新配置副本集。
- en: Though the option `votes` is available, which can potentially change the number
    of votes to form a majority, it usually doesn't add much value and not a recommended
    option to use in production.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管`votes`选项可用，可以潜在地改变形成多数的选票数，但通常并不增加太多价值，也不建议在生产中使用。
- en: 'Next replica set configuration option is called the `priority`. It determines
    the eligibility of a replica set member to become a primary (or not to become
    a primary). The option is set as follows:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个副本集配置选项称为`priority`。它确定副本集成员成为主服务器的资格（或不成为主服务器）。该选项设置如下：
- en: '[PRE92]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Higher number indicates more likely hood of becoming a primary, the primary
    would always be the one with the highest priority amongst the members alive in
    a replica set. Setting this option in an already configured replica set will trigger
    an election.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更高的数字表示更有可能成为主要成员，主要成员始终是副本集中活跃成员中优先级最高的成员。在已配置的副本集中设置此选项将触发选举。
- en: Setting the priority to `0` will ensure that a member will never become primary.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优先级设置为`0`将确保成员永远不会成为主服务器。
- en: 'Next option we would be looking at is `hidden`. Setting the value of this option
    to true ensures that the replica set member is hidden. The option is set as follows:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们将看到的选项是`hidden`。将此选项的值设置为true可确保副本集成员处于隐藏状态。该选项设置如下：
- en: '[PRE93]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: One thing to keep in mind is that when the replica set member is hidden, its
    priority too should be made `0` to ensure it doesn't become primary. Though this
    seems redundant; as of the current version, the value or priority needs to be
    set explicitly.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要记住的一点是，当副本集成员处于隐藏状态时，其优先级也应设置为`0`，以确保它不会成为主服务器。尽管这似乎多余；但截至目前的版本，值或优先级需要明确设置。
- en: When a programming language client connects to a replica set, it would not be
    able to discover hidden members. However, after using `rs.status()` from the shell,
    the member's status would be visible.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当编程语言客户端连接到副本集时，无法发现隐藏成员。但是，在从shell中使用`rs.status()`后，成员的状态将可见。
- en: 'Let''s look at the `slaveDelay` option now. This option is used to set lag
    in time for the slave from the primary of the replica set. The option is set as
    follows:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们看看`slaveDelay`选项。此选项用于设置从副本集的主服务器到从服务器的时间延迟。该选项设置如下：
- en: '[PRE94]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Like the hidden member, slave delayed members too should have the priority set
    to `0` to ensure they don't ever become primary. This needs to be set explicitly.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与隐藏成员一样，延迟的从服务器成员也应将优先级设置为`0`，以确保它们永远不会成为主服务器。这需要明确设置。
- en: 'Let''s look at the final configuration option: `buildIndexes`. This value if
    not specified by default, is true, which indicates if an index is created on the
    primary, it needs to be replicated on the secondary too. The option is set as
    follows:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看最终的配置选项：`buildIndexes`。如果未指定，默认情况下为true，这表示在主服务器上创建索引时，需要在从服务器上复制该索引。该选项设置如下：
- en: '[PRE95]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: When using this option with a value set to false, the priority is set to `0`
    to ensure they don't ever become primary. This needs to be set explicitly. Also,
    this option cannot be set after the replica set is initiated. Just like an arbiter
    node, this needs to be set when the replica set is being created or when a new
    member node is being added to the replica set.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当将此选项与设置为false的值一起使用时，优先级设置为`0`，以确保它们永远不会成为主服务器。这需要明确设置。此外，在初始化副本集后无法设置此选项。就像仲裁节点一样，这需要在创建副本集或向副本集添加新成员节点时设置。
- en: How it works…
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we will explain and understand the significance of different
    types of members and the configuration options we saw in the previous section.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释和理解不同类型成员的重要性以及在上一节中看到的配置选项。
- en: Replica set member as an arbiter
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 副本集成员作为仲裁者
- en: The English meaning of the word *arbiter* is a judge who resolves a dispute.
    In the case of replica sets, the arbiter node is present just to vote in case
    of elections and not replicate any data. This is in fact, a pretty common scenario
    due to a fact that that a Mongo replica set needs to have at least three instances
    (and preferably odd number of instances, 3 or more). A lot of applications do
    not need to maintain three copies of data and are happy with just two instances,
    one primary and a secondary with the data.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '*仲裁者*这个词的英文含义是解决争端的法官。在副本集的情况下，仲裁者节点只是在选举时投票，而不复制任何数据。事实上，这是一个非常常见的情况，因为Mongo副本集至少需要三个实例（最好是奇数个实例，3个或更多）。许多应用程序不需要维护三份数据，只需要两个实例，一个主服务器和一个带有数据的辅助服务器。'
- en: Consider the scenario where only two instances are present in the replica set.
    When the primary goes down, the secondary instance cannot form a proper majority
    because it only has 50 percent votes (its own vote) and thus cannot become a primary.
    If a majority of secondary instances goes down, then the primary instance steps
    down from primary and becomes a secondary, thus making the replica set unavailable
    for writes. Thus, a two-node replica set is useless as it doesn't stay available
    even when any of the instances goes down. It defeats the purpose of setting up
    a replica set and thus at least three instances are needed in a replica set.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑只有两个实例存在于副本集的情况。当主服务器宕机时，辅助实例无法形成适当的多数，因为它只有50%的选票（自己的选票），因此无法成为主服务器。如果大多数辅助实例宕机，那么主服务器实例将从主服务器退下，并成为辅助服务器，从而使副本集无法进行写入。因此，两节点副本集是无用的，因为即使其中任何一个实例宕机，它也无法保持可用。这违背了设置副本集的目的，因此副本集至少需要三个实例。
- en: Arbiters come handy in such scenarios. We set up a replica set instance with
    three instances with only two having data and one acting as an arbiter. We need
    not maintain three copies of data at the same time we eliminate the problem we
    faced by setting up a two-instance replica set.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，仲裁者非常有用。我们设置了一个包含三个实例的副本集实例，其中只有两个实例具有数据，另一个充当仲裁者。我们无需同时维护三份数据，通过设置一个两实例副本集来消除我们设置两实例副本集时遇到的问题。
- en: Priority of replica set members
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 副本集成员的优先级
- en: The priority flag can be used by itself or in conjunction with other options
    like `hidden`, `slaveDelay`, and `buildIndexes`, where we don't want the member
    with one of these three options to be ever made primary. We will look at these
    options soon.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级标志可以单独使用，也可以与`hidden`、`slaveDelay`和`buildIndexes`等其他选项一起使用，其中我们不希望具有这三个选项之一的成员被选为主服务器。我们将很快看到这些选项。
- en: 'Some more possible use cases where we would never want a replica set to become
    a primary are as follows:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些可能的用例，我们永远不希望副本集成为主服务器，如下所示：
- en: When the hardware configuration of a member would not be able to deal with the
    write and read requests should it become a primary and the only reason it is being
    put in there is for replicating the data.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当成员的硬件配置无法处理写入和读取请求时，如果成为主服务器，那么将其放在那里的唯一原因就是复制数据。
- en: We have a multi data centers setup where one replica set instance is present
    in another data center for the sake of geographically distributing the data for
    DR purposes. Ideally, the network latency between the application server hosting
    the application and the database should be minimal for optimum performance. This
    could be achieved if both the servers (application server and the database server)
    are in the same data center. Not changing the priority of the replica set instance
    in another data center makes it equally eligible for being chosen as a primary
    and thus compromising on the application's performance if the server in other
    data center gets chosen as primary. In such scenarios, we can set the priority
    to be `0` for the server in the second data center and a manual cutover would
    be needed by the administrator to fail over to another data center should an emergency
    arise.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个多数据中心的设置，其中一个副本集实例存在于另一个数据中心，为了地理分布数据以用于灾难恢复目的。理想情况下，应用程序服务器和数据库之间的网络延迟应该最小，以实现最佳性能。如果两台服务器（应用程序服务器和数据库服务器）在同一个数据中心，就可以实现这一点。不改变另一个数据中心副本集实例的优先级，使其同样有资格被选为主服务器，从而在其他数据中心的服务器被选为主服务器时会影响应用程序的性能。在这种情况下，我们可以将第二个数据中心的服务器的优先级设置为`0`，并且需要管理员手动切换到另一个数据中心，以应对紧急情况。
- en: In both scenarios mentioned here, we could also have the respective members
    hidden so that the application client doesn't have a view of these members in
    the first place.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们还可以将相应的成员隐藏起来，以便应用客户端首先看不到这些成员。
- en: Similar to setting a priority to `0` for not allowing one to be primary, we
    can also be biased to one member to be primary whenever it is available by setting
    its priority to a value greater than 1, because the default value of priority
    is `1`.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 与将优先级设置为`0`以防止某个成员成为主服务器类似，我们也可以通过将其优先级设置为大于1的值来偏向于某个成员在可用时成为主服务器，因为优先级的默认值是`1`。
- en: Suppose we have a scenario for budget reasons we have one of the members storing
    data on SSDs and remaining on spinning disks. We would ideally want the member
    with SSDs to be the primary whenever it is up and running. It is only when it
    is not available we would want another member to become a primary, In such scenarios
    we can set the priority of the member running on SSD to a value greater than 1\.
    The value doesn't really matter as long as it is greater than the rest, that is,
    setting it to `1.5` or `2` makes no difference as long as priority of other members
    is less.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 假设由于预算原因，我们有一个成员将数据存储在固态硬盘上，其余成员存储在机械硬盘上。我们理想情况下希望固态硬盘的成员在运行时成为主服务器。只有在它不可用时，我们才希望另一个成员成为主服务器，在这种情况下，我们可以将运行在固态硬盘上的成员的优先级设置为大于1的值。该值实际上并不重要，只要它大于其他成员的值，也就是说，将其设置为`1.5`或`2`都没有关系，只要其他成员的优先级较低。
- en: Hidden, slave delayed, and build index configuration
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏、从属延迟和构建索引配置
- en: The term hidden for a replica set node is from an application client that is
    connected to the replica set and not for an administrator. For an administrator,
    the hidden members are equally important to be monitored and thus their state
    is seen in the `rs.status()` response. Hidden members participate in elections
    too like all other members.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集节点的隐藏术语是指连接到副本集的应用程序客户端，而不是管理员。对于管理员来说，隐藏成员同样重要，因此它们的状态在`rs.status()`响应中可见。隐藏成员也像所有其他成员一样参与选举。
- en: For the `slaveDelay` option, most common use case is to ensure that the data
    in a member as a particular point of time lags behind the primary by the provided
    number of seconds and can be restored in case some unforeseen error has happened,
    say a human error for erroneously updating some data. Remember, longer the time
    delay, more is the time we get recover but at the cost of possibly stale data.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`slaveDelay`选项，最常见的用例是确保成员在特定时间点的数据落后于主要成员提供的秒数，并且可以在发生某些意外错误时进行恢复，例如错误地更新了一些数据。请记住，延迟时间越长，我们就能够获得更多的恢复时间，但可能会导致数据过时。
- en: The `buildIndexes` option is useful in cases where we have a replica set member
    with non-production standard hardware and the cost of maintaining the indexes
    are not worth it. You may choose to set this option for members where no queries
    are executed on it. Obviously, if you set this option it can never become a primary
    member, and thus the priority option is forced to be set to `0`.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '`buildIndexes`选项在以下情况下很有用：我们有一个副本集成员，其硬件不符合生产标准，维护索引的成本不值得。您可以选择为不执行任何查询的成员设置此选项。显然，如果设置了此选项，它永远不会成为主要成员，因此优先级选项被强制设置为`0`。'
- en: There's more…
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: You can achieve some interesting things using tags in replica sets. This would
    be discussed in a later recipe, after we learn about tags in the recipe *Building
    tagged replica sets*.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用副本集中的标签实现一些有趣的事情。这将在稍后的食谱中讨论，在我们学习有关标签的食谱*构建带标签的副本集*之后。
- en: Stepping down as primary from the replica set
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从副本集中下台为主要成员
- en: There are times when for some maintenance activity during business hours we
    would like to take a server out from the replica set, perform the maintenance
    and put it back in the replica set. If the server to be worked upon is the primary,
    we somehow need to step down from the primary member position, perform re-election
    and ensure that it doesn't get re-elected for a minimum given time frame. After
    the server becomes secondary once the step down operation is successful, we can
    take it out of the replica set, perform the maintenance activity and put it back
    in the replica set.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在工作时间进行某些维护活动时，我们希望将服务器从副本集中取出，执行维护活动，然后将其放回副本集。如果要处理的服务器是主服务器，我们需要从主成员位置下台，执行重新选举，并确保在一定的时间范围内不会再次被选中。一旦下台操作成功，服务器成为辅助服务器后，我们可以将其从副本集中取出，执行维护活动，然后将其放回副本集。
- en: Getting ready
  id: totrans-522
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Set up a simple three-node replica set on your computer, as mentioned
    in the recipe.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件和副本集基础知识，请参考[第1章](ch01.html "第1章。安装和启动服务器")中的食谱*作为副本集的一部分启动多个实例*，*安装和启动服务器*。按照食谱中提到的方法，在计算机上设置一个简单的三节点副本集。
- en: How to do it…
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Assuming at this point of time we have a replica set up and running, do the
    following:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 假设此时我们已经设置并运行了一个副本集，请执行以下操作：
- en: 'Execute the following from the shell connected to one of the replica set members
    and see which instance currently is the primary:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从连接到副本集成员之一的shell中执行以下操作，并查看当前是主要实例的哪个实例：
- en: '[PRE96]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Connect to that primary instance from the mongo shell and execute the following
    on the shell:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell连接到主实例，并在shell上执行以下操作：
- en: '[PRE97]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The shell should reconnect again and you should see that the instance connected
    to and initially a primary instance now becomes secondary. Execute the following
    from the shell so that a new primary is now re-elected:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: shell应该重新连接，您应该看到连接到并最初是主要实例的实例现在变为辅助实例。从shell执行以下操作，以便现在重新选举一个新的主要实例：
- en: '[PRE98]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: You can now connect to the primary, modify the replica set configuration and
    go ahead with the administration on the servers.
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您可以连接到主服务器，修改副本集配置，并继续对服务器进行管理。
- en: How it works…
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The preceding steps mentioned are pretty simple, but there are a couple of interesting
    things that we will see.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的步骤非常简单，但我们将看到一些有趣的事情。
- en: The method we saw previously, `rs.stepDown()` did not have any parameters. The
    function can in fact take a numeric value, which is the number of seconds for
    which the instance stepped down won't participate in the elections and won't become
    a primary and the default value for this is `60` seconds.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的`rs.stepDown()`方法没有任何参数。实际上，该函数可以接受一个数字值，即实例下台不参与选举并且不会成为主要实例的秒数，默认值为`60`秒。
- en: Another interesting thing to try out is what if the instance that was asked
    to step down has a higher priority than other instances. Well, it turns out that
    the priority doesn't matter when you step down. The instance stepped down will
    not become primary no matter what for the provided number of seconds. However,
    if priority is set for the instance stepped down and it is higher than others,
    then after the time given to `stepDown` elapses an election will happen and the
    instance with higher priority will become primary again.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的尝试是，如果被要求下台的实例的优先级高于其他实例会发生什么。事实证明，当您下台时，优先级并不重要。被下台的实例无论如何都不会在提供的秒数内成为主要实例。但是，如果为被下台的实例设置了优先级，并且优先级高于其他实例，则在给定的`stepDown`时间过去后，将发生选举，并且优先级较高的实例将再次成为主要实例。
- en: Exploring the local database of a replica set
  id: totrans-537
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索副本集的本地数据库
- en: In this recipe, we will explore the local database from a replica set's perspective.
    The local database may contain collections that are not specific to replica sets,
    but we will focus only on the replica set specific collections and try to take
    a look at what's in them and what they mean.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将从副本集的角度探索本地数据库。本地数据库可能包含不特定于副本集的集合，但我们将只关注副本集特定的集合，并尝试查看其中的内容和含义。
- en: Getting ready
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer,
    as mentioned in the recipe.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 有关先决条件和副本集基础知识，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的食谱*作为副本集的一部分启动多个实例*，*安装和启动服务器*。继续在计算机上设置一个简单的三节点副本集，如食谱中所述。
- en: How to do it…
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: With the replica set up and running, we need to open a shell connected to the
    primary. You may connect randomly to any one member; use `rs.status()` and then
    determine the primary.
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行副本集后，我们需要打开一个连接到主节点的 shell。您可以随机连接到任何一个成员；使用`rs.status()`然后确定主节点。
- en: 'With shell open, first switch to `local` database and the view the collections
    in the `local` database as follows:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 shell 后，首先切换到`local`数据库，然后按以下方式查看`local`数据库中的集合：
- en: '[PRE99]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'You should find a collection called `me`. Querying this collection should show
    us one document and it contains the hostname of the server to which we are currently
    connected to:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该找到一个名为`me`的集合。查询此集合应该显示一个文档，其中包含我们当前连接到的服务器的主机名：
- en: '[PRE100]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: There would be two fields, the hostname and the `_id` field. Take a note of
    the `_id` field—it is important.
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将会有两个字段，主机名和`_id`字段。记下`_id`字段-这很重要。
- en: 'Take a look at the `replset.minvalid` collection. You will have to connect
    to a secondary member from the shell to execute the following query. Switch to
    the `local` database first:'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`replset.minvalid`集合。您将需要从 shell 连接到次要成员才能执行以下查询。首先切换到`local`数据库：
- en: '[PRE101]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: This collection just contains the single document with a key `ts` and a value
    that is the timestamp till the time the secondary we are connected to is synchronized.
    Note down this time.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此集合只包含一个带有键`ts`和值的单个文档，该值是我们连接到的次要实例同步的时间戳。记下这个时间。
- en: 'From the shell in primary, insert a document in any collection. We will use
    the database as test. Execute the following from the shell of the primary member:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主要的 shell 中，在任何集合中插入一个文档。我们将使用数据库作为测试。从主要成员的 shell 中执行以下操作：
- en: '[PRE102]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Query the secondary again, as follows:'
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次查询次要，如下所示：
- en: '[PRE103]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We should see that the time against the field `ts` has now incremented corresponding
    to the time this replication happened from primary to secondary. With a slave
    delayed node, you will see this time getting updated only after the delay period
    has elapsed.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该看到`ts`字段的时间现在已经增加，对应于此复制从主要到次要的时间。对于延迟的从属节点，只有在延迟期过去后，才会看到此时间得到更新。
- en: 'Finally, we will see the collection `system.replset`. This collection is the
    place where the replica set configuration is stored. Execute the following:'
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将看到`system.replset`集合。这个集合是存储副本集配置的地方。执行以下操作：
- en: '[PRE104]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Actually, when we execute `rs.conf()`, the following query gets executed:'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，当我们执行`rs.conf()`时，将执行以下查询：
- en: '[PRE105]'
  id: totrans-559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: How it works…
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The database local is a special (non-replicated) database that is used to hold
    the replication and instance specific details in it. Try creating a collection
    of your own in the local database and insert some data in it; it would not be
    replicated to the secondary nodes.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 本地数据库是一个特殊的（非复制）数据库，用于保存其中的复制和实例特定的详细信息。尝试在本地数据库中创建自己的集合，并向其中插入一些数据；这些数据不会被复制到辅助节点。
- en: This database gives us some view of the data stored by mongo for internal use.
    However, as an administrator, it is good to know about these collections and the
    type of data in it.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据库为我们提供了一些关于 mongo 存储的内部数据的视图。然而，作为管理员，了解这些集合和其中的数据类型是很重要的。
- en: Most the collections are pretty straightforward. From the shell of the secondary
    execute the query `db.me.findOne()` in the local database and we should see that
    `_id` there should match the `_id` field of the document present in the slaves
    collection.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数集合都很简单。从辅助节点的 shell 中在本地数据库中执行查询`db.me.findOne()`，我们应该看到那里的`_id`应该与从属集合中的文档中的`_id`字段匹配。
- en: The config document we see gives the hostname of the secondary instance that
    we are referring to. Note that the port and other configuration options of the
    replica set member are not present in this document. Finally, the `syncedTo` time
    tells us till what time the secondary instances are synced up with the primary.
    We saw the collection `replset.minvalid` on the secondary, which tells us the
    time till which it is synced with primary. This value in the `syncedTo` time on
    primary would be same as in `replset.minvalid` on respective secondary.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的配置文档给出了我们所指的辅助实例的主机名。请注意，副本集成员的端口和其他配置选项在此文档中不存在。最后，`syncedTo`时间告诉我们次要实例与主要实例同步的时间。我们在次要实例上看到了`replset.minvalid`集合，它告诉我们它与主要实例同步的时间。主要实例上的`syncedTo`时间的值与相应次要实例上的`replset.minvalid`中的值相同。
- en: There's more…
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We have not seen the oplog, which is interesting to look at. We would take a
    look at this special collection in a separate recipe, *Understanding and analyzing
    oplogs*.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有看到 oplog，这是一个有趣的地方。我们将在单独的食谱中查看这个特殊集合，*理解和分析 oplog*。
- en: Understanding and analyzing oplogs
  id: totrans-567
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和分析 oplog
- en: Oplog is a special collection and forms the backbone of the MongoDB replication.
    When any write operation or configuration changes are done on the replica set's
    primary, they are written to the oplog on the primary. All the secondary members
    then tail this collection to get the changes to be replicated. Tailing is synonymous
    to tail command in Unix and can only be done on a special type of collection called
    capped collection. Capped collections are fixed size collections which maintain
    the insertion order just like a queue. When the collection's allocated space becomes
    full, the oldest data is overwritten. If you are not aware of capped collections
    and what tailable cursors are, please refer to *Creating and tailing a capped
    collection cursors in MongoDB* in [Chapter 5](ch05.html "Chapter 5. Advanced Operations"),
    *Advanced Operations* for more details.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: Oplog是一个特殊的集合，是MongoDB复制的支柱。当在副本集的主服务器上执行任何写操作或配置更改时，它们都会被写入主服务器的oplog中。然后，所有次要成员都会追踪此集合以获取要复制的更改。追踪类似于Unix中的tail命令，只能在一种特殊类型的集合上进行，称为受限集合。受限集合是固定大小的集合，它们保持插入顺序，就像队列一样。当集合的分配空间变满时，最旧的数据将被覆盖。如果您不了解受限集合和可追踪游标是什么，请参考[第5章](ch05.html
    "第5章 高级操作")中的*在MongoDB中创建和追踪受限集合游标*，了解更多详情。
- en: Oplog is a capped collection present in the non-replicated database called **local**.
    In our previous recipe, we saw what a `local` database is and what collections
    are present in it. Oplog is something we didn't discuss in last recipe, as it
    demands a lot more explanation and a dedicated recipe is needed to do justice.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: Oplog是一个受限集合，存在于名为**local**的非复制数据库中。在我们之前的配方中，我们看到了`local`数据库是什么，以及其中存在哪些集合。Oplog是我们在上一篇配方中没有讨论的内容，因为它需要更多的解释，需要一个专门的配方来做出解释。
- en: Getting ready
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for the prerequisites and know about the replica set
    basics. Go ahead and set up a simple three-node replica set on your computer as
    mentioned in the recipe. Open a shell and connect to the primary member of the
    replica set. You will need to start the mongo shell and connect to the primary
    instance.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[第1章](ch01.html "第1章 安装和启动服务器")中的配方*作为副本集的一部分启动多个实例*，了解先决条件并了解副本集的基础知识。按照配方中提到的步骤，在计算机上设置一个简单的三节点副本集。打开一个shell并连接到副本集的主成员。您需要启动mongo
    shell并连接到主实例。
- en: How to do it…
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: Execute the following steps after connecting to a primary from the shell to
    get the timestamp of the last operation present in the oplog. We would be interested
    in looking at the operations after this time.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到shell后，执行以下步骤以获取oplog中存在的最后一个操作的时间戳。我们对此时间之后的操作感兴趣。
- en: '[PRE106]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Execute the following from the shell. Keep the output in the shell or copy
    it to some place. We will analyze it later:'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从shell中执行以下操作。保留shell中的输出或将其复制到其他地方。我们稍后会对其进行分析：
- en: '[PRE107]'
  id: totrans-576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Insert 10 documents as follows:'
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按以下方式插入10个文档：
- en: '[PRE108]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Execute the following update to set a string value for all documents with value
    of `i` greater than `5`, which is 6, 7, 8 and 9 in our case. It would be a multiupdate
    operation:'
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下更新，为所有值大于`5`的文档设置一个字符串值，即我们的情况下的6、7、8和9。这将是一个多更新操作：
- en: '[PRE109]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Now, create the index as follows:'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下步骤创建索引：
- en: '[PRE110]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Execute the following query on oplog:'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在oplog上执行以下查询：
- en: '[PRE111]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: How it works…
  id: totrans-585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: For those aware of messaging and its terminologies, Oplog can be looked at as
    a topic in messaging world with one producer, the primary instance, and multiple
    consumers, the secondary instances. Primary instance writes to an oplog all the
    contents that need to be replicated. Thus, any create, update, and delete operations
    as well as any reconfigurations on the replica sets would be written to the oplog
    and the secondary instances would tail (continuously read the contents of the
    oplog being added to it, similar to a tail with `-f` option command in Unix) the
    collection to get documents written by the primary. If the secondary has a `slaveDelay`
    configured, it will not read documents more than the maximum time minus the `slaveDelay`
    time from the oplog.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 对于了解消息传递及其术语的人来说，Oplog可以被看作是消息传递世界中的一个主题，其中有一个生产者，即主实例，和多个消费者，即次要实例。主实例将所有需要复制的内容写入oplog。因此，任何创建、更新和删除操作以及副本集上的任何重新配置都将被写入oplog，次要实例将追踪（连续读取被添加到其中的oplog内容，类似于Unix中带有`-f`选项的tail命令）集合以获取主要写入的文档。如果次要实例配置了`slaveDelay`，它将不会从oplog中读取超过最大时间减去`slaveDelay`时间的文档。
- en: We started by saving an instance of the local database in the variable called
    `local` and identified a cutoff time that we would use for querying all the operations
    we will perform in this recipe from the oplog.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将local数据库的一个实例保存在名为`local`的变量中，并确定一个截止时间，我们将使用它来查询我们将在本配方中执行的所有操作。
- en: Executing a query on the `system.namespaces` collection in the local database
    shows us that the collection is a capped collection with a fixed size. For performance
    reasons capped collections are allocated continuous space on the filesystem and
    are preallocated. The size allocated by the server is dependent on the OS and
    CPU architecture. While starting the server the option `oplogSize` can be provided
    to mention the size of the oplog. The defaults are generally good enough for most
    cases. However, for development purpose, you can choose to override this value
    for a smaller value. Oplogs are capped collections that need to be preallocated
    a space on disk. This preallocation not only takes time when the replica set is
    first initialized but takes up a fixed amount of disk space. For development purpose,
    we generally start multiple MongoDB processes as part of the same replica set
    on same machine and would want them to be up and running as quickly as possible
    with minimum resource usage. Also, having the entire oplog in memory becomes possible
    if the oplog size is small. For all these reasons, it is advisable to start the
    local instances for development purpose with a small oplog size.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地数据库的`system.namespaces`集合上执行查询，可以看到该集合是一个具有固定大小的封顶集合。出于性能原因，封顶集合在文件系统上分配连续的空间，并且是预分配的。服务器分配的大小取决于操作系统和CPU架构。在启动服务器时，可以提供`oplogSize`选项来指定oplog的大小。默认值通常对大多数情况都足够好。但是，出于开发目的，您可以选择覆盖此值为较小的值。oplog是需要在磁盘上预分配空间的封顶集合。这种预分配不仅在副本集首次初始化时需要时间，而且占用了固定数量的磁盘空间。出于开发目的，我们通常在同一台机器上作为同一副本集的一部分启动多个MongoDB进程，并希望它们尽快运行，并且占用最少的资源。此外，如果oplog大小较小，则可以将整个oplog放入内存中。出于所有这些原因，建议出于开发目的以较小的oplog大小启动本地实例。
- en: 'We performed some operations such as insert 10 documents and update four documents
    using a multi update and create an index. If we query the oplog for entries after
    the cutoff, we computed earlier we see 10 documents for each insert in it. The
    document looks something like this:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行了一些操作，比如插入10个文档，使用多次更新更新了四个文档，并创建了一个索引。如果我们查询截止日期后的oplog条目，我们计算出10个文档，每个插入一个。文档看起来像这样：
- en: '[PRE112]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'As we can see, we first look at the three fields: `op`, `ns`, and `o`. These
    stand for the operation, the fully qualified name of the collection into which
    the data is being inserted, and the actual object to be inserted. The operation
    `i` stand for insert operation. Note that the value of `o`, which is the document
    to be inserted, contains the `_id` field that got generated on the primary. We
    should see 10 such documents, one for each insert. What is interesting to see
    is what happens on a multi update operation. The primary puts four documents,
    one for each of them affected for the updates. In this case, the value `op` is
    `u`, which is for update and the query used to match the document is not the same
    as what we gave in the update function, but it is a query that uniquely finds
    a document based on the `_id` field. Since there is an index already in place
    for the `_id` field (created automatically for each collection), this operation
    to find the document to be updated is not expensive. The value of the field `o`
    is the same document we passed to the update function from the shell. The sample
    document in the oplog for the update is as follows:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们首先看三个字段：`op`，`ns`和`o`。这些代表操作，被插入数据的集合的完全限定名称，以及要插入的实际对象。操作`i`代表插入操作。请注意，`o`的值，即要插入的文档，包含在主键上生成的`_id`字段。我们应该看到10个这样的文档，每个插入一个。有趣的是在多次更新操作中发生了什么。主键为每个受影响的文档放入了四个文档。在这种情况下，值`op`是`u`，表示更新，用于匹配文档的查询与我们在更新函数中给出的查询不同，但是是一个基于`_id`字段唯一找到文档的查询。由于`_id`字段已经存在索引（每个集合自动创建），因此查找要更新的文档的操作并不昂贵。字段`o`的值是我们从shell中传递给更新函数的相同文档。更新的oplog中的示例文档如下：
- en: '[PRE113]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: The update in the oplog is the same as the one we provided. This is because
    the `$set` operation is idempotent, which means you may apply an operation safely
    any number of times.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: oplog中的更新与我们提供的更新相同。这是因为`$set`操作是幂等的，这意味着可以安全地多次应用操作。
- en: 'However, update using `$inc` operator is not idempotent. Let''s execute the
    following update:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用`$inc`运算符的更新不是幂等的。让我们执行以下更新：
- en: '[PRE114]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: In this case, the oplog would have the following as the value of `o`.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，oplog将具有以下值作为`o`的值。
- en: '[PRE115]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: This non-idempotent operation is put into oplog by Mongo smartly as an idempotent
    operation with the value of i set to a value that is expected to be after the
    increment operation once. Thus it is safe to replay an oplog any number of times
    without corrupting the data.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 这个非幂等操作被Mongo智能地放入oplog中，作为一个幂等操作，其值为i设置为一次增量操作后预期的值。因此，可以安全地重放oplog任意次数，而不会损坏数据。
- en: Finally, we can see that the index creation process is put in the oplog as an
    insert operation in the `system.indexes` collection. For large collections, index
    creation can take hours and thus the size of the oplog is very important to let
    the secondary catch up from where it hasn't replicated since the index creation
    started. However, since version 2.6, index creation initiated in background on
    primary will also be built in background on secondary instances.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以看到索引创建过程被放入oplog作为`system.indexes`集合中的插入操作。对于大型集合，索引创建可能需要几个小时，因此oplog的大小非常重要，以便让从未复制的次要服务器从索引创建开始复制。然而，自2.6版本以来，在主服务器上后台启动的索引创建也将在次要实例上后台构建。
- en: 'For more details on the index creation on replica sets, visit the following
    URL: [http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/](http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/).'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 有关副本集上索引创建的更多详细信息，请访问以下网址：[http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/](http://docs.mongodb.org/master/tutorial/build-indexes-on-replica-sets/)。
- en: Building tagged replica sets
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建标记的副本集
- en: In [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, we saw how to set up a simple replica in *Starting multiple
    instances as part of a replica set* and saw what is the purpose of a replica set.
    We also have a good deal of explanation on what `WriteConcern` is in the [Appendix](apa.html
    "Appendix A. Concepts for Reference") of the book and why it is used. What we
    saw about write concerns is that it offers a minimum level guarantee for a certain
    write operation. However, with the concept of tags and write concerns, we can
    define a variety of rules and conditions which must be satisfied before a write
    operation is deemed successful and a response is sent to the user.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html "第1章 安装和启动服务器")中，*安装和启动服务器*，我们看到了如何设置一个简单的副本集，*作为副本集的一部分启动多个实例*，并了解了副本集的目的。我们还在本书的[附录](apa.html
    "附录 A. 参考概念")中对`WriteConcern`有了很好的解释，以及为什么要使用它。我们所看到的关于写入关注的内容是，它为某个写入操作提供了最低级别的保证。然而，通过标签和写入关注的概念，我们可以定义各种规则和条件，在写入操作被视为成功并向用户发送响应之前，这些规则和条件必须得到满足。
- en: 'Consider some common use cases such as the following:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一些常见的用例，比如以下情况：
- en: Application wants the write operation to be propagated to at least one server
    in each of its data center. This ensures that in event of a data center shutdown,
    other data centers will have the data that was written by the application.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序希望写入操作至少传播到每个数据中心中的一个服务器。这样可以确保在数据中心关闭时，其他数据中心将拥有应用程序写入的数据。
- en: If there are no multiple data centers, at least one member of a replica set
    is kept on different rack. For instance, if the rack's power supply goes down,
    the replica set will still be available (not necessarily for writes) as at least
    one member is running on a different rack. In such scenarios, we would want the
    write to be propagated to at least two racks before responding back to the client
    with a successful write.
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有多个数据中心，副本集中至少有一个成员放在不同的机架上。例如，如果机架的电源供应中断，副本集仍然可用（不一定是用于写入），因为至少有一个成员在不同的机架上运行。在这种情况下，我们希望在向客户端返回成功写入之前，将写入传播到至少两个机架。
- en: It is possible that a reporting application queries a group of secondary of
    a replica set for generating some reports regularly. (Such secondary might be
    configured to never become a primary). After each write, we want to ensure that
    the write operation is replicated to at least one reporting replica member before
    acknowledging the write as successful.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能有一个报告应用程序定期查询副本集的一组辅助成员以生成一些报告。（这样的辅助成员可能被配置为永远不会成为主要成员）。在每次写入后，我们希望确保写入操作至少被复制到至少一个报告副本成员，然后才确认写入成功。
- en: The preceding use cases are a few of the common use cases that arise and are
    not addressed using simple write concerns that we have seen earlier. We need a
    different mechanism to cater to these requirements and replica sets with tags
    is what we need.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的用例是一些常见的用例，这些用例是我们之前看到的简单写入关注所不能解决的。我们需要一个不同的机制来满足这些要求，而带有标签的副本集就是我们需要的。
- en: 'Obviously, the next question is what exactly are tags? Let''s take an example
    of a blog. Various posts in the blog have different tags attached to them. These
    tags allow us to easily search, group, and relate posts together. Tags are some
    user defined text with some meaning attached to it. If we draw an analogy between
    the blog post and the replica set members, similar to how we attach tags to a
    post, we can attach tags to each replica set member. For example, in a multiple
    data center scenario with two replica set members in data center 1 (`dc1`) and
    one member in data center 2 (`dc2`), we can have the following tags assigned to
    the members. The name of the key and the value assigned to the tag is arbitrary
    and is chosen during design of the application; you may choose to even assign
    any tags like the administrator who set up the server if you really find it useful
    to address your use case:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，下一个问题是标签到底是什么？让我们以博客为例。博客中的各种帖子都附有不同的标签。这些标签使我们能够轻松搜索、分组和关联帖子。标签是一些用户定义的文本，附有一些含义。如果我们将博客帖子和副本集成员进行类比，就像我们给帖子附上标签一样，我们可以给每个副本集成员附上标签。例如，在一个多数据中心的情况下，数据中心1（`dc1`）有两个副本集成员，数据中心2（`dc2`）有一个成员，我们可以给成员分配以下标签。键的名称和标签分配的值是任意的，并且在应用程序设计期间选择；您甚至可以选择分配任何标签，比如设置服务器的管理员，如果您真的发现它对您的用例有用：
- en: '| Replica Set Member | Tag |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| 副本集成员 | 标签 |'
- en: '| --- | --- |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Replica set member 1 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-1''}`
    |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| 副本集成员1 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-1''}` |'
- en: '| Replica set member 2 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-2''}`
    |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 副本集成员2 | `{''datacentre'': ''dc1'', ''rack'': ''rack-dc1-2''}` |'
- en: '| Replica set member 3 | `{''datacentre'': ''dc2'', ''rack'': ''rack-dc2-2''}`
    |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 副本集成员3 | `{''datacentre'': ''dc2'', ''rack'': ''rack-dc2-2''}` |'
- en: That is good enough to lay the foundation of what a replica set tags are. In
    this recipe, we will see how to assign tags to replica set members and more importantly,
    how to make use of them to address some of the sample use cases we saw earlier.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 这足以奠定副本集标签的基础。在本教程中，我们将看到如何为副本集成员分配标签，更重要的是，如何利用它们来解决我们之前看到的一些示例用例。
- en: Getting ready
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set from
    Chapter 1, Installing and Starting the Server* for the prerequisites and know
    about the replica set basics. Go ahead and set up a simple three-node replica
    set on your computer, as mentioned in the recipe. Open a shell and connect to
    the primary member of the replica set.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关写入关注的信息，请参考第1章《安装和启动服务器》中的*作为副本集的一部分启动多个实例*的教程，了解先决条件并了解副本集的基础知识。按照教程中的说明，在计算机上设置一个简单的三节点副本集。打开一个shell并连接到副本集的主要成员。
- en: If you need to know about write concerns, refer to the overview of write concerns
    in the [Appendix](apa.html "Appendix A. Concepts for Reference") of the book.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要了解写入关注，请参阅本书的[附录](apa.html "附录 A. 参考概念")中的写入关注概述。
- en: For inserting documents in the database, we will use Python as it gives us an
    interactive interface like the mongo shell. Refer to the recipe *Connecting to
    a single node using a Python client* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for steps on how
    to install pymongo. Mongo shell would have been the most ideal candidate for the
    demonstration of the insert operations, but there are certain limitations around
    the usage of the shell with our custom write concern. Technically, any programming
    language with the write concerns mentioned in the recipe for insert operations
    would work fine.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在数据库中插入文档，我们将使用Python，因为它为我们提供了像mongo shell一样的交互式界面。有关如何安装pymongo的步骤，请参阅[第1章](ch01.html
    "第1章。安装和启动服务器")中的配方*使用Python客户端连接到单个节点*，*安装和启动服务器*。Mongo shell本来是插入操作演示的最理想候选者，但是在使用自定义写关注时存在某些限制。从技术上讲，任何编程语言都可以使用插入操作的配方中提到的写关注来正常工作。
- en: How to do it…
  id: totrans-619
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'With the replica set started, we will add tags to it and reconfigure it as
    follows. The following commands are executed from the mongo shell:'
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在复制集启动后，我们将为其添加标签并重新配置如下。以下命令从mongo shell中执行：
- en: '[PRE116]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'With the replica set tags set (not that we have not yet reconfigured the replica
    set), we need to define some custom write concerns. First, we define one that
    will ensure that the data gets replicated to at least to one server in each data
    center. Execute the following in the mongo shell again:'
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置了复制集标签后（注意我们尚未重新配置复制集），我们需要定义一些自定义写关注。首先，我们定义一个可以确保数据至少被复制到每个数据中心中的一个服务器的写关注。再次在mongo
    shell中执行以下操作：
- en: '[PRE117]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Start the python shell and execute the following:'
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Python shell并执行以下操作：
- en: '[PRE118]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'We will now execute the following insert:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将执行以下插入操作：
- en: '[PRE119]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The preceding insert goes through successfully and the `ObjectId` would be printed
    out; you may query the collection to confirm from either the mongo shell or Python
    shell.
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的插入成功进行，`ObjectId`将被打印出；您可以从mongo shell或Python shell中查询集合以进行确认。
- en: Since our primary is one of the servers in data centre `1`, we will now stop
    the server listening to port `27002`, which is the one with priority `0` and tagged
    to be in a different data center.
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的主服务器是数据中心`1`中的一个，我们现在将停止监听端口`27002`的服务器，该服务器的优先级为`0`，并标记为位于不同数据中心中的服务器。
- en: 'Once the server is stopped (you may confirm using the `rs.status()` helper
    function from the mongo shell), execute the following insert again, this insert
    should error out:'
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦服务器停止（您可以使用mongo shell中的`rs.status()`辅助函数进行确认），再次执行以下插入操作，这次应该会出现错误：
- en: '[PRE120]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Restart the stopped mongo server.
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动已停止的mongo服务器。
- en: 'Similarly, we can achieve rack awareness by ensuring that the write propagates
    to at least two racks (in any data centre) by defining a new configuration as
    follows from the mongo shell:'
  id: totrans-633
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过定义一个新的配置来确保写入至少传播到两个机架（在任何数据中心中）来实现机架感知。从mongo shell中执行以下操作：
- en: '[PRE121]'
  id: totrans-634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'The settings value of the conf object would then be as follows. Once set, reconfigure
    the replica set again using `rs.reconfig(conf)` from the mongo shell:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，conf对象的设置值将如下所示。设置后，再次使用mongo shell中的`rs.reconfig(conf)`重新配置复制集：
- en: '[PRE122]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: We saw `WriteConcern` used with replica set tags to achieve some functionality
    like data center and rack awareness. Let's see how we can use replica set tags
    with read operations.
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到了使用复制集标签和`WriteConcern`来实现数据中心和机架感知的一些功能。让我们看看如何在读取操作中使用复制集标签。
- en: We will see how to make use of replica set tags with read preference. Let's
    reconfigure the set by adding one more tag to mark a secondary member that will
    be used to execute some hourly stats reporting.
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将看到如何使用复制集标签和读取偏好。通过添加一个标签来重新配置集合，以标记一个次要成员，该成员将用于执行一些每小时的统计报告。
- en: 'Execute the following steps to reconfigure the set from the mongo shell:'
  id: totrans-639
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell中执行以下步骤重新配置集合：
- en: '[PRE123]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: This will configure the same member with priority `0` and the one in a different
    data center with an additional tag called type with a value reports.
  id: totrans-641
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使用额外的标签type和值reports配置相同的优先级为`0`的成员，并且在不同数据中心中的服务器。
- en: 'We now go back to the python shell and perform the following steps:'
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们回到Python shell并执行以下步骤：
- en: '[PRE124]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: The preceding execution should show us one document from the collection (as
    we has inserted data in this test collection in previous steps).
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的执行应该向我们展示集合中的一个文档（因为我们在之前的步骤中向测试集合中插入了数据）。
- en: 'Stop the instance which we have tagged for reporting, that is, the server listening
    to connections on port `27002` and execute the following on the python shell again:'
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止我们标记为报告的实例，即在端口`27002`上监听连接的服务器，并再次在Python shell上执行以下操作：
- en: '[PRE125]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: This time around, the execution should fail and state that no secondary found
    with the required tag sets.
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一次，执行应该失败，并指出找不到具有所需标签集的次要服务器。
- en: How it works…
  id: totrans-648
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: In this recipe, we did a lot of operations on tagged replica sets and saw how
    it can affect the write operations using `WriteConcern` and read operations using
    `ReadPreference`. Let's look at them in some details now.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们对标记的复制集进行了许多操作，并看到了它如何影响使用`WriteConcern`的写操作和使用`ReadPreference`的读操作。现在让我们更详细地看一下它们。
- en: WriteConcern in tagged replica sets
  id: totrans-650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在标记的复制集中的写关注
- en: We set up a replica set that was up and running, which we reconfigured to add
    tags. We tagged the first two servers in datacenter 1 and in different racks (servers
    running listening to port `27000` and `27001` for client connections) and the
    third one in datacenter 2 (server listening to port `27002` for client connections).
    We also ensured that the member in datacenter 2 doesn't become a primary by setting
    its priority to `0`.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了一个正在运行的复制集，我们重新配置以添加标签。我们为数据中心1中的前两台服务器和不同机架（运行监听端口`27000`和`27001`以供客户端连接）添加了标签，并为数据中心2中的第三台服务器（运行监听端口`27002`以供客户端连接）添加了标签。我们还确保了数据中心2中的成员不会成为主服务器，将其优先级设置为`0`。
- en: 'Our first objective is to ensure that the write operations to the replica set
    gets replicated to at least one member in the two datacenters. To ensure this,
    we define a write concern as follows `{''MultiDC'':{datacentre : 2}}`. Here, we
    first define the name of the write concern as MultiDC. The value which is a JSON
    object has one key with name datacenter, which is same as the key used for the
    tag we attached to the replica set and the value is a number `2`, which will be
    looked as the number of distinct values of the given tag that should acknowledge
    the write before it is deemed successful.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个目标是确保对副本集的写操作至少被复制到两个数据中心中的一个成员。为了确保这一点，我们定义了一个写关注，如下所示`{'MultiDC':{datacentre：2}}`。在这里，我们首先将写关注的名称定义为MultiDC。值是一个JSON对象，其中有一个名为datacenter的键，与我们附加到副本集的标签使用的键相同，值是一个数字`2`，在被视为成功之前，应该确认写入的给定标签的不同值的数量。
- en: 'For instance, in our case, when the write comes to server 1 in datacenter 1,
    the number of distinct values of the tag datacenter is 1\. If the write operation
    gets replicated to the second server, the number still stays one as the value
    of the tag datacenter is same as the first member. It is only when the third server
    acknowledges the write operation, the write satisfies the defined condition of
    replicating the write to distinct two values of the tag *datacenter* in the replica
    set. Note that the value can only be a number and not have something like `{datacentre
    : ''dc1''}` this definition is invalid and an error will be thrown while re-configuring
    the replica set.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的情况下，当写入到数据中心1的服务器1时，标签数据中心的不同值数量为1。如果写操作被复制到第二个服务器，数量仍然保持为1，因为标签数据中心的值与第一个成员相同。只有当第三个服务器确认写操作时，写操作才满足将写操作复制到副本集中标签*datacenter*的两个不同值的定义条件。请注意，该值只能是一个数字，而不能像`{datacentre：'dc1'}`这样的定义是无效的，并且在重新配置副本集时会抛出错误。
- en: But we need to register this write concern somewhere with the server. This is
    done in the final step of the configuration by setting the settings value in configuration
    JSON. The value to set is `getLastErrorModes`. The value of `getLastErrorModes`
    is a JSON document with all possible write concerns defined in it. We later defined
    one more write concern for write propagated to at least two racks. This is conceptually
    in line with MultiDC write concern and thus we will not be discussing it in details
    here. After setting all the required tags and the settings, we reconfigure the
    replica set for the changes to take effect.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们需要在服务器的某个地方注册这个写关注。这是通过在配置JSON中设置settings值来完成的。要设置的值是`getLastErrorModes`。`getLastErrorModes`的值是一个包含所有可能的写关注的JSON文档。我们稍后为至少复制到两个机架的写操作定义了一个更多的写关注。这与MultiDC写关注的概念一致，因此我们在这里不会详细讨论它。设置所有必需的标签和设置后，我们重新配置副本集以使更改生效。
- en: Once reconfigured, we perform some write operations using the MultiDC write
    concern. When two members in two distinct datacenters are available, the write
    goes through successfully. However, when the server in second datacenter goes
    down, the write operation times out and throws an exception to the client initiating
    the write. This demonstrates that the write operation will succeed or fail as
    per how we intended.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 重新配置后，我们使用MultiDC写关注进行一些写操作。当两个不同数据中心中的两个成员可用时，写操作成功进行。然而，当第二个数据中心的服务器宕机时，写操作超时并向发起写操作的客户端抛出异常。这表明写操作将根据我们的意图成功或失败。
- en: We just saw how these custom tags can be used to address some interesting use
    cases, which are not supported by the product implicitly as far as write operations
    are concerned. Similar to write operations, read operations can take full advantages
    of these tags to address some use cases such as reading from a fixed set of secondary
    members that are tagged with a particular value.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了这些自定义标签如何用于解决一些有趣的用例，这些用例在产品隐式支持的情况下，对于写操作而言是不支持的。与写操作类似，读操作可以充分利用这些标签来解决一些用例，例如从标有特定值的固定次要成员集中读取。
- en: ReadPreference in tagged replica sets
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记的副本集中的读取偏好
- en: We added another custom tag annotating a member to be used for reporting purposes,
    we then fire a query operation with the read preference to query a secondary and
    provide the tag sets that should be looked for before considering the member as
    a candidate for read operation. Remember that when using primary as the read preference,
    we cannot use tags and that is reason we explicitly specified the value of the
    `read_preference` to `SECONDARY`.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了另一个自定义标签，用于注释要用于报告目的的成员，然后我们发出一个带有读取偏好的查询操作，以查询次要成员，并提供在考虑成员作为读取操作候选之前应查找的标签集。请记住，当使用主读取偏好时，我们不能使用标签，这就是我们明确指定`read_preference`的值为`SECONDARY`的原因。
- en: Configuring the default shard for non-sharded collections
  id: totrans-659
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置非分片集合的默认分片
- en: In the recipe *Starting a simple sharded environment of two shards* in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* we set up a simple two-shard server. In the recipe *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* we added data
    to a person collection that was sharded. However, for any collection that is not
    sharded, all the documents end up on one shard called the primary shard. This
    situation is acceptable for small databases with relatively small number of collections.
    However, if the database size increases and at the same time the number of un-sharded
    collections increase, we end up overloading a particular shard (which is the primary
    shard for a database) with a lot of data from these un-sharded collections. All
    query operations for such un-sharded collections as well as those on the collections
    whose particular range in the shard reside on this server instance will be directed
    to this it. In such scenario, we can have the primary shard of a database changed
    to some other instance so that these un-sharded collections get balanced out across
    different instances.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html "第1章 安装和启动服务器")的*启动一个简单的两个分片的分片环境*教程中，我们设置了一个简单的两个分片服务器。在[第1章](ch01.html
    "第1章 安装和启动服务器")的*连接到shell中的分片并执行操作*教程中，我们向一个被分片的person集合添加了数据。然而，对于任何未被分片的集合，所有文档最终都会位于一个称为主分片的分片上。对于相对较小数量的集合的小型数据库，这种情况是可以接受的。然而，如果数据库大小增加，同时未被分片的集合数量也增加，我们就会使一个特定的分片（对于数据库来说是主分片）过载，因为来自这些未被分片的集合的大量数据都会存储在这个分片上。所有对这些未被分片的集合以及那些在分片中特定范围内的集合的查询操作都将被定向到这个分片。在这种情况下，我们可以将数据库的主分片更改为其他实例，以便这些未被分片的集合在不同的实例之间得到平衡。
- en: In this recipe, we will see how to view this primary shard and change it to
    some other server whenever needed.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将看到如何查看这个主分片，并在需要时将其更改为其他服务器。
- en: Getting ready
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: Following the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* set up and start a sharded environment. From the shell,
    connect to the started mongos process. Also, assuming that the two shards servers
    are listening to port `27000` and `27001`, connect from the shell to these two
    processes. So, we have a total of three shells opened, one connected to the mongos
    process and two to these individual shards.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 按照[第1章](ch01.html "第1章 安装和启动服务器")中的*启动一个简单的两个分片的分片环境*教程，设置并启动一个分片环境。从shell连接到已启动的mongos进程。另外，假设两个分片服务器分别监听端口`27000`和`27001`，从shell连接到这两个进程。因此，我们总共打开了三个shell，一个连接到mongos进程，另外两个连接到这两个单独的分片。
- en: 'We need are using the `test` database for this recipe and sharding has to be
    enabled on it. If it not, then you need to execute the following on the shell
    connected to the mongos process:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这个教程中使用`test`数据库，并且必须在其上启用分片。如果没有启用，则需要在连接到mongos进程的shell上执行以下操作：
- en: '[PRE126]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: How to do it…
  id: totrans-666
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'From the shell connected to the mongos process, execute the following two commands:'
  id: totrans-667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从连接到mongos进程的shell中，执行以下两个命令：
- en: '[PRE127]'
  id: totrans-668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'In the databases, look out for `test` database and take a note of the `primary`.
    Suppose the following is a part (showing the part under databases only) of the
    output of `sh.status()`:'
  id: totrans-669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据库中，查找`test`数据库，并记下`primary`。假设以下是`sh.status()`输出的一部分（仅显示数据库部分）：
- en: '[PRE128]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: The second document under the databases shows us that the database `test` is
    enabled for sharding (because partitioned is true) and the primary shard is `shard0000`.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库下的第二个文档显示，数据库`test`已启用分片（因为partitioned为true），主分片是`shard0000`。
- en: 'The primary shard, which is `shard0000` in our case, is the mongod process
    listening to port `27000`. Open the shell connected to this process and execute
    the following in it:'
  id: totrans-672
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的情况下，主分片是`shard0000`，是监听端口`27000`的mongod进程。打开连接到此进程的shell，并在其中执行以下操作：
- en: '[PRE129]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Now, connect to another mongod process listening to port `27001` and again
    execute the following query:'
  id: totrans-674
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，连接到另一个监听端口`27001`的mongod进程，并再次执行以下查询：
- en: '[PRE130]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Note that the data would be found only on the primary shard and not on other
    shard.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据只会在主分片上找到，而不会在其他分片上找到。
- en: 'Execute the following command from the mongos shell:'
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongos shell执行以下命令：
- en: '[PRE131]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Execute the following command from mongo shell connected to the mongos process:'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从连接到mongos进程的mongo shell执行以下命令：
- en: '[PRE132]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'From the shell connected to the mongos processes running on port `27000` and
    `27001`, execute the following query:'
  id: totrans-681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从运行在端口`27000`和`27001`的mongos进程连接的shell中，执行以下查询：
- en: '[PRE133]'
  id: totrans-682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: How it works…
  id: totrans-683
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We started a sharded setup and connected to it from the mongos process. We started
    by inserting a document in the `testCol` collection that is not enabled for sharding
    in the test database, which is not enabled for sharding as well. In such cases,
    the data lies on shard called the **primary shard**. Do not misunderstand this
    for the primary of a replica set. This is a shard (that itself can be a replica
    set) and it is the shard chosen by default for all database and collection for
    which sharding is not enabled.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动了一个分片设置，并从mongos进程连接到它。我们首先在`test`数据库中的`testCol`集合中插入了一个文档，该集合也没有启用分片。在这种情况下，数据位于称为**主分片**的分片上。不要将其误解为副本集的主分片。这是一个分片（它本身可以是一个副本集），它是默认选择的所有未启用分片的数据库和集合的分片。
- en: 'When we add the data to a non-sharded collection, it was seen only on the shard
    that is primary. Executing `sh.status()` tells us the primary shard. To change
    the primary, we need to execute a command from the admin database from the shell
    connected to the mongos process. The command is as follows:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据添加到非分片集合时，只能在主分片上看到。执行`sh.status()`告诉我们主分片。要更改主分片，我们需要从连接到mongos进程的shell中的admin数据库执行命令。命令如下：
- en: '[PRE134]'
  id: totrans-686
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: Once the primary shard was changed, all existing data of non-sharded database
    and collection was migrated to the new primary and all subsequent writes to non-sharded
    collections will go to this shard.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主分片更改，所有非分片数据库和集合的现有数据都将迁移到新的主分片，并且所有后续写入非分片集合的操作都将转到此分片。
- en: Use this command with caution as it will migrate all the unsharded collections
    to the new primary, which may take time for big collections.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此命令时要小心，因为它将把所有未分片的集合迁移到新的主分片，这可能需要大量时间。
- en: Manual split and migration of chunks
  id: totrans-689
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动拆分和迁移块
- en: Though MongoDB does a good job of splitting and migrating chunks across shards
    to maintain the balance, under some circumstances such as a small number of documents
    or relatively large number of small documents where the automatic balancer doesn't
    split the collection, an administrator might want to split and migrate the chunks
    manually. In this recipe, we will see how to split and migrate the collection
    manually across shards. For this recipe, we will set up a simple shard as we saw
    in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MongoDB在跨分片上拆分和迁移块以保持平衡方面做得很好，但在某些情况下，例如文档数量较少或相对较多的小文档数量，自动平衡器无法拆分集合，管理员可能希望手动拆分和迁移块。在本教程中，我们将看到如何手动在分片之间拆分和迁移集合。对于本教程，我们将设置一个简单的分片，就像我们在[第1章](ch01.html
    "第1章。安装和启动服务器")中看到的那样，*安装和启动服务器*。
- en: Getting ready
  id: totrans-691
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: Refer to the recipe *Starting a simple sharded environment of two shards* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* to set up and start a sharded environment. It is preferred
    to start a clean environment without any data in it. From the shell, connect to
    the started mongos process.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的*启动一个简单的两个分片的分片环境*，*安装和启动服务器*，以设置和启动分片环境。最好在没有任何数据的情况下启动一个干净的环境。从shell连接到已启动的mongos进程。
- en: How to do it…
  id: totrans-693
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Connect to the mongos process from the mongo shell and enable sharding on the
    `test` database and the `splitAndMoveTest` collection as follows:'
  id: totrans-694
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell连接到mongos进程，并按以下方式在`test`数据库和`splitAndMoveTest`集合上启用分片：
- en: '[PRE135]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Let''s load the data in the collection as follows:'
  id: totrans-696
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按照以下方式在集合中加载数据：
- en: '[PRE136]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Once the data is loaded, execute the following:'
  id: totrans-698
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据加载完成，执行以下操作：
- en: '[PRE137]'
  id: totrans-699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: Note the number of documents in two shards in the plan. The value to lookout
    for is in the two documents under the shards key in the result of explain plan.
    Within these two documents the field to lookout for is `n`.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 注意计划中两个分片中的文档数量。要注意的值在解释计划结果的shards键下的两个文档中。在这两个文档中，要注意的字段是`n`。
- en: 'Execute the following to see the splits of the collection:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以查看集合的拆分：
- en: '[PRE138]'
  id: totrans-702
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Split the chunk into two at `5000` as follows:'
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`5000`处将块拆分为两个部分：
- en: '[PRE139]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'Splitting it doesn''t migrate it to the second server. See what exactly happened
    with the chunks by executing the following query again:'
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拆分它不会将其迁移到第二个服务器。通过再次执行以下查询来查看块的确切情况：
- en: '[PRE140]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'We will now move the second chunk to the second shard:'
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将第二个块移动到第二个分片：
- en: '[PRE141]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Execute the following query again and confirm the migration:'
  id: totrans-709
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次执行以下查询并确认迁移：
- en: '[PRE142]'
  id: totrans-710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Alternatively, the following explain plan will show a split of about 50-50:'
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，以下解释计划将显示大约50-50的拆分：
- en: '[PRE143]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: How it works…
  id: totrans-713
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: We simulate a small data load by adding monotonically increasing numbers and
    discover that the numbers are not split across two shards evenly by viewing the
    query plan. It is not a problem as the chunk size needs to reach a particular
    threshold, 64 MB by default, before the balancer decides to migrate the chunks
    across the shards to maintain balance. This is pretty perfect as in real world,
    when the data size gets huge we will see that eventually over a period of time
    the shards are well balanced.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过添加单调递增的数字来模拟小数据负载，并发现查询计划中的数字没有均匀分布在两个分片上。这不是问题，因为在平衡器决定迁移块以保持平衡之前，块大小需要达到特定的阈值，默认为64
    MB。这非常完美，因为在现实世界中，当数据量变得巨大时，我们将看到随着时间的推移，分片将保持良好的平衡。
- en: However, if the administration does decide to split and migrate the chunks,
    it is possible to do it manually. The two helper functions `sh.splitAt` and `sh.moveChunk`
    are there to do this work. Let's look at their signatures and see what they do.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果管理人员决定手动拆分和迁移块，可以手动执行。有两个辅助函数`sh.splitAt`和`sh.moveChunk`来执行这项工作。让我们看看它们的签名并了解它们的作用。
- en: The function `sh.splitAt` takes two arguments, first is the namespace, which
    has the format `<database>.<collection name>` and the second parameter is the
    query that acts as the split point to split the chunk into two, possibly two uneven
    portions depending on where the given document is in the chunk. There is another
    method, `sh.splitFind`, which will try and split the chunk in two equal portions.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`sh.splitAt`接受两个参数，第一个是命名空间，格式为`<数据库>.<集合名称>`，第二个参数是作为拆分点的查询，将块拆分为两个部分，可能是两个不均匀的部分，取决于给定文档在块中的位置。还有另一种方法`sh.splitFind`，它将尝试将块分成两个相等的部分。
- en: Splitting doesn't mean the chunk moves to another shard, it just breaks one
    big chunk into two, but the data stays on the same shard. It is an inexpensive
    operation which involves updating the config DB.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分并不意味着块移动到另一个分片，它只是将一个大块分成两个，但数据仍然留在同一个分片上。这是一个廉价的操作，涉及更新配置数据库。
- en: Next, we executed was to migrate the chunk to a different shard after we split
    it into two. The operation `sh.MoveChunk` is used just to do that. This function
    takes three parameters, first one is again the namespace of the collection that
    has the format `<database>.<collection name>`, second parameter is a query a document
    whose chunk would be migrated, and the third parameter is the destination chunk.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行的是将块分成两部分后将其迁移到不同分片的操作。操作`sh.MoveChunk`就是用来做这个的。这个函数接受三个参数，第一个是集合的命名空间，格式为`<数据库>.<集合名称>`，第二个参数是一个查询文档，其块将被迁移，第三个参数是目标块。
- en: Once the migration is done, the query's plan shows us that the data is split
    in two chunks.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移完成后，查询计划显示数据分为两个块。
- en: Domain-driven sharding using tags
  id: totrans-720
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标签进行领域驱动分片
- en: The recipes *Starting a simple sharded environment of two shards* and *Connecting
    to a shard in the shell and performing operations* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* explained how
    to start a simple two server shard and then insert data in a collection after
    choosing a shard key. The data that gets sharded is more technical where the data
    chunk is kept to a manageable size by Mongo by splitting it into multiple chunks
    and migrating the chunks across shards to keep the chunk distribution even across
    shards. But what if we want the sharding to be more domain oriented? Suppose we
    have a database for storing postal addresses and we shard based on postal codes
    where we know the postal code range of a city. What we can do is tag the shard
    servers according to the city name as the tag, add shard range (postal codes),
    and associate this range with the tag. This way, we can state which servers can
    contain the postal addresses of which cities. For instance, we know that Mumbai
    being most populous city, the number of addresses would be huge and thus we add
    two shards for Mumbai. On the other hand, one shard should be enough to cope up
    with the volumes of the Pune city. For now we tag just one shard. In this recipe,
    we will see how to achieve this use case using tag aware sharding. If the description
    is confusing, don't worry, we will see how to implement what we just discussed.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html "第1章。安装和启动服务器")的*安装和启动服务器*中，*启动一个简单的两个分片的环境*和*在shell中连接到一个分片并执行操作*解释了如何启动一个简单的两个服务器分片，然后在选择分片键后插入集合中的数据。被分片的数据更加技术化，Mongo通过将其分成多个块并迁移这些块来保持块在分片之间的均匀分布。但是，如果我们希望分片更加领域化呢？假设我们有一个用于存储邮政地址的数据库，我们根据我们知道的城市的邮政编码范围来进行分片。我们可以根据城市名称作为标签对分片服务器进行标记，添加分片范围（邮政编码），并将此范围与标签关联起来。这样，我们可以说明哪些服务器可以包含哪些城市的邮政地址。例如，我们知道孟买是人口最多的城市，地址数量将会很大，因此我们为孟买添加了两个分片。另一方面，一个分片应该足够应对浦那市的数量。目前我们只标记了一个分片。在这个配方中，我们将看到如何使用标签感知分片来实现这个用例。如果描述令人困惑，不用担心，我们将看到如何实现我们刚刚讨论的内容。
- en: Getting ready
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting a simple sharded environment of two shard* in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for information on how to start a simple shard. However,
    for this recipe, we will add an additional shard. So, we will now start three
    mongo servers listening to port `27000`, `27001`, and `27002`. Again, it is recommended
    to start off with a clean database. For the purpose of this recipe, we will be
    using the collection `userAddress` to store the data.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html "第1章。安装和启动服务器")的*安装和启动服务器*中，参考配方*启动一个简单的两个分片的环境*，了解如何启动一个简单的分片的信息。然而，对于这个配方，我们将添加一个额外的分片。因此，我们现在将启动三个监听端口为`27000`、`27001`和`27002`的mongo服务器。同样，建议从一个干净的数据库开始。为了这个配方，我们将使用集合`userAddress`来存储数据。
- en: How to do it…
  id: totrans-724
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Assuming that we have three shard up and running, let''s execute the following:'
  id: totrans-725
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有三个分片正在运行，让我们执行以下操作：
- en: '[PRE144]'
  id: totrans-726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'With tags defined, let''s define range of pin codes that will map to a tag:'
  id: totrans-727
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了标签后，让我们定义将映射到标签的邮政编码范围：
- en: '[PRE145]'
  id: totrans-728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'Enable sharding for the test database and the `userAddress` collection as follows:'
  id: totrans-729
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为测试数据库和`userAddress`集合启用分片，如下所示：
- en: '[PRE146]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Insert the following documents in the `userAddress` collection:'
  id: totrans-731
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`userAddress`集合中插入以下文档：
- en: '[PRE147]'
  id: totrans-732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Execute the following plans:'
  id: totrans-733
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下计划：
- en: '[PRE148]'
  id: totrans-734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: How it works…
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Suppose we want to partition data driven by domain in a shard, we can use tag
    aware sharding. It is an excellent mechanism that lets us tag the shards and then
    split the data range across shards identified by the tags. We don't really have
    to bother about the actual machines and their address hosting the shard. Tags
    act as a good abstraction in the way, we can tag a shard with multiple tags and
    one tag can be applied to multiple shards.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要根据领域来分区数据，我们可以使用标签感知分片。这是一个很好的机制，让我们可以为分片打标签，然后根据标签将数据范围分布到被标记的分片上。我们实际上不需要担心托管分片的实际机器和它们的地址。标签在这方面起到了很好的抽象作用，我们可以为分片打上多个标签，一个标签可以应用到多个分片上。
- en: In our case, we have three shards and we apply tags to each of them using the
    `sh.addShardTag` method. The method takes the shard ID, which we can see in the
    `sh.status` call with the *shards* key. This `sh.addShardTag` method can be used
    to keep adding tags to a shard. Similarly, there is a helper method `sh.removeShardTag`
    to remove an assignment of the tag from the shard. Both these methods take two
    parameters, the first one is the shard ID and second one of the tag to remove.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们有三个分片，并使用`sh.addShardTag`方法为每个分片应用标签。该方法接受分片ID，我们可以在`sh.status`调用中看到带有*shards*键的分片ID。`sh.addShardTag`方法可用于不断向分片添加标签。类似地，还有一个辅助方法`sh.removeShardTag`，用于从分片中删除标签的分配。这两种方法都接受两个参数，第一个是分片ID，第二个是要删除的标签。
- en: Once the tagging is done, we assign range of the values of the shard key to
    the tag. The method `sh.addTagRange` is used to do that. It accepts four parameters,
    first one is the namespace, which is the fully qualified name of the collection,
    second and third parameters are the start and end value of the range for this
    shard key and the fourth parameter is the tag name of the shards hosting the range
    being added. For example, the call `sh.addTagRange('test.userAddress', {pincode:400001},
    {pincode:400999}, 'Mumbai')` says we are adding the shard range `400001` to `400999`
    for the collection `test.userAddress`, and this range will be stored in the shards
    tagged as `Mumbai`.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 标记完成后，我们将分片键的值范围分配给标记。使用方法`sh.addTagRange`来执行。它接受四个参数，第一个是命名空间，即集合的完全限定名称，第二个和第三个参数是此分片键范围的起始和结束值，第四个参数是添加范围的分片的标记名称。例如，调用`sh.addTagRange('test.userAddress',
    {pincode:400001}, {pincode:400999}, 'Mumbai')`表示我们正在为集合`test.userAddress`添加分片范围`400001`到`400999`，并且此范围将存储在标记为`Mumbai`的分片中。
- en: Once the tagging and adding tag range is done, we enabled sharding on database
    and collection and add data to it from Mumbai and Pune city with respective pin
    codes. We then query and explain the plan to see that the data did indeed reside
    on the shards we have tagged for Pune and Mumbai city. We can also add new shards
    to this sharded setup and accordingly tag the new shard. The balancer will then
    accordingly balance the data based on the value it is tagged. For instance, if
    the addresses in Pune increase overloading a shard, we can add a new shard with
    tag as Pune. The postal address for Pune will then be sharded across these two
    server instances for tagged for Pune city.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和添加标记范围完成后，我们在数据库和集合上启用了分片，并从孟买和浦那城市的相应邮政编码添加了数据。然后，我们查询并解释计划，以查看数据确实驻留在我们为浦那和孟买城市标记的分片上。我们还可以向此分片设置添加新的分片，并相应地标记新的分片。然后，平衡器将根据其标记的值平衡数据。例如，如果浦那的地址增加导致分片过载，我们可以添加一个新的标记为浦那的分片。然后，浦那的邮政地址将在这两个标记为浦那城市的服务器实例上进行分片。
- en: Exploring the config database in a sharded setup
  id: totrans-740
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分片设置中探索配置数据库
- en: Config database is the backbone of a sharded setup in Mongo. It stores all the
    metadata of the shard setup and has a dedicated mongod process running for it.
    When a mongos process is started we provide it with the config servers' URL. In
    this recipe, we will take a look at some collections in the config database and
    dive deep into their content and significance.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 配置数据库是Mongo中分片设置的支柱。它存储分片设置的所有元数据，并且有一个专用的mongod进程在运行。当启动mongos进程时，我们会为其提供配置服务器的URL。在本教程中，我们将深入研究配置数据库中的一些集合，并深入了解它们的内容和重要性。
- en: Getting ready
  id: totrans-742
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We need a sharded setup for this recipe. Refer to the recipe *Starting a simple
    sharded environment of two shard* in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* for information
    on how to start a simple shard. Additionally, connect to the mongos process from
    a shell.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为本教程准备一个分片设置。有关如何启动一个简单的分片的信息，请参考[第1章](ch01.html "第1章。安装和启动服务器")中的*启动一个简单的两个分片的分片环境*，*安装和启动服务器*。此外，从shell连接到mongos进程。
- en: How to do it…
  id: totrans-744
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'From the console connected to the mongos process, switch to the config database
    and execute the following:'
  id: totrans-745
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从连接到mongos进程的控制台，切换到配置数据库并执行以下操作：
- en: '[PRE149]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'From the list of all collections, we will visit a few. We start with the databases
    collection. This keeps a track of all the databases on this shard. Execute the
    following from the shell:'
  id: totrans-747
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有集合的列表中，我们将访问一些。我们从数据库集合开始。这会跟踪此分片上的所有数据库。在shell上执行以下操作：
- en: '[PRE150]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: The content of the result is pretty straightforward, the value of the field
    `_id` is for the database. The value of field partitioned tells us whether sharding
    is enabled for the database or not; true indicates it is enabled and the field
    primary gives the primary shard where the data of non-sharded collections reside
    upon.
  id: totrans-749
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果的内容非常简单，字段`_id`的值是数据库的值。字段partitioned的值告诉我们数据库是否启用了分片；true表示已启用，并且字段primary给出了非分片集合数据所在的主分片。
- en: 'Next, we will visit the `collections` collection. Execute the following from
    the shell:'
  id: totrans-750
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将访问`collections`集合。在shell上执行以下操作：
- en: '[PRE151]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: This collection, unlike the databases collection we saw earlier, contains only
    those collections for which we have enabled sharding. The field `_id` gives the
    namespace of the collection in the `<database>.<collection name>` format, the
    field key gives the shard key and the field unique, indicates whether the shard
    key is unique or not. These three fields come as the three parameters of the `sh.shardCollection`
    function in that very order.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前看到的数据库集合不同，此集合仅包含我们已启用分片的集合。字段`_id`以`<database>.<collection name>`格式给出集合的命名空间，字段key给出分片键，字段unique指示分片键是否唯一。这三个字段按照`sh.shardCollection`函数的三个参数的顺序给出。
- en: 'Next, we look at the `chunks` collection. Execute the following on the shell.
    If the database was clean when we started this recipe, we won''t have a lot of
    data in this:'
  id: totrans-753
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们查看`chunks`集合。在shell上执行以下操作。如果数据库在我们开始本教程时是干净的，那么在这里我们不会有很多数据：
- en: '[PRE152]'
  id: totrans-754
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'We then look at the tags collection and execute the following query:'
  id: totrans-755
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们查看tags集合并执行以下查询：
- en: '[PRE153]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: Let's query the mongos collection as follows.
  id: totrans-757
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按照以下方式查询mongos集合。
- en: '[PRE154]'
  id: totrans-758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: This is a simple collection that gives the list of all mongos instances connected
    to the shard with the details like the host and port on which the mongos instance
    is running, which forms the `_id` field. The version and figures like for how
    much time the process is up and running in seconds.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的集合，它列出了所有连接到具有详细信息的mongos实例的分片，例如mongos实例正在运行的主机和端口，这形成了`_id`字段。版本和数字，例如进程运行的时间。
- en: 'Finally, we look at the version collection. Execute the following query. Note
    that is not similar to other queries we execute:'
  id: totrans-760
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们查看version集合。执行以下查询。请注意，这与我们执行的其他查询不同：
- en: '[PRE155]'
  id: totrans-761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: How it works…
  id: totrans-762
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We saw the collections and databases collection while we queried them and they
    are pretty simple. Let''s look at the collection called `chunks`. Here is a sample
    document from this collection:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查询集合和数据库集合时，它们非常简单。让我们来看看名为`chunks`的集合。这是该集合中的一个示例文档：
- en: '[PRE156]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: The fields of interest are `ns`, `min`, `max`, and `shard`, which are the namespace
    of the collection, the minimum value present in the chunk, the maximum value present
    in the chunk, and the shard on which this chunk lies, respectively. The value
    of the chunk size is 64 MB by default. This can be seen in the settings collection.
    Execute `db.settings.find()` from the shell and look at the value of the field
    value, which is the size of the chunk in MB. Chunks are restricted to this small
    size to ease the migration process across shards, if needed. When the size of
    the chunk exceeds this threshold, mongo server finds a suitable point in the existing
    chunk to break it into two and adds a new entry in this chunks collection. This
    operation is called splitting, which is inexpensive as the data stays where it
    is; it is just logically split into multiple chunks. The balancer on mongo tries
    to keep the chunks across shards balanced and the moment it sees some imbalance,
    it migrates these chunks to a different shard. This is expensive and also depends
    largely on the network bandwidth. If we use `sh.status()`, the implementation
    actually queries the collections we saw and prints the pretty formatted result.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的字段是`ns`、`min`、`max`和`shard`，分别是集合的命名空间、块中存在的最小值、块中存在的最大值以及该块所在的分片。默认情况下，块大小的值为64
    MB。这可以在设置集合中看到。从shell中执行`db.settings.find()`并查看字段值的值，即块的大小（以MB为单位）。块的大小受限于这个小的大小，以便在需要时跨分片进行迁移过程。当块的大小超过这个阈值时，Mongo服务器会找到现有块中合适的点将其分成两部分，并在这个块集合中添加一个新条目。这个操作被称为分裂，它是廉价的，因为数据仍然保持在原地；它只是在逻辑上分成多个块。Mongo上的平衡器会尝试保持分片之间的块平衡，一旦发现不平衡，它就会将这些块迁移到不同的分片。这是昂贵的，也在很大程度上取决于网络带宽。如果我们使用`sh.status()`，实际上是查询我们看到的集合，并打印出漂亮格式的结果。
