- en: Chapter 5. Advanced Operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。高级操作
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Atomic find and modify operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子查找和修改操作
- en: Implementing atomic counters in Mongo
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中实现原子计数器
- en: Implementing server-side scripts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现服务器端脚本
- en: Creating and tailing a capped collection cursors in MongoDB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MongoDB中创建和追踪封顶集合游标
- en: Converting a normal collection to capped collection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将普通集合转换为封顶集合
- en: Storing binary data in Mongo
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中存储二进制数据
- en: Storing large data in Mongo using GridFS
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GridFS在Mongo中存储大数据
- en: Storing data to GridFS from Java client
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Java客户端将数据存储到GridFS
- en: Storing data to GridFS from Python client
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Python客户端将数据存储到GridFS
- en: Implementing triggers in Mongo using oplog
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用oplog在Mongo中实现触发器
- en: Flat plane (2D) geospatial queries in Mongo using geospatial indexes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中使用平面（2D）地理空间索引进行查询
- en: Spherical indexes and GeoJSON compliant data in Mongo
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中使用球形索引和GeoJSON兼容数据
- en: Implementing full text search in Mongo
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mongo中实现全文搜索
- en: Integrating MongoDB for full text search with Elasticsearch
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将MongoDB集成到Elasticsearch进行全文搜索
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we saw how to perform basic operations
    from the shell to query, update, and insert documents, and also saw different
    types of indexes and index creation. In this chapter, we will see some of the
    advanced features of Mongo, such as GridFS, Geospatial Indexes, and Full text
    search. Other recipes we will see include an introduction and use of capped collections
    and implementing server-side scripts in MongoDB.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章。命令行操作和索引")中，*命令行操作和索引*，我们看到了如何从shell执行基本操作来查询、更新和插入文档，还看到了不同类型的索引和索引创建。在本章中，我们将看到Mongo的一些高级功能，如GridFS、地理空间索引和全文搜索。我们还将看到其他配方，包括封顶集合的介绍和使用以及在MongoDB中实现服务器端脚本。
- en: Atomic find and modify operations
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子查找和修改操作
- en: In [Chapter 2](ch02.html "Chapter 2. Command-line Operations and Indexes"),
    *Command-line Operations and Indexes*, we had some recipes that explained various
    CRUD operations we perform in MongoDB. There was one concept that we didn't cover
    and it is atomically find and modify documents. Modification consists of both
    update and delete operations. In this recipe, we will go through the basics of
    MongoDB's `findAndModify` operation. In the next recipe, we will use this method
    to implement a counter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章。命令行操作和索引")中，*命令行操作和索引*，我们有一些配方解释了我们在MongoDB中执行的各种CRUD操作。有一个概念我们没有涵盖到，那就是原子查找和修改文档。修改包括更新和删除操作。在这个配方中，我们将介绍MongoDB的`findAndModify`操作的基础知识。在下一个配方中，我们将使用这种方法来实现一个计数器。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. That is the only prerequisite
    for this recipe. Start a mongo shell and connect to the started server.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*和*安装和启动服务器*的配方，并启动MongoDB的单个实例。这是这个配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will test a document in `atomicOperationsTest` collection. Execute the following
    from the shell:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在`atomicOperationsTest`集合中测试一个文档。从shell执行以下操作：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Execute the following from the mongo shell and observe the output:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell执行以下操作并观察输出：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will execute another one this time but with slightly different parameters;
    observe the output for this operation:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们将执行另一个操作，但参数略有不同；观察此操作的输出：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will execute another update this time that would upsert the document as
    follows:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次我们将执行另一个更新，将会插入文档，如下所示：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, query the collection once as follows and see the documents present:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下方式查询集合并查看当前存在的文档：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will finally execute the delete as follows:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将按以下方式执行删除：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works…
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: If we perform find and update operations independently by first finding the
    document and then updating it in MongoDB, the results might not be as expected.
    There might be an interleaving update between the find and the update operations,
    which may have changed the document state. In some of the specific use cases,
    like implementing atomic counters, this is not acceptable and thus we need a way
    to atomically find, update, and return a document. The returned value is either
    the one before the update is applied or after the update is applied and is decided
    by the invoking client.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在MongoDB中首先查找文档，然后再更新它，结果可能不如预期。在查找和更新操作之间可能存在交错的更新，这可能已更改文档状态。在某些特定用例中，比如实现原子计数器，这是不可接受的，因此我们需要一种方法来原子地查找、更新和返回文档。返回的值是在更新应用之前或之后的值，由调用客户端决定。
- en: Now that we have executed the steps in the preceding section, let's see what
    we actually did and what all these fields in the JSON document passed as the parameter
    to the `findAndModify` operation mean. Starting with step 3, we gave a document
    as a parameter to the `findAndModify` function that contains the fields `query`,
    `update`, and `new`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经执行了前一节中的步骤，让我们看看我们实际做了什么，以及作为参数传递给`findAndModify`操作的JSON文档中的所有这些字段的含义。从第3步开始，我们将一个包含字段`query`、`update`和`new`的文档作为参数传递给`findAndModify`函数。
- en: The `query` field specifies the search parameters that would be used to find
    the document and the `update` field contains the modifications that need to be
    applied. The third field, new, if set to `true`, tells MongoDB to return the updated
    document.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`query`字段指定用于查找文档的搜索参数，`update`字段包含需要应用的修改。第三个字段`new`，如果设置为`true`，告诉MongoDB返回更新后的文档。'
- en: In step 4, we actually added a new field to the document passed as a parameter
    called **fields** that is used to select a limited set of fields from the result
    document returned. Also, the value of the field new is `true`, which tells that
    we want the updated document that is, the one after the update operation is executed
    and not the one before.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们实际上向作为参数传递的文档添加了一个名为**fields**的新字段，用于从返回的结果文档中选择一组有限的字段。此外，`new`字段的值为`true`，表示我们希望更新的文档，即在执行更新操作之后的文档，而不是之前的文档。
- en: In step 5 contains a new field called `upsert`, which upserts (update + insert)
    the document. That is, if the document with the given query is found, it is updated
    else a new one is created and updated. If the document didn't exist and an upsert
    happened, having the value of the parameter `new` as `false` will return `null`.
    This is because there was nothing present before the update operation was executed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步包含一个名为`upsert`的新字段，该字段执行upsert（更新+插入）文档。也就是说，如果找到具有给定查询的文档，则更新该文档，否则创建并更新一个新文档。如果文档不存在并且发生了upsert，那么将参数`new`的值设置为`false`将返回`null`。这是因为在执行更新操作之前没有任何内容存在。
- en: Finally, in step 7, instead of the `update` field, we used the `remove` field
    with the value `true` indicating that the document is to be removed. Also, the
    value of the new field is `false`, which means that we expect the document that
    got deleted.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第7步中，我们使用了`remove`字段，其值为`true`，表示要删除文档。此外，`new`字段的值为`false`，这意味着我们期望被删除的文档。
- en: See also
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: An interesting use case of atomic `FindandModify` operations is developing an
    atomic counter in Mongo. In our next recipe, we will see how to implement this
    use case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原子`FindandModify`操作的一个有趣的用例是在Mongo中开发原子计数器。在下一个配方中，我们将看到如何实现这个用例。
- en: Implementing atomic counters in Mongo
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo中实现原子计数器
- en: Atomic counters are a necessity for a large number of use cases. Mongo doesn't
    have a built in feature for atomic counters; nevertheless, it can be easily implemented
    using some of its cool offerings. In fact, with the help of previously described
    `findAndModify()` command, implementing is quite simple. Refer to the previous
    recipe *Atomic find and modify operations* to know what atomic find and modify
    operations are in Mongo.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原子计数器是许多用例的必需品。Mongo没有原子计数器的内置功能；然而，可以使用一些其很酷的功能很容易地实现。事实上，借助先前描述的`findAndModify()`命令，实现起来非常简单。参考之前的配方*原子查找和修改操作*，了解Mongo中的原子查找和修改操作是什么。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，开始Mongo的单个实例。这是此配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Execute the following piece of code from the mongo shell:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell中执行以下代码：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now from the shell invoke the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在从shell中调用以下命令：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works…
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: The function is as simple as a `findAndModify` operation on a collection used
    to store all the counters. The counter identifier is the `_id` field of the document
    stored and the value of the counter is stored in the field `count`. The document
    passed to the `findAndModify` operations accepts the query, which uniquely identifies
    the document storing the current count—a query using the `_id` field. The update
    operation is an `$inc` operation that will increment the value of the `count`
    field by 1\. But what if the document doesn't exist? This will happen on the first
    invocation of the counter. To take care of this scenario, we will set the `upsert`
    flag to `true`. The value of `count` will always start with 1 and there is no
    way it would accept any user-defined start number for the sequence or a custom
    increment step. To address such requirements, we will have to specifically add
    a document with the initialized values to the counters collection. Finally, we
    are interested in the state of the counter after the value is incremented; hence,
    we set the value of the field `new` as `true`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数就是在用于存储所有计数器的集合上执行的`findAndModify`操作。计数器标识符是存储的文档的`_id`字段，计数器的值存储在`count`字段中。传递给`findAndModify`操作的文档接受查询，该查询唯一标识存储当前计数的文档，即使用`_id`字段的查询。更新操作是一个`$inc`操作，将通过1递增`count`字段的值。但是如果文档不存在怎么办？这将发生在对计数器的第一次调用。为了处理这种情况，我们将将`upsert`标志设置为`true`。`count`的值将始终从1开始，没有办法接受任何用户定义的序列起始数字或自定义递增步长。为了满足这样的要求，我们将不得不将具有初始化值的文档添加到计数器集合中。最后，我们对计数器值递增后的状态感兴趣；因此，我们将`new`字段的值设置为`true`。
- en: 'On invoking this method thrice (as we did), we should see the following in
    the collection counters. Simply execute the following query:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用此方法三次（就像我们做的那样）后，我们应该在计数器集合中看到以下内容。只需执行以下查询：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Using this small function, we now have implemented atomic counters in Mongo.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个小函数，我们现在已经在Mongo中实现了原子计数器。
- en: See also
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We can store such common code on a Mongo server that would be available for
    execution in other functions. Look at the recipe *Implementing* *server-side scripts*
    to see how we can store JavaScript functions on the Mongo server. This allows
    us even to invoke this function from other programming language clients.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这样的通用代码存储在Mongo服务器上，以便在其他函数中执行。查看配方*实现服务器端脚本*，了解如何在Mongo服务器上存储JavaScript函数。这甚至允许我们从其他编程语言客户端调用此函数。
- en: Implementing server-side scripts
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现服务器端脚本
- en: In this recipe, we will see how to write server stored JavaScript similar to
    stored procedures in relational databases. This is a common use case where other
    pieces of code require access to these common functions and we have them in one
    central place. To demonstrate server-side scripts, the function will simply add
    two numbers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将看到如何编写服务器存储的JavaScript，类似于关系数据库中的存储过程。这是一个常见的用例，其他代码片段需要访问这些常见函数，我们将它们放在一个中心位置。为了演示服务器端脚本，该函数将简单地添加两个数字。
- en: There are two parts to this recipe. First, we see how to load the scripts from
    the collections on the client-side JavaScript shell and secondly, we will see
    how to execute these functions on the server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方有两个部分。首先，我们看看如何从客户端JavaScript shell中的集合加载脚本，其次，我们将看到如何在服务器上执行这些函数。
- en: Note
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The documentation specifically mentions that it is not recommended to use server-side
    scripts. Security is one concern though if the data is not properly audited and
    hence we need to be careful with what functions are defined. Since Mongo 2.4,
    the server-side JavaScript engine is V8, which can execute multiple threads in
    parallel as opposed to the engine prior to version 2.4 of Mongo, which executes
    only one thread at a time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 文档明确提到不建议使用服务器端脚本。安全性是一个问题，尽管如果数据没有得到适当的审计，因此我们需要小心定义哪些函数。自Mongo 2.4以来，服务器端JavaScript引擎是V8，可以并行执行多个线程，而不是Mongo
    2.4之前的引擎，每次只能执行一个线程。
- en: Getting ready
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。这是这个配方的唯一先决条件。启动一个mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create a new function called `add` and save it to the collection `db.system.js`
    as follows. The current database should be test:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`add`的新函数，并将其保存到集合`db.system.js`中，如下所示。当前数据库应该是test：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that this function is defined, load all the functions as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在这个函数已经定义，加载所有函数如下：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, invoke `add` and see if it works:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，调用`add`并查看是否有效：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will now use this function and execute this on the server-side instead:
    Execute the following from the shell:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用这个函数，并在服务器端执行它：从shell执行以下操作：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Execute the following steps (you can execute the preceding command):'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下步骤（可以执行前面的命令）：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works…
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The collection `system.js` is a special MongoDB collection used to store JavaScript
    code. We add a new server-side JavaScript using the `save` function in this collection.
    The `save` function is just a convenience function that inserts the document if
    it is not present or updates an existing one. The objective is to add a new document
    to this collection which you may add even using `insert` or `upsert`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 集合`system.js`是一个特殊的MongoDB集合，用于存储JavaScript代码。我们使用该集合中的`save`函数添加一个新的服务器端JavaScript。`save`函数只是一个方便的函数，如果文档不存在则插入文档，如果文档已存在则更新文档。目标是向该集合添加一个新文档，即使您可以使用`insert`或`upsert`来添加。
- en: 'The secret lies in the method `loadServerScripts`. Let''s look at the code
    of this method: `this.system.js.find().forEach(function(u){eval(u._id + " = "
    + u.value);});`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密在于`loadServerScripts`方法。让我们看看这个方法的代码：`this.system.js.find().forEach(function(u){eval(u._id
    + " = " + u.value);});`
- en: It evaluates a JavaScript using the `eval` function and assigns the function
    defined in the `value` attribute of the document to a variable named with the
    name given in the `_id` field of the document for each document present in the
    collection `system.js`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`eval`函数评估JavaScript，并为`system.js`集合中每个文档的`value`属性中定义的函数分配一个与文档的`_id`字段中给定的名称相同的变量。
- en: 'For example, if the following document is present in the collection `system.js`,
    `{ _id : ''add'', value : function(num1, num2) {return num1 + num2}}`, then the
    function given in the `value` field of the document will be assigned to the variable
    named as `add` in the current shell. The value `add` is given in the `_id` field
    of the document.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，如果集合`system.js`中存在以下文档，`{ _id : ''add'', value : function(num1, num2) {return
    num1 + num2}}`，那么文档的`value`字段中给定的函数将分配给当前shell中名为`add`的变量。文档的`_id`字段中给定了值`add`。'
- en: These scripts do not really execute on the server but their definition is stored
    on the server in a collection. The JavaScript method `loadServerScripts`, just
    instantiates some variables in the current shell and make those functions available
    for invocation. It is the JavaScript interpreter of the shell that executes these
    functions and not the server. The collection `system.js` is defined in the scope
    of the database. Once loaded, these act as JavaScript functions defined in the
    shell and hence the functions are available throughout the scope of the shell
    irrespective of the database currently active.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本实际上并不在服务器上执行，但它们的定义存储在服务器的一个集合中。JavaScript方法`loadServerScripts`只是在当前shell中实例化一些变量，并使这些函数可用于调用。执行这些函数的是shell的JavaScript解释器，而不是服务器。集合`system.js`在数据库的范围内定义。一旦加载，这些函数就像在shell中定义的JavaScript函数一样，在shell的范围内都是可用的，而不管当前活动的数据库是什么。
- en: As far as security is concerned, if the shell is connected to the server with
    security enabled, then the user invoking `loadServerScripts` must have privileges
    to read the collections in the database. For more details on enabling security
    and various roles a user can have, refer to the recipe *Setting up users in Mongo*
    in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*. As we
    saw earlier, the function `loadServerScripts` reads data from the collection `system.js`
    and if the user doesn't have privileges to read from the collection, the function
    invocation will fail. Apart from that, the functions executed from the shell after
    being loaded should have appropriate privileges. For instance, if a function inserts/updates
    in any collection, the user should have read and write privileges on that particular
    collection accessed from the function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就安全性而言，如果shell连接到启用了安全性的服务器，则调用`loadServerScripts`的用户必须具有读取数据库中集合的权限。有关启用安全性和用户可以拥有的各种角色的更多详细信息，请参阅[第4章](ch04.html
    "第4章。管理")中的食谱*在Mongo中设置用户*，*管理*。正如我们之前所看到的，`loadServerScripts`函数从`system.js`集合中读取数据，如果用户没有权限从该集合中读取数据，则函数调用将失败。除此之外，从加载后的shell中执行的函数应该具有适当的权限。例如，如果函数在任何集合中插入/更新数据，则用户应该对从函数访问的特定集合具有读取和写入权限。
- en: Executing scripts on the server is perhaps what one would expect to be server-side
    script as opposed to executing in the shell connected. In this case, the functions
    are evaluated on the server's JavaScript engine and the security checks are more
    stringent as long running functions can hold locks, having detrimental effects
    on the performance. The wrapper to invoke the execution of a JavaScript code on
    the server-side is the `db.eval` function accepting the code to evaluate on the
    server-side along with the parameters if any.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器上执行脚本可能是人们期望的服务器端脚本，而不是在连接的shell中执行。在这种情况下，函数在服务器的JavaScript引擎上进行评估，安全检查更为严格，因为长时间运行的函数可能会持有锁，对性能产生不利影响。在服务器端调用JavaScript代码执行的包装器是`db.eval`函数，接受要在服务器端评估的代码以及参数（如果有）。
- en: Before evaluating the function, the write operation takes a global lock; this
    can be skipped if the parameter `nolock` is used. For instance, the preceding
    `add` function can be invoked as follows instead of calling `db.eval` and achieving
    the same results. We additionally provided the `nolock` field to instruct the
    server not to acquire the global lock before evaluating the function. If this
    function were to perform write operations on a collection, then the `nolock` field
    is ignored.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估函数之前，写操作会获取全局锁；如果使用参数`nolock`，则可以跳过这一步。例如，可以按照以下方式调用前面的`add`函数，而不是调用`db.eval`并获得相同的结果。我们另外提供了`nolock`字段，指示服务器在评估函数之前不要获取全局锁。如果此函数要在集合上执行写操作，则`nolock`字段将被忽略。
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If security is enabled on the server, the invoking user needs to have the following
    four roles: `userAdminAnyDatabase`, `dbAdminAnyDatabase`, `readWriteAnyDatabase`,
    and `clusterAdmin` (on the admin database) to successfully invoke the `db.eval`
    function.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务器启用了安全性，则调用用户需要具有以下四个角色：`userAdminAnyDatabase`、`dbAdminAnyDatabase`、`readWriteAnyDatabase`和`clusterAdmin`（在管理数据库上）才能成功调用`db.eval`函数。
- en: Programming languages do provide a way for invocation of such server-side scripts
    using the `eval` function. For instance, in Java API, the class `com.mongodb.DB`
    has the method `eval` to invoke server-side JavaScript code. Such server-side
    executions are highly useful when we want to avoid unnecessary network traffic
    for the data and get the result to the clients. However, too much logic on the
    database server can quickly make things difficult to maintain and affect the performance
    of the server badly.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 编程语言确实提供了一种调用这种服务器端脚本的方法，使用`eval`函数。例如，在Java API中，类`com.mongodb.DB`有一个方法`eval`来调用服务器端的JavaScript代码。当我们想要避免数据不必要的网络流量并将结果传递给客户端时，这种服务器端执行非常有用。然而，在数据库服务器上有太多的逻辑可能会很快使事情难以维护，并严重影响服务器的性能。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: As of MongoDB 3.0.3, the `db.eval()` method is being deprecated and it is advised
    that users do not rely on this method but instead use client-side scripts. See
    [https://jira.mongodb.org/browse/SERVER-17453](https://jira.mongodb.org/browse/SERVER-17453)
    for more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 截至MongoDB 3.0.3，`db.eval()`方法已被弃用，建议用户不要依赖该方法，而是使用客户端脚本。有关更多详细信息，请参阅[https://jira.mongodb.org/browse/SERVER-17453](https://jira.mongodb.org/browse/SERVER-17453)。
- en: Creating and tailing a capped collection cursors in MongoDB
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MongoDB中创建和追踪固定大小集合的游标
- en: Capped collections are fixed size collections where documents are added towards
    the end of the collection, similar to a queue. As capped collection have a fixed
    size, older documents are removed if the limit is reached.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 固定大小的集合是固定大小的集合，其中文档被添加到集合的末尾，类似于队列。由于固定大小的集合有一个固定的大小，如果达到限制，旧的文档将被删除。
- en: They are naturally sorted by the order of the insertion and any retrieval needed
    on them required ordered by time can be retrieved using the `$natural` sort order.
    This makes document retrieval very fast.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它们按插入顺序自然排序，任何需要按时间顺序检索的检索都可以使用`$natural`排序顺序进行检索。这使得文档检索非常快速。
- en: The following figure gives a pictorial representation of a capped collection
    of a size which is good enough to hold up to three documents of equal size (which
    is too small for any practical use, but good for understanding). As we can see
    in the image, the collection is similar to a circular queue where the oldest document
    is replaced by the newly added document should the collection become full. The
    tailable cursors are special types of cursors that tail the collection similar
    to a tail command in Unix. These cursors iterate through the collection similar
    to a normal cursors do, but additionally wait for data to be available in the
    collection if it is not available. We will see capped collections and tailable
    cursors in detail in this recipe.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图给出了一个有限大小的集合的图形表示，足以容纳最多三个相等大小的文档（对于任何实际用途来说都太小，但用于理解是很好的）。正如我们在图像中所看到的，该集合类似于循环队列，其中最旧的文档将被新添加的文档替换，如果集合变满。可追加的游标是特殊类型的游标，类似于Unix中的tail命令，它们遍历集合，类似于普通游标，但同时等待集合中的数据是否可用。我们将在本节详细介绍有限集合和可追加游标。
- en: '![Creating and tailing a capped collection cursors in MongoDB](img/B04831_05_01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![在MongoDB中创建和追加有限集合游标](img/B04831_05_01.jpg)'
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* recipe in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a MongoDB shell and connect to the started server.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。这是本配方的唯一先决条件。启动MongoDB
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤...
- en: 'There are two parts to this recipe: in the first part, we will create a capped
    collection called `testCapped` and try performing some basic operations on it.
    Next, we will be creating a tailable cursor on this capped collection.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方有两个部分：在第一部分中，我们将创建一个名为`testCapped`的有限集合，并尝试对其执行一些基本操作。接下来，我们将在这个有限集合上创建一个可追加游标。
- en: Drop the collection if one already exists with this name.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果已存在具有此名称的集合，请删除该集合。
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now create a capped collection as follows. Note the size given here is the
    size in bytes allocated for the collection and not the number of documents it
    contains:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按以下方式创建一个有限集合。请注意，此处给定的大小是为集合分配的字节数，而不是它包含的文档数量：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will now insert 100 documents in the capped collection as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将按以下方式在有限集合中插入100个文档：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now query the collection as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按以下方式查询集合：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Try to remove the data from the collection as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试按以下方式从集合中删除数据：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will now create and demonstrate a tailable cursor. It is recommended that
    you type/copy the following pieces of code into a text editor and keep it handy
    for execution.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建并演示一个可追加游标。建议您将以下代码片段输入/复制到文本编辑器中，并随时准备执行。
- en: 'To insert data in a collection, we will be using the following fragment of
    code. Execute this piece of code in the shell:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在集合中插入数据，我们将使用以下代码片段。在shell中执行此代码片段：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To tail a capped collection, we use the following piece of code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要追加有限集合，我们使用以下代码片段：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Open a shell and connect to the running mongod process. This will be the second
    shell opened and connected to the server. Copy and paste the code mentioned in
    step 8 in this shell and execute it.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个shell并连接到正在运行的mongod进程。这将是第二个打开并连接到服务器的shell。在此shell中复制并粘贴第8步中提到的代码，然后执行它。
- en: Observe how the records inserted are shown as they are inserted into the capped
    collection.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察插入的记录如何显示为它们插入到有限集合中。
- en: How it works…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We will create a capped collection explicitly using the `createCollection` function.
    This is the only way a capped collection is created. There are two parameters
    to the `createCollection` function. The first one is the name of the collection
    and the second is a JSON document that contains the two fields, `capped` and `size`,
    which are used to inform the user that the collection is capped or not and the
    size of the collection in bytes respectively. An additional field `max` can be
    provided to specify the maximum number of documents in the collection. The field
    size is required even if the `max` field is specified. We then insert and query
    the documents. When we try to remove the documents from the collection, we would
    see an error that removal is not permitted from the capped collection. It allows
    the documents to be deleted only when new documents are added and there isn't
    space available to accommodate them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`createCollection`函数显式创建一个有限集合。这是创建有限集合的唯一方法。`createCollection`函数有两个参数。第一个是集合的名称，第二个是一个包含两个字段`capped`和`size`的JSON文档，用于通知用户集合是否被限制以及集合的大小（以字节为单位）。还可以提供一个额外的`max`字段来指定集合中的最大文档数。即使指定了`max`字段，也需要`size`字段。然后我们插入和查询文档。当我们尝试从集合中删除文档时，我们会看到一个错误，即不允许从有限集合中删除文档。它只允许在添加新文档并且没有空间可容纳它们时才能删除文档。
- en: What we see next is a tailable cursor we created. We start two shells and one
    of them is a normal insertion of documents with an interval of 1 second between
    subsequent insertions. In the second shell, we create a cursor and iterate through
    it and print the documents that we get from the cursor onto the shell. The additional
    options we added to the cursor make the difference though. There are two options
    added, `DBQuery.Option.tailable` and `DBQuery.Option.awaitData`. These options
    are for instructing that the cursor is tailable, rather than normal, where the
    last position is marked and we can resume where we left off, and secondly to wait
    for more data for some time rather than returning immediately when no data is
    available and when we reach towards the end of the cursor, respectively. The `awaitData`
    option can be used with tailable cursors only. The combination of these two options
    gives us a feel similar to the tail command in Unix filesystem.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们看到的是我们创建的可追溯游标。我们启动两个shell，其中一个是以1秒的间隔插入文档的普通插入。在第二个shell中，我们创建一个游标并遍历它，并将从游标获取的文档打印到shell上。然而，我们添加到游标的附加选项使得有所不同。添加了两个选项，`DBQuery.Option.tailable`和`DBQuery.Option.awaitData`。这些选项用于指示游标是可追溯的，而不是正常的，其中最后的位置被标记，我们可以恢复到上次离开的位置，其次是在没有数据可用时等待更多数据一段时间，以及当我们接近游标的末尾时立即返回而不是返回。`awaitData`选项只能用于可追溯游标。这两个选项的组合使我们感觉类似于Unix文件系统中的tail命令。
- en: 'For a list of available options, visit the following page: [http://docs.mongodb.org/manual/reference/method/cursor.addOption/](http://docs.mongodb.org/manual/reference/method/cursor.addOption/).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可用选项的列表，请访问以下页面：[http://docs.mongodb.org/manual/reference/method/cursor.addOption/](http://docs.mongodb.org/manual/reference/method/cursor.addOption/)。
- en: There's more…
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In the next recipe, we will see how to convert a normal collection to a capped
    collection.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将看到如何将普通集合转换为固定集合。
- en: Converting a normal collection to a capped collection
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将普通集合转换为固定集合
- en: This recipe will demonstrate the process of converting a normal collection to
    a capped collection.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方将演示将普通集合转换为固定集合的过程。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a mongo shell and connect to the started server.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的*安装单节点MongoDB*和*安装和启动服务器*的配方，并启动Mongo的单个实例。这是本配方的唯一先决条件。启动mongo
    shell并连接到已启动的服务器。
- en: How to do it…
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Execute the following to ensure you are in the `test` database:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下操作以确保您在`test`数据库中：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a normal collection as follows. We will be adding 100 documents to it,
    type/copy the following code snippet on to the mongo shell and execute it. The
    command is as follows:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式创建一个普通集合。我们将向其添加100个文档，将以下代码片段输入/复制到mongo shell上并执行。命令如下：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Query the collection as follows to confirm it contains the data:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式查询集合以确认其中包含数据：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, query the collection `system.namespaces` as follows and note the result
    document:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，按照以下方式查询集合`system.namespaces`，并注意结果文档：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Execute the following command to convert the collection to capped collection:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令将集合转换为固定集合：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Query the collection to take a look at the data:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询集合以查看数据：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Query the collection `system.namespaces` as follows and note the result document:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式查询集合`system.namespaces`，并注意结果文档：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works…
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We created a normal collection with 100 documents and then tried to convert
    it to a capped collection with 100 bytes size. The command has the following JSON
    document passed to the `runCommand` function, `{convertToCapped : <name of normal
    collection>, size: <size in bytes of the capped collection>}`. This command creates
    a capped collection with the mentioned size and loads the documents in natural
    ordering from the normal collection to the target capped collection. If the size
    of the capped collection reaches the limit mentioned, the old documents are removed
    in the FIFO order making space for new documents. Once this is done, the created
    capped collection is renamed. Executing a find on the capped collection confirms
    that not all 100 documents originally present in the normal collection are present
    in the capped collection. A query on the `system.namespaces` collection before
    and after the execution of the `convertToCapped` command shows the change in the
    `collection` attributes. Note that, this operation acquires a global write lock
    blocking all read and write operations in this database. Also, any indexes present
    on the original collection are not created for the capped collection, upon conversion.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们创建了一个包含100个文档的普通集合，然后尝试将其转换为具有100字节大小的固定集合。命令将以下JSON文档传递给`runCommand`函数，`{convertToCapped:
    <普通集合的名称>, size: <固定集合的字节大小>}`。此命令创建一个具有指定大小的固定集合，并将文档以自然顺序从普通集合加载到目标固定集合中。如果固定集合的大小达到所述限制，旧文档将以FIFO顺序删除，为新文档腾出空间。完成后，创建的固定集合将被重命名。在固定集合上执行查找确认，最初在普通集合中存在的100个文档并不都存在于固定集合中。在执行`convertToCapped`命令之前和之后对`system.namespaces`集合进行查询，显示了`collection`属性的变化。请注意，此操作获取全局写锁，阻止此数据库中的所有读取和写入操作。此外，对于转换后的固定集合，不会创建原始集合上存在的任何索引。'
- en: There's more…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Oplog is an important collection used for replication in MongoDB and is a capped
    collection. For more information on replication and oplogs, refer to the recipe
    *Understanding and analyzing oplogs* in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. In a recipe later in this chapter, we will use this oplog to
    implement a feature similar to after insert/update/delete trigger of a relational
    database.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Oplog是MongoDB中用于复制的重要集合，是一个有上限的集合。有关复制和oplogs的更多信息，请参阅[第4章](ch04.html "第4章.管理")中的*理解和分析oplogs*，*管理*中的配方。在本章的后面的一个配方中，我们将使用这个oplog来实现类似于关系数据库中的插入/更新/删除触发器的功能。
- en: Storing binary data in Mongo
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo中存储二进制数据
- en: So far, we saw how to store text values, dates, and numbers fields in a document.
    Binary content also needs to be stored at times in the database. Consider cases
    where users would need to store files in a database. In relational databases,
    the BLOB data type is most commonly used to address this requirement. MongoDB
    also supports binary contents to be stored in a document in the collection. The
    catch is that the total size of the document shouldn't exceed 16 MB, which is
    the upper limit of the document size as of the writing this book. In this recipe,
    we will store a small image file into Mongo's document and also retrieve it later.
    If the content you wish to store in MongoDB collections is greater than 16 MB,
    then MongoDB offers an out of the box solution called **GridFS**. We will see
    how to use GridFS in another recipe later in this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到了如何在文档中存储文本值、日期和数字字段。有时还需要在数据库中存储二进制内容。考虑用户需要在数据库中存储文件的情况。在关系数据库中，BLOB数据类型最常用于满足这一需求。MongoDB也支持将二进制内容存储在集合中的文档中。问题在于文档的总大小不应超过16MB，这是写作本书时文档大小的上限。在这个配方中，我们将把一个小图像文件存储到Mongo的文档中，并在以后检索它。如果您希望存储在MongoDB集合中的内容大于16MB，则MongoDB提供了一个名为**GridFS**的开箱即用的解决方案。我们将在本章的另一个配方中看到如何使用GridFS。
- en: Getting ready
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of MongoDB. Also, the program to write binary
    content to the document is written in Java. Refer to the recipes *Executing query
    and insert operations using a Java client*, *Implementing aggregation in Mongo
    using a Java client* and *Executing MapReduce in Mongo using a Java client* in
    [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*, for more details on Java drivers. Open a mongo shell and connect
    to the local MongoDB instance listening to port `27017`. For this recipe, we will
    be using the project `mongo-cookbook-bindata`. This project is available in the
    source code bundle downloadable from Packt site. The folder needs to be extracted
    on the local filesystem. Open a command line shell and go to the root of the project
    extracted. It should be the directory where the file `pom.xml` is found.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章.安装和启动服务器")中的*安装单节点MongoDB*配方，*安装和启动服务器*并启动MongoDB的单个实例。还有一个用于将二进制内容写入文档的程序是用Java编写的。有关Java驱动程序的更多详细信息，请参阅[第3章](ch03.html
    "第3章.编程语言驱动程序")中的*使用Java客户端执行查询和插入操作*，*使用Java客户端实现Mongo中的聚合*和*使用Java客户端在Mongo中执行MapReduce*的配方，*编程语言驱动程序*。打开一个mongo
    shell并连接到监听端口`27017`的本地MongoDB实例。对于这个配方，我们将使用项目`mongo-cookbook-bindata`。这个项目可以从Packt网站下载的源代码包中获取。需要在本地文件系统上提取文件夹。打开一个命令行shell并转到提取的项目的根目录。应该是找到文件`pom.xml`的目录。
- en: How to do it…
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'On the operating system shell with the `pom.xml` present in the current directory
    of the `mongo-cookbook-bindata` project, execute the following command:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在操作系统shell中，`mongo-cookbook-bindata`项目的当前目录中存在`pom.xml`，执行以下命令：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Observe the output; the execution should be successful.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察输出；执行应该成功。
- en: 'Switch to mongo shell that is connected to the local instance and execute the
    following query:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到连接到本地实例的mongo shell并执行以下查询：
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Scroll through the document and take a note of the fields in the document.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动文档并记下文档中的字段。
- en: How it works…
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: If we scroll through the large document printed out, we see that the fields
    are `fileName`, `size`, and `data`. The first two fields are of type string and
    number respectively, which we populated on document creation and hold the name
    of the file we provide and the size in bytes. The data field is a field of BSON
    type BinData, where we see the data encoded in Base64 format.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们滚动查看打印出的大型文档，我们会看到字段`fileName`，`size`和`data`。前两个字段分别是字符串和数字类型，我们在文档创建时填充了这些字段，并保存了我们提供的文件名和以字节为单位的大小。数据字段是BSON类型BinData的字段，我们在其中看到数据以Base64格式编码。
- en: 'The following lines of code show how we populated the DBObject that we added
    to the collection:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行显示了我们如何填充添加到集合中的DBObject：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we see above, two fields `fileName` and `size` are used to store the name
    of the file and the size of the file and are of type string and number respectively.
    The field data is added to the `DBObject` as a byte array, it gets stored automatically
    as the BSON type BinData in the document.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，使用两个字段`fileName`和`size`来存储文件名和文件大小，分别为字符串和数字类型。数据字段作为字节数组添加到`DBObject`中，它会自动存储为文档中的BSON类型BinData。
- en: See also
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: What we saw in this recipe is straightforward as long as the document size is
    less than 16 MB. If the size of the files stored exceeds this value, we have to
    resort to solutions like GridFS, which is explained in next recipe *Storing large
    data in Mongo using GridFS*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们看到的是直接的，只要文档大小小于16MB。如果存储的文件大小超过这个值，我们必须求助于像GridFS这样的解决方案，这在下一个配方*使用GridFS在Mongo中存储大数据*中有解释。
- en: Storing large data in Mongo using GridFS
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GridFS在Mongo中存储大数据
- en: A document size in MongoDB can be up to 16 MB. But does that mean we cannot
    store data more than 16 MB in size? There are cases where you prefer to store
    videos and audio files in database rather than in a filesystem for a number of
    advantages such as a few of them are storing metadata along with them, when accessing
    the file from an intermediate location, and replicating the contents for high
    availability if replication is enabled on the MongoDB server instances. GridFS
    can be used to address such use cases in MongoDB. We will also see how GridFS
    manages large content that exceeds 16 MB and analyzes the collections it uses
    for storing the content behind the scene. For test purpose, we will not use data
    exceeding 16 MB but something smaller to see GridFS in action.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB中的文档大小可以达到16 MB。 但这是否意味着我们不能存储超过16 MB大小的数据？ 有些情况下，您更喜欢将视频和音频文件存储在数据库中，而不是在文件系统中，因为有许多优势，比如存储与它们一起的元数据，从中间位置访问文件时，以及在MongoDB服务器实例上启用复制时为了高可用性而复制内容。
    GridFS可以用来解决MongoDB中的这些用例。 我们还将看到GridFS如何管理超过16 MB的大容量，并分析其用于在幕后存储内容的集合。 为了测试目的，我们不会使用超过16
    MB的数据，而是使用一些更小的数据来查看GridFS的运行情况。
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Look at the recipe *Installing single node MongoDB* in [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server* and start a single instance of Mongo. That is the only prerequisite for
    this recipe. Start a Mongo shell and connect to the started server. Additionally,
    we will use the mongofiles utility to store data in GridFS from command line.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[第1章](ch01.html "第1章。安装和启动服务器")中的配方*安装单节点MongoDB*，*安装和启动服务器*并启动Mongo的单个实例。
    这是此配方的唯一先决条件。 启动Mongo shell并连接到已启动的服务器。 另外，我们将使用mongofiles实用程序从命令行将数据存储在GridFS中。
- en: How to do it…
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: Download the code bundle of the book and save the image file `glimpse_of_universe-wide.jpg`
    to your local drive (you may choose any other large file as the matter of fact
    and provide appropriate names of the file with the commands we execute). For the
    sake of the example, the image is saved in the home directory. We will split our
    steps into three parts.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载该书的代码包，并将图像文件`glimpse_of_universe-wide.jpg`保存到本地驱动器（您可以选择任何其他大文件作为事实，并使用我们执行的命令提供适当的文件名）。
    为了举例，图像保存在主目录中。 我们将把我们的步骤分为三个部分。
- en: With the server up and running, execute the following command from the operating
    system's shell with the current directory being the home directory. There are
    two arguments here. The first one is the name of the file on the local filesystem
    and the second one is the name that would be attached to the uploaded content
    in MongoDB.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务器运行并且当前目录为主目录的情况下，从操作系统的shell中执行以下命令。 这里有两个参数。 第一个是本地文件系统上文件的名称，第二个是将附加到MongoDB中上传内容的名称。
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let's now query the collections to see how this content is actually stored in
    the collections behind the scenes. With the shell open, execute the following
    two queries. Make sure that in the second query, you ensure to mention not selecting
    the data field.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们查询集合，看看这些内容实际上是如何在幕后的集合中存储的。 打开shell，执行以下两个查询。 确保在第二个查询中，您确保不选择数据字段。
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now that we have put a file to GridFS from the operating system''s local filesystem,
    we will see how we can get the file to the local filesystem. Execute the following
    from the operating system shell:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经从操作系统的本地文件系统中将文件放入了GridFS，我们将看到如何将文件获取到本地文件系统。 从操作系统shell中执行以下操作：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we will delete the file we uploaded as follows. From the operating
    system shell, execute the following:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将删除我们上传的文件。 从操作系统shell中，执行以下操作：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Confirm the deletion using the following queries again:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用以下查询确认删除：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Mongo distribution comes with a tool called mongofiles, which lets us upload
    the large content to Mongo server that gets stored using the GridFS specification.
    GridFS is not a different product but a specification that is standard and followed
    by different drivers for MongoDB for storing data greater than 16 MB, which is
    the maximum document size. It can even be used for files less than 16 MB, as we
    did in our recipe, but there isn't really a good reason to do that. There is nothing
    stopping us from implementing our own way of storing these large files, but it
    is preferred to follow the standard. This is because all drivers support it and
    does the heavy lifting of splitting of big file into small chunks and assembling
    them back when needed.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo分发包带有一个名为mongofiles的工具，它允许我们将大容量上传到Mongo服务器，该服务器使用GridFS规范进行存储。 GridFS不是一个不同的产品，而是一个标准规范，由不同的MongoDB驱动程序遵循，用于存储大于16
    MB的数据，这是最大文档大小。 它甚至可以用于小于16 MB的文件，就像我们在我们的示例中所做的那样，但实际上没有一个很好的理由这样做。 没有什么能阻止我们实现自己的存储这些大文件的方式，但最好遵循标准。
    这是因为所有驱动程序都支持它，并且在需要时进行大文件的分割和组装。
- en: We kept the image downloaded from the Packt Publishing site and uploaded using
    mongofiles to MongoDB. The command to do that is `put` and the `-l` option gives
    the name of the file on the local drive that we want to upload. Finally, the name
    `universe.jpg` is the name of the file we want it to be stored as on GridFS.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Packt Publishing网站下载的图像，并使用mongofiles上传到MongoDB。 执行此操作的命令是`put`，`-l`选项给出了我们要上传的本地驱动器上的文件的名称。
    最后，名称`universe.jpg`是我们希望它在GridFS上存储的文件的名称。
- en: 'On successful execution, we should see something like the following on the
    console:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行后，我们应该在控制台上看到以下内容：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This gives us some details of the upload, the unique `_id` for the uploaded
    file, the name of the file, the chunk size, which is the size of the chunk this
    big file is broken into (by default 256 KB), the date of upload, the checksum
    of the uploaded content, and the total length of upload. This checksum can be
    computed beforehand and then compared after the upload to check if the uploaded
    content was not corrupt.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一些上传的细节，上传文件的唯一`_id`，文件的名称，块大小，这是这个大文件被分成的块的大小（默认为256 KB），上传日期，上传内容的校验和以及上传的总长度。这个校验和可以事先计算，然后在上传后进行比较，以检查上传的内容是否损坏。
- en: 'Execute the following query from the mongo shell in test database:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据库的mongo shell中执行以下查询：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We see that the output we saw for the `put` command of mongofiles same as the
    document queried above in the collection `fs.files`. This is the collection where
    all the uploaded file details are put when some data is added to GridFS. There
    will be one document per upload. Applications can later also modify this document
    to add their own custom meta data along with the standard details added to my
    Mongo when adding the data. Applications can very well use this collection to
    add details like, the photographer, the location where the image was taken, where
    was it taken, and details like tags for individuals in the image in this collection
    if the document is for an image upload.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们在`mongofiles`的`put`命令中看到的输出与上面在`fs.files`集合中查询的文档相同。这是当向GridFS添加数据时，所有上传的文件细节都会放在这个集合中。每次上传都会有一个文档。应用程序以后还可以修改此文档，以添加自己的自定义元数据以及在添加数据时添加到我的Mongo的标准细节。如果文档是用于图像上传，应用程序可以很好地使用此集合来添加诸如摄影师、图像拍摄地点、拍摄地点以及图像中个人的标签等细节。
- en: 'The file content is something that contains this data. Let''s execute the following
    query:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 文件内容是包含这些数据的内容。让我们执行以下查询：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We have deliberately left out the data field from the result selected. Let''s
    look at the structure of the result document:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意从所选结果中省略了数据字段。让我们看一下结果文档的结构：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For the file we uploaded, we have 11 chunks of a maximum 256 KB each. When a
    file is being requested, the `fs.chunks` collection is searched by the `file_id`
    that comes from the `_id` field of `fs.files` collection and the field `n`, which
    is the chunk's sequence. A unique index is created on these two fields when this
    collection is created for the first time when a file is uploaded using GridFS
    for the fast retrieval of chunks using the file ID sorted by chunk sequence number.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们上传的文件，我们有11个最大为256 KB的块。当请求文件时，`fs.chunks`集合通过来自`fs.files`集合的`_id`字段的`file_id`和字段`n`（块的序列）进行搜索。当第一次使用GridFS上传文件时，为了快速检索使用文件ID按块序列号排序的块，这两个字段上创建了唯一索引。
- en: Similar to `put`, the `get` option is used to retrieve the files from the GridFS
    and put them on local filesystem. The difference in the command is to use the
    `get` instead of `put`, the `-l` still is used to provide the name of the file
    that this file would be saved as on the local filesystem and the final command
    line parameter is the name of the file as stored in GridFS. This is the value
    of the `filename` field in `fs.files` collection. Finally, the `delete` command
    of mongofiles simply removes the entry of the file from `fs.files` and `fs.chunks`
    collections. The name of the file given for delete is again the value present
    in the `filename` field of the `fs.files` collection.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与`put`类似，`get`选项用于从GridFS检索文件并将其放在本地文件系统上。命令的不同之处在于使用`get`而不是`put`，`-l`仍然用于提供此文件在本地文件系统上保存的名称，最后的命令行参数是GridFS中存储的文件的名称。这是`fs.files`集合中`filename`字段的值。最后，`mongofiles`的`delete`命令简单地从`fs.files`和`fs.chunks`集合中删除文件的条目。删除的文件名再次是`fs.files`集合中`filename`字段中的值。
- en: Some important use cases of using GridFS are when there is some user generated
    contents like large reports on some static data that doesn't change too often
    and are expensive to generate frequently. Instead of running them all the times,
    it can be run once and stored until a change in the static data is detected; in
    which case, the stored report is deleted and re-executed on next request of the
    data. The filesystem may not always be available to the application to write the
    files to, in which case this is a good alternative. There are cases where one
    might be interested in some intermediate chunk of the data stored, in which case
    the chunk containing the required data be accessed. You get some nice features
    like the MD5 content of the data, which is stored automatically and is available
    for use by the application.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GridFS的一些重要用例是当存在一些用户生成的内容，比如一些静态数据上的大型报告，这些数据不经常更改，而且频繁生成成本很高。与其每次都运行它们，不如运行一次并存储，直到检测到静态数据的更改；在这种情况下，存储的报告将被删除，并在下一次请求数据时重新执行。文件系统可能并不总是可用于应用程序写入文件，这种情况下这是一个很好的替代方案。有些情况下，人们可能对存储的一些中间数据块感兴趣，这种情况下可以访问包含所需数据的数据块。您可以获得一些不错的功能，比如数据的MD5内容，它会自动存储并可供应用程序使用。
- en: Now that we have seen what GridFS is, let's see some scenarios where using GridFS
    might not be a very good idea. The performance of accessing the content from MongoDB
    using GridFS and directly from the filesystem will not be same. Direct filesystem
    access will be faster than GridFS and **Proof of Concept** (**POC**) for the system
    to be developed is recommended to measure the performance hit and see if it is
    within the acceptable limits; if so, the trade off in performance might be worth
    for the benefits we get. Also, if your application server is fronted with CDN,
    you might not actually need a lot of IO for static data stored in GridFS. Since
    GridFS stores the data in multiple documents in collections, atomically updating
    them is not possible. If we know the content is less than 16 MB, which is the
    case in lot of user-generated content, or some small files uploaded, we may skip
    GridFS altogether and store the content in one document as BSON supports storing
    binary content in the document. Refer to the previous recipe *Storing binary data
    in Mongo* for more details.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了GridFS是什么，让我们看看在哪些情况下使用GridFS可能不是一个很好的主意。通过GridFS从MongoDB访问内容的性能和直接从文件系统访问的性能不会相同。直接文件系统访问将比GridFS更快，建议对要开发的系统进行**概念验证**（**POC**）以测量性能损失，并查看是否在可接受的范围内；如果是，那么性能上的折衷可能是值得的。此外，如果您的应用服务器前端使用CDN，您实际上可能不需要在GridFS中存储静态数据的大量IO。由于GridFS将数据存储在多个集合中的多个文档中，因此无法以原子方式更新它们。如果我们知道内容小于16MB，这在许多用户生成的内容中是情况，或者上传了一些小文件，我们可以完全跳过GridFS，并将内容存储在一个文档中，因为BSON支持在文档中存储二进制内容。有关更多详细信息，请参考上一个教程*在Mongo中存储二进制数据*。
- en: We would rarely use mongofiles utility to store, retrieve, and delete data from
    GridFS. Though it may occasionally be used, we will mostly perform these operations
    from an application. In the next couple of recipes, we will see how to connect
    to GridFS to store, retrieve, and delete files using Java and Python clients.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很少使用mongofiles实用程序来从GridFS存储、检索和删除数据。虽然偶尔可能会使用它，但我们大多数情况下会从应用程序执行这些操作。在接下来的几个教程中，我们将看到如何连接到GridFS，使用Java和Python客户端存储、检索和删除文件。
- en: There's more…
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Though this is not much to do with Mongo, Openstack is an **Infrastructure
    as a Service** (**IaaS**) platform and offers a variety of services for Compute,
    Storage, Networking, and so on. One of the image storage service called **Glance**
    supports a lot of persistent stores to store the images. One of the supported
    stores by Glace is MongoDB''s GridFS. You can find more information on how to
    configure Glance to use GridFS at the following URL: [http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html](http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这与Mongo不太相关，但Openstack是一个**基础设施即服务**（**IaaS**）平台，提供各种计算、存储、网络等服务。其中一个名为**Glance**的镜像存储服务支持许多持久存储来存储图像。Glance支持的存储之一是MongoDB的GridFS。您可以在以下网址找到有关如何配置Glance使用GridFS的更多信息：[http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html](http://docs.openstack.org/trunk/config-reference/content/ch_configuring-openstack-image-service.html)。
- en: See also
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can refer to the following recipes:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下教程：
- en: '*Storing data to GridFS from Java client*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从Java客户端将数据存储到GridFS*'
- en: '*Storing data to GridFS from Python client*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Python客户端将数据存储到GridFS
- en: Storing data to GridFS from Java client
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Java客户端将数据存储到GridFS
- en: 'In the previous recipe, we saw how to store data to GridFS using a command-line
    utility that comes with MongoDB to manage large data files: mongofiles. To get
    an idea of what GridFS is and what collections are used behind the scenes to store
    the data, refer to the previous recipe *Storing large data in Mongo using GridFS*.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个教程中，我们看到了如何使用MongoDB自带的命令行实用程序mongofiles来将数据存储到GridFS，以管理大型数据文件。要了解GridFS是什么，以及在幕后用于存储数据的集合，请参考上一个教程*在Mongo中使用GridFS存储大型数据*。
- en: In this recipe, we will look at storing data to GridFS using a Java client.
    The program will be a highly scaled down version of mongofiles utility and focus
    only on how to store, retrieve, and delete data rather than trying to provide
    a lot of options like mongofiles do.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将看看如何使用Java客户端将数据存储到GridFS。该程序将是mongofiles实用程序的一个大大简化版本，只关注如何存储、检索和删除数据，而不是试图提供像mongofiles那样的许多选项。
- en: Getting ready
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Installing single node MongoDB* from [Chapter 1](ch01.html
    "Chapter 1. Installing and Starting the Server"), *Installing and Starting the
    Server*, for all the necessary setup for this recipe. If you are interested in
    more details on Java drivers, refer to the recipes *Implementing aggregation in
    Mongo using a Java client* and *Executing MapReduce in Mongo using a Java client*
    in [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers*. Open a mongo shell and connect to the local mongod instance
    listening to port `27017`. For this recipe, we will be using the project `mongo-cookbook-gridfs`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a terminal of your
    operating system and go to the root of the project extracted. It should be the
    directory where the file `pom.xml` is found. Also, save the file `glimpse_of_universe-wide.jpg`
    on the local filesystem, similar to the previous recipe, found in the downloadable
    bundle for the book from the Packt site.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本教程所需的所有必要设置，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的教程*安装单节点MongoDB*，*安装和启动服务器*。如果您对Java驱动程序有更多详细信息感兴趣，请参考[第3章](ch03.html
    "第3章。编程语言驱动程序")中的教程*使用Java客户端在Mongo中实现聚合*和*使用Java客户端在Mongo中执行MapReduce*。打开一个mongo
    shell并连接到监听端口`27017`的本地mongod实例。对于本教程，我们将使用项目`mongo-cookbook-gridfs`。该项目可在Packt网站上提供的源代码包中找到。需要在本地文件系统上提取该文件夹。打开操作系统的终端并转到提取的项目的根目录。这应该是找到文件`pom.xml`的目录。还要像上一个教程一样，在本地文件系统上保存文件`glimpse_of_universe-wide.jpg`，该文件可以在Packt网站上提供的书籍可下载包中找到。
- en: How to do it…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: We are assuming that the collections of GridFS are clean and no prior data is
    uploaded. If there is nothing crucial in the database, you can execute the following
    to clear the collection. Do exercise caution before dropping the collections.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们假设GridFS的集合是干净的，没有先前上传的数据。如果数据库中没有重要数据，您可以执行以下操作来清除集合。在删除集合之前，请小心行事。
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Open an operating system shell and execute the following:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开操作系统shell并执行以下操作：
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The file I need to upload was placed in the home directory. You can choose to
    give the file path of the image file after the `put` command. Bear in mind if
    the path contains spaces, the whole path need to be given within single quotes.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我需要上传的文件放在主目录中。在`put`命令之后，您可以选择给出图像文件的文件路径。请记住，如果路径中包含空格，则整个路径需要在单引号内给出。
- en: 'If the preceding command runs successfully, we should expect the following
    output to the command line:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果前面的命令成功运行，我们应该期望在命令行输出以下内容：
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the preceding execution is successful, which we can confirm from the console
    output, execute the following from the mongo shell:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦前面的执行成功，我们可以从控制台输出确认，然后从mongo shell执行以下操作：
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we will get the file from GridFS to local filesystem, execute the following
    to perform this operation:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从GridFS获取文件到本地文件系统，执行以下操作来执行此操作：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Confirm the file is present on the local filesystem at the mentioned location.
    We should see the following printed to the console output to indicate a successful
    write operation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 确认文件是否存在于所述位置的本地文件系统上。我们应该看到以下内容打印到控制台输出，以指示成功的写操作：
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we will delete the file from GridFS:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将从GridFS中删除文件：
- en: '[PRE47]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'On successful deletion, we should see the following output in the console:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功删除后，我们应该在控制台中看到以下输出：
- en: '[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How it works…
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The class `com.packtpub.mongo.cookbook.GridFSTests` accepts three types of
    operations: `put` to upload file to GridFS, `get` to get contents from GridFS
    to local filesystem, and `delete` to delete files from GridFS.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 类`com.packtpub.mongo.cookbook.GridFSTests`接受三种类型的操作：`put`将文件上传到GridFS，`get`从GridFS获取内容到本地文件系统，`delete`从GridFS删除文件。
- en: The class accepts up to three parameters, the first one is the operation with
    valid values as `get`, `put`, and `delete`. The second parameter is relevant for
    `get` and `put` operations and is the name of the file on local filesystem to
    write the downloaded content to be written or source the content from for upload
    respectively. The third parameter is the name of the file in GridFS, which is
    not necessarily same as the name on local filesystem. For `delete`, however, only
    the filename on GridFS is needed which would be deleted.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该类最多接受三个参数，第一个是操作，有效值为`get`，`put`和`delete`。第二个参数与`get`和`put`操作相关，是本地文件系统上要写入下载内容的文件的名称，或者用于上传的内容的源。第三个参数是GridFS中的文件名，不一定与本地文件系统上的文件名相同。但是，对于`delete`，只需要GridFS上的文件名，该文件将被删除。
- en: Let's see some important snippets of code from the class which is specific to
    GridFS.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下该类中与GridFS特定的一些重要代码片段。
- en: Open the class `com.packtpub.mongo.cookbook.GridFSTests` in your favorite IDE
    and look for the methods `handlePut` , `handleGet`, and `handleDelete`. These
    are the methods where all the logic is. We will start with the `handlePut` method
    first, which is for uploading the contents of the file from local filesystem to
    GridFS.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在您喜欢的IDE中打开类`com.packtpub.mongo.cookbook.GridFSTests`，查找方法`handlePut`，`handleGet`和`handleDelete`。这些方法是所有逻辑的地方。我们将首先从`handlePut`方法开始，该方法用于将文件内容从本地文件系统上传到GridFS。
- en: 'Irrespective of the operation we perform, we will create an instance of the
    class `com.mongodb.gridfs.GridFS`. In our case, we instantiated it as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们执行什么操作，我们都将创建`com.mongodb.gridfs.GridFS`类的实例。在我们的情况下，我们将其实例化如下：
- en: '[PRE49]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The constructor of this class takes the database instance of class `com.mongodb.DB`.
    Once the instance of GridFS is created, we will invoke the method `createFile`
    on it. This method accepts two parameters, the first one is the `InputStream`
    sourcing the bytes of the content to be uploaded and the second parameter is the
    name of the file on GridFS for the file that would be saved on GridFS. However,
    this method doesn't create the file on GridFS but returns and instance of `com.mongodb.gridfs.GridFSInputFile`.
    The upload will happen only when we call `save` method in this returned object.
    There are few overloaded variants of this `createFile` method. Please refer to
    Javadocs of the class `com.mongodb.gridfs.GridFS` for more details.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的构造函数接受`com.mongodb.DB`类的数据库实例。创建GridFS实例后，我们将调用其上的`createFile`方法。此方法接受两个参数，第一个是`InputStream`，用于提供要上传的内容的字节，第二个参数是GridFS上的文件名，该文件将保存在GridFS上。但是，此方法不会在GridFS上创建文件，而是返回`com.mongodb.gridfs.GridFSInputFile`的实例。只有在调用此返回对象中的`save`方法时，上传才会发生。此`createFile`方法有几个重载的变体。有关更多详细信息，请参阅`com.mongodb.gridfs.GridFS`类的Javadocs。
- en: Our next method is `handleGet`, which gets the contents of the file saved on
    GridFS to the local filesystem. Similar to the `com.mongodb.DBCollection` class,
    the class `com.mongodb.gridfs.GridFS` has the `find` and `findOne` methods for
    searching. However, instead of accepting any DBObject query, `find` and `findOne`
    in GridFS accept filename or the ObjectID value of the document to search in `fs.files`
    collection. Similarly, the return value is not a DBCursor but an instance of `com.mongodb.gridfs.GridFSDBFile`.
    This class has various methods that get the `InputStream` of the bytes of content
    present in the file on GridFS, `writeTo` file or `OutputStream` and a method,
    `getLength` that gives the number of bytes in the file. Refer to the Javadocs
    of the class `com.mongodb.gridfs.GridFSDBFile` for details.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个方法是`handleGet`，它从GridFS上保存的文件中获取内容到本地文件系统。与`com.mongodb.DBCollection`类似，`com.mongodb.gridfs.GridFS`类具有用于搜索的`find`和`findOne`方法。但是，与接受任何DBObject查询不同，GridFS中的`find`和`findOne`接受文件名或要在`fs.files`集合中搜索的文档的ObjectID值。同样，返回值不是DBCursor，而是`com.mongodb.gridfs.GridFSDBFile`的实例。该类具有各种方法，用于获取GridFS文件中存在的内容的字节的`InputStream`，将文件或`OutputStream`写入文件的方法`writeTo`，以及一个`getLength`方法，用于获取文件中的字节数。有关详细信息，请参阅`com.mongodb.gridfs.GridFSDBFile`类的Javadocs。
- en: 'Finally, we look at the method `handleDelete`, which is used to delete the
    files on GridFS and is the simplest of the lot. The method on the object of GridFS
    is `remove`, which accepts a string argument: the name of the file to delete on
    the server. The `return` type of this method is `void`. So irrespective of whether
    the content is present on GridFS or not, the method will not return a value nor
    throw an exception if a name is provided to this method for a file that doesn''t
    exist.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看看`handleDelete`方法，它用于删除GridFS上的文件，是最简单的方法。GridFS对象上的方法是`remove`，它接受一个字符串参数：要在服务器上删除的文件的名称。此方法的`return`类型是`void`。因此，无论GridFS上是否存在内容，如果为此方法提供了一个不存在的文件的名称，该方法都不会返回值，也不会抛出异常。
- en: See also
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can refer to the following recipes:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下配方：
- en: '*Storing binary data in Mongo*'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Mongo中存储二进制数据*'
- en: '*Storing data to GridFS from Python client*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从Python客户端将数据存储到GridFS*'
- en: Storing data to GridFS from Python client
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Python客户端将数据存储到GridFS
- en: In the recipe *Storing large data in Mongo using GridFS*, we saw what GridFS
    is and how it could be used to store the large files in MongoDB. In the previous
    recipe, we saw to use GridFS API from a Java client. In this recipe, we will see
    how to store image data into MongoDB using GridFS from a Python program.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在配方*使用GridFS在Mongo中存储大数据*中，我们看到了GridFS是什么，以及如何使用它来在MongoDB中存储大文件。在上一个配方中，我们看到了如何从Java客户端使用GridFS
    API。在这个配方中，我们将看到如何使用Python程序将图像数据存储到MongoDB中的GridFS。
- en: Getting ready
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好
- en: 'Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more detail on Python drivers refer to the following recipes:
    *Executing query and insert operations with PyMongo* and *Executing update and
    delete operations using PyMongo* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Download and save the image
    `glimpse_of_universe-wide.jpg` from the downloadable bundle available with the
    book from the Packt site to local filesystem as we did in the previous recipe.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本配方的所有必要设置，请参考[第1章](ch01.html "第1章。安装和启动服务器")中的配方*使用Java客户端连接到单个节点*，*安装和启动服务器*。如果您对Python驱动程序的更多详细信息感兴趣，请参考以下配方：*使用PyMongo执行查询和插入操作*和*使用PyMongo执行更新和删除操作*在[第3章](ch03.html
    "第3章。编程语言驱动程序")中，*编程语言驱动程序*。从Packt网站的可下载捆绑包中下载并保存图像`glimpse_of_universe-wide.jpg`到本地文件系统，就像我们在上一个配方中所做的那样。
- en: How to do it…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Open a Python interpreter by typing in the following in the operating system
    shell. Note that the current directory is same as the directory where the image
    file `glimpse_of_universe-wide.jpg` is placed:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在操作系统shell中输入以下内容来打开Python解释器。请注意，当前目录与放置图像文件`glimpse_of_universe-wide.jpg`的目录相同：
- en: '[PRE50]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import the required packages as follows:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式导入所需的包：
- en: '[PRE51]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Once the Python shell is opened, create a `MongoClient` and a database object
    to the test database as follows:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦打开了Python shell，就按如下方式创建`MongoClient`和数据库对象到测试数据库：
- en: '[PRE52]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To clear the GridFS-related collections execute the following:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要清除与GridFS相关的集合，请执行以下操作：
- en: '[PRE53]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create the instance of GridFS as follows:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建GridFS的实例如下：
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we will read the file and upload its contents to GridFS. First, create
    the file object as follows:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将读取文件并将其内容上传到GridFS。首先，按如下方式创建文件对象：
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now put the file into GridFS as follows
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按如下方式将文件放入GridFS
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: On successfully executing `put`, we should see the ObjectID for the file uploaded.
    This would be same as the `_id` field of the `fs.files` collection for this file.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功执行`put`后，我们应该看到上传文件的ObjectID。这将与此文件的`fs.files`集合的`_id`字段相同。
- en: Execute the following query from the Python shell. It should print out the `dict`
    object with the details of the upload. Verify the contents
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Python shell执行以下查询。它应该打印出包含上传详细信息的`dict`对象。验证内容
- en: '[PRE57]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we will get the uploaded content and write it to a file on the local filesystem.
    Let''s get the `GridOut` instance representing the object to read the data out
    of GridFS as follows:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将获取上传的内容并将其写入本地文件系统中的文件。让我们获取表示要从GridFS中读取数据的`GridOut`实例，如下所示：
- en: '[PRE58]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'With this instance available, let''s write the data to the file to a file on
    local filesystem as follows. First, open a handle to the file on local filesystem
    to write to as follows:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这个实例，让我们按如下方式将数据写入本地文件系统中的文件。首先，按如下方式打开本地文件系统上的文件句柄以进行写入：
- en: '[PRE59]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We will then write content to it as follows:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将按如下方式向其写入内容：
- en: '[PRE60]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now verify the file on the current directory on the local filesystem. A new
    file called `universe.jpg` will be created with same number of bytes as the source
    present in it. Verify it by opening it in an image viewer.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在本地文件系统的当前目录上验证文件。将创建一个名为`universe.jpg`的新文件，其字节数与源文件相同。通过在图像查看器中打开它来进行验证。
- en: How it works…
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Let's look at the steps we executed. In the Python shell, we import two packages,
    `pymongo` and `gridfs`, and instantiate the `pymongo.MongoClient` and `gridfs.GridFS`
    instances. The constructor of the class `gridfs.GridFS` takes on an argument,
    which is the instance of `pymongo.Database`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们执行的步骤。在Python shell中，我们导入了两个包，`pymongo`和`gridfs`，并实例化了`pymongo.MongoClient`和`gridfs.GridFS`实例。类`gridfs.GridFS`的构造函数接受一个参数，即`pymongo.Database`的实例。
- en: We open a file in binary mode using the `open` function and pass the file object
    to the GridFS `put` method. There is an additional argument called `filename`
    passed, which would be the name of the file put into GridFS. The first parameter
    need not be a file object but any object with a `read` method defined.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`open`函数以二进制模式打开文件，并将文件对象传递给GridFS的`put`方法。还传递了一个名为`filename`的额外参数，这将是放入GridFS的文件的名称。第一个参数不需要是文件对象，而是任何定义了`read`方法的对象。
- en: Once the `put` operation succeeds, the `return` value is an ObjectID for the
    uploaded document in `fs.files` collection. A query on `fs.files` can confirm
    that the file is uploaded. Verify that the size of the data uploaded matches the
    size of the file.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`put`操作成功，`return`值就是`fs.files`集合中上传文档的ObjectID。对`fs.files`的查询可以确认文件已上传。验证上传的数据大小是否与文件大小匹配。
- en: Our next objective is to get the file from GridFS on to the local filesystem.
    Intuitively, one would imagine if the method to put a file in GridFS is `put`,
    then the method to get a file would be `get`. True, the method is indeed `get`,
    however, it will get only based on the `ObjectId` that was returned by the `put`
    method. So, if you are okay to fetch by `ObjectId`, `get` is the method for you.
    However, if you want to get by the filename, the method to use is `get_last_version`.
    It accepts the name of the filename that we uploaded and the return type of this
    method is of type `gridfs.gridfs_file.GridOut`. This class contains the method
    `read`, which will read out all the bytes from the uploaded file to GridFS. We
    open a file called `universe.jpg` for writing in binary mode and write all the
    bytes read from the `GridOut` object.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个目标是将文件从GridFS获取到本地文件系统。直觉上，人们会想象如果将文件放入GridFS的方法是`put`，那么获取文件的方法将是`get`。确实，该方法的确是`get`，但是它只会基于`put`方法返回的`ObjectId`进行获取。因此，如果您愿意按`ObjectId`获取，`get`就是您的方法。但是，如果您想按文件名获取，要使用的方法是`get_last_version`。它接受我们上传的文件名，并且此方法的返回类型是`gridfs.gridfs_file.GridOut`类型。该类包含`read`方法，它将从GridFS中上传的文件中读取所有字节。我们以二进制模式打开一个名为`universe.jpg`的文件进行写入，并将从`GridOut`对象中读取的所有字节写入其中。
- en: See also
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can refer to the following recipes:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下配方：
- en: '*Storing binary data in Mongo*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Mongo中存储二进制数据*'
- en: '*Storing data to GridFS from Java client*'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从Java客户端将数据存储到GridFS*'
- en: Implementing triggers in Mongo using oplog
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用oplog在Mongo中实现触发器
- en: In a relational database, a trigger is a code that gets invoked when an `insert`,
    `update`, or a `delete` operation is executed on a table in the database. A trigger
    can be invoked either before or after the operation. Triggers are not implemented
    in MongoDB out of the box and in case you need some sort of notification for your
    application whenever any `insert`/`update`/`delete` operations are executed, you
    are left to manage that by yourself in the application. One approach is to have
    some sort of data access layer in the application, which is the only place to
    query, insert, update, or delete documents from the collections. However, there
    are few challenges to it. First, you need to explicitly code the logic to accommodate
    this requirement in the application, which may or may not be feasible. If the
    database is shared and multiple applications access it, things become even more
    difficult. Secondly, the access needs to be strictly regulated and no other source
    of insert/update/delete be permitted.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库中，触发器是在数据库表上执行`insert`、`update`或`delete`操作时被调用的代码。触发器可以在操作之前或之后被调用。MongoDB中并没有内置实现触发器，如果您需要在应用程序中任何`insert`/`update`/`delete`操作执行时得到通知，您需要自己在应用程序中管理。一种方法是在应用程序中有一种数据访问层，这是唯一可以从集合中查询、插入、更新或删除文档的地方。但是，这也存在一些挑战。首先，您需要在应用程序中明确编写逻辑以满足此要求，这可能是可行的，也可能是不可行的。如果数据库是共享的，并且多个应用程序访问它，事情会变得更加困难。其次，访问需要严格管理，不允许其他来源的插入/更新/删除。
- en: Alternatively, we need to look at running some sort of logic in a layer close
    to the database. One way to track all write operations is by using an oplog. Note
    that read operations cannot be tracked using oplogs. In this recipe, we will write
    a small Java application that would tail an oplog and get all the `insert`, `update`
    and `delete` operations happening on a Mongo instance. Note that this program
    is implemented in Java and works equally well in any other programming language.
    The crux lies in the logic for the implementation, the platform for implementation
    can be any. Also, this works only if the mongod instance is started as a part
    of replica set and not a standalone instance. Also, this trigger like functionality
    can only be invoked only after the operation is performed and not before the data
    gets inserted/updated or deleted from the collection.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们需要考虑在靠近数据库的层中运行某种逻辑。跟踪所有写操作的一种方法是使用oplog。请注意，无法使用oplog跟踪读操作。在本配方中，我们将编写一个小型的Java应用程序，该应用程序将尾随oplog并获取在Mongo实例上发生的所有`insert`、`update`和`delete`操作。请注意，此程序是用Java实现的，并且在任何其他编程语言中同样有效。关键在于实现的逻辑，实现的平台可以是任何。此外，只有在mongod实例作为副本集的一部分启动时，此触发器类功能才能被调用，而不是在数据被插入/更新或从集合中删除之前。
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Starting multiple instances as part of a replica set* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. If you
    are interested in more details on Java drivers, refer to the following recipes
    *Executing query and insert operations using a Java client* and *Executing update
    and delete operations using a Java client* in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*. Prerequisites of these two
    recipes are all we need for this recipe.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此示例的所有必要设置，请参考[第1章](ch01.html "第1章。安装和启动服务器")中的示例*作为副本集的一部分启动多个实例*，*安装和启动服务器*。如果您对Java驱动程序的更多细节感兴趣，请参考[第3章](ch03.html
    "第3章。编程语言驱动程序")中的以下示例*使用Java客户端执行查询和插入操作*和*使用Java客户端执行更新和删除操作*。这两个示例的先决条件是我们这个示例所需要的一切。
- en: Refer to the recipe *Creating and tailing a capped collection cursors in MongoDB*
    in this chapter to know more about capped collections and tailable cursors if
    you are not aware or need a refresher. Finally, though not mandatory, [Chapter
    4](ch04.html "Chapter 4. Administration"), *Administration*, explains oplog in
    depth in the recipe *Understanding and analyzing oplogs*. This recipe will not
    explain oplog in depth as we did in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*. Open a shell and connect it to the primary of the replica set.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不了解或需要复习，请参考本章中的示例*在MongoDB中创建和跟踪封顶集合游标*，了解有关封顶集合和可跟踪游标的更多信息。最后，尽管不是强制性的，[第4章](ch04.html
    "第4章。管理")中的*管理*解释了oplog的深度，解释了*理解和分析oplog*中的oplog。这个示例不会像我们在[第4章](ch04.html "第4章。管理")中所做的那样深入解释oplog。打开一个shell并将其连接到副本集的主服务器。
- en: For this recipe, we will be using the project `mongo-cookbook-oplogtrigger`.
    This project is available in the source code bundle downloadable from Packt site.
    The folder needs to be extracted on the local filesystem. Open a command line
    shell and go to the root of the project extracted. It should be the directory
    where the file `pom.xml` is found. Also, the `TriggerOperations.js` file would
    be needed to trigger operations in the database that we intend to capture.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用项目`mongo-cookbook-oplogtrigger`。该项目可以从Packt网站下载的源代码包中获取。需要在本地文件系统上提取文件夹。打开命令行shell并转到提取的项目的根目录。这应该是找到文件`pom.xml`的目录。还需要`TriggerOperations.js`文件来触发我们打算捕获的数据库中的操作。
- en: How to do it…
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'Open an operating system shell and execute the following:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开操作系统shell并执行以下操作：
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'With the Java program started, we will open the shell as follows with the file
    `TriggerOperations.js` present in the current directory and the mongod instance
    listening to port `27000` as the primary:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Java程序启动后，我们将打开shell，当前目录中存在文件`TriggerOperations.js`，mongod实例监听端口`27000`作为主服务器：
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once the shell is connected, execute the following function we loaded from
    the JavaScript:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到shell后，执行我们从JavaScript中加载的以下函数：
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Observe the output printed out on the console where the Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is being executed using Maven.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察在控制台上打印出的输出，Java程序`com.packtpub.mongo.cookbook.OplogTrigger`正在使用Maven执行。
- en: How it works…
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: The functionality we implemented is pretty handy for a lot of use cases but
    let's see what we did at a higher level first. The Java program `com.packtpub.mongo.cookbook.OplogTrigger`
    is something that acts as a trigger when new data is inserted, updated, or deleted
    from a collection in MongoDB. It uses oplog collection that is the backbone of
    the replication in Mongo to implement this functionality.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的功能对于许多用例非常方便，但首先让我们看一下更高层次上做了什么。Java程序`com.packtpub.mongo.cookbook.OplogTrigger`是一个在MongoDB中插入、更新或删除集合中的新数据时触发的东西。它使用oplog集合，这是Mongo中复制的支柱，来实现这个功能。
- en: The JavaScript we have just acts as a source of producing, updating, and deleting
    data from the collection. You may choose to open the `TriggerOperations.js` file
    and take a look at how it is implemented. The collection on which it performs
    is present in the test database and is called `oplogTriggerTest`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚编写的JavaScript作为一个数据的生产、更新和删除的源。您可以选择打开`TriggerOperations.js`文件，看一下它是如何实现的。它执行的集合位于测试数据库中，称为`oplogTriggerTest`。
- en: 'When we execute the JavaScript function, we should see something like the following
    printed to the output console:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行JavaScript函数时，应该看到类似以下内容打印到输出控制台：
- en: '[PRE64]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The Maven program will be continuously running and never terminate as the Java
    program doesn't. You may hit *Ctrl* + *C* to stop the execution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Maven程序将持续运行，永远不会终止，因为Java程序不会。您可以按*Ctrl* + *C*停止执行。
- en: 'Let''s analyze the Java program, which is where the meat of the content is.
    The first assumption is that for this program to work, a replica set must be set
    up as we will use Mongo''s oplog collection. The Java programs created a connection
    to the primary of the replica set members, connects to the local database, and
    gets the `oplog.rs` collection. Then, all it does is find the last or nearly the
    last timestamp in the oplog. This is done to prevent the whole oplog to be replayed
    on startup but to mark a point towards the end in the oplog. Here is the code
    to find this timestamp value:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下Java程序，这是内容的核心所在。首先假设这个程序要工作，必须设置一个副本集，因为我们将使用Mongo的oplog集合。Java程序创建了一个连接到副本集成员的主服务器，连接到本地数据库，并获取了`oplog.rs`集合。然后，它所做的就是找到oplog中的最后一个或几乎最后一个时间戳。这样做是为了防止在启动时重放整个oplog，而是标记oplog末尾的一个点。以下是找到这个时间戳值的代码：
- en: '[PRE65]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The oplog is sorted in the reverse natural order to find the time in the last
    document in it. Since oplogs follow the first in first out pattern, sorting the
    oplog in the descending natural order is equivalent to sorting by the timestamp
    in descending order.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: oplog按照自然逆序排序，以找到其中最后一个文档中的时间。由于oplog遵循先进先出模式，将oplog按降序自然顺序排序等同于按时间戳降序排序。
- en: 'Once this is done, finding the timestamp as before, we query the oplog collection
    as usual but with two additional options:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，像以前一样找到时间戳，我们通常查询操作日志集合，但增加了两个额外的选项：
- en: '[PRE66]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The query finds all documents greater than a particular timestamp and adds two
    options, `Bytes.QUERYOPTION_TAILABLE` and `Bytes.QUERYOPTION_AWAITDATA`. The latter
    option can only be added when the former option is added. This not only queries
    and returns the data, but also waits for some time when the execution reaches
    the end of the cursor for some more data. Eventually, when no data arrives, it
    terminates.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 查询找到所有大于特定时间戳的文档，并添加两个选项，`Bytes.QUERYOPTION_TAILABLE`和`Bytes.QUERYOPTION_AWAITDATA`。只有在添加前一个选项时才能添加后一个选项。这不仅查询并返回数据，还在执行到游标末尾时等待一段时间以获取更多数据。最终，当没有数据到达时，它终止。
- en: During every iteration, store the last seen timestamp as well. This is used
    when the cursor closes when no more data is available and we query again to get
    a new tailable cursor instance. The query this time will use the timestamp we
    have stored on previous iteration, when the last document was seen. This process
    continues indefinitely and we basically tail the collection in a similar way to
    how we tail a file in Unix using the `tail` command.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代期间，还要存储上次看到的时间戳。当游标关闭且没有更多数据可用时，我们再次查询以获取新的可追溯游标实例时会使用这个时间戳。这个过程将无限期地继续下去，基本上我们以类似于在Unix中使用`tail`命令追踪文件的方式追踪集合。
- en: The oplog document contains a field called `op` for the operation whose value
    is `i`, `u`, and `d` for insert, update, and delete, respectively. The field `o`
    contains the inserted or deleted object's ID (`_id`) in case of insert and delete.
    In case of update, the file `o2` contains the `_id`. All we do is simply check
    for these conditions and print out the operation and the ID of the document inserted/deleted
    or updated.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 操作日志文档包含一个名为`op`的字段，其值为`i`，`u`和`d`，分别表示插入，更新和删除的操作。字段`o`包含插入或删除对象的ID（`_id`）（在插入和删除的情况下）。在更新的情况下，文件`o2`包含`_id`。我们所做的就是简单地检查这些条件，并打印出插入/删除或更新的操作和文档的ID。
- en: Something to be careful about is as follows. Obviously, the deleted documents
    would not be available in the collection so, the `_id` would not really be useful
    if you intend to query. Also, be careful when selecting a document after update
    using the ID we get as some other operation later in the oplog might already have
    performed more updates on the same document and our application's tailable cursor
    has yet to reach that point. This is common in case of high-volume systems. Similarly,
    for inserts we have a similar problem. The document we might query using the provided
    ID might be updated/deleted already. Applications using this logic to track these
    operations must be aware of them.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些需要注意的事情如下。显然，已删除的文档在集合中将不可用，因此，如果您打算进行查询，`_id`将不会真正有用。此外，在使用我们获得的ID更新后选择文档时要小心，因为操作日志中的某些其他操作可能已经对同一文档执行了更多的更新，而我们应用程序的可追溯游标尚未达到那一点。这在高容量系统中很常见。同样，对于插入，我们也有类似的问题。我们可能使用提供的ID查询的文档可能已经被更新/删除。使用此逻辑跟踪这些操作的应用程序必须意识到这些问题。
- en: Alternatively, take a look at the oplog that contains more details. Like the
    document inserted, the `update` statement executed, and so on. Updates in the
    oplog collection are idempotent, which means they can be applied any number of
    times without unintended side effects. For instance, if the actual update was
    to increment the value by 1, the update in the oplog collection will have the
    `set` operator with the final value to be expected. This way, the same update
    can be applied multiple times. The logic you would use would then have to be more
    sophisticated to implement such scenarios.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，查看包含更多详细信息的操作日志。比如插入的文档，执行的`update`语句等。操作日志集合中的更新是幂等的，这意味着它们可以应用任意次数而不会产生意外的副作用。例如，如果实际的更新是将值增加1，那么操作日志集合中的更新将具有`set`运算符，并且最终值将被期望。这样，相同的更新可以应用多次。然后，您将使用的逻辑必须更复杂，以实现这样的情况。
- en: Also, failovers are not handled here. This is needed for production based systems.
    The infinite loop on the other hand opens a new cursor as soon as the first one
    terminates. There could be a sleep duration introduced before the oplog is queried
    again to avoid overwhelming the server with queries. Note that the program given
    here is not a production quality code but just a simple demo of the technique
    that is being used by a lot of other systems to get notified for new data insert,
    delete, and updates to collections in MongoDB.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这里没有处理故障转移。这对于基于生产的系统是必要的。另一方面，无限循环在第一个游标终止时立即打开一个新的游标。在再次查询操作日志之前，可以引入一个睡眠持续时间，以避免用查询过度压倒服务器。请注意，此处提供的程序不是生产质量的代码，而只是使用了许多其他系统用于获取有关MongoDB中集合的新数据插入，删除和更新的通知技术的简单演示。
- en: MongoDB didn't have the text search feature until version 2.4 and prior to that
    all full text search was handled using external search engines like Solr or Elasticsearch.
    Even now, though the text search feature in MongoDB is production ready, many
    would still use an external dedicated search indexer. It won't be a surprise if
    the decision is taken to use an external full text index search tool instead of
    leveraging the MongoDB's inbuilt one. In case of Elasticsearch, the abstraction
    to flow the data in to the indexes is known as a river. The MongoDB river in Elasticsearch,
    which adds data to the indexes as and when the data gets added to the collections
    in Mongo is built on the same logic as we saw in the simple program implemented
    in Java.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB直到2.4版本之前都没有文本搜索功能，之前所有的全文搜索都是使用Solr或Elasticsearch等外部搜索引擎处理的。即使现在，尽管MongoDB中的文本搜索功能已经可以投入生产使用，许多人仍然会使用外部专用的搜索索引器。如果决定使用外部全文索引搜索工具而不是利用MongoDB内置的工具，这也不足为奇。在Elasticsearch中，将数据流入索引的抽象称为“river”。Elasticsearch中的MongoDB
    river会在Mongo中的集合添加数据时将数据添加到索引中，其构建逻辑与我们在Java中实现的简单程序中看到的逻辑相同。
- en: Flat plane 2D geospatial queries in Mongo using geospatial indexes
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用地理空间索引在Mongo中进行平面2D地理空间查询
- en: In this recipe, we will see what geospatial queries are and then see how to
    apply these queries on flat planes. We will put it to use in a test map application.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将看到什么是地理空间查询，然后看看如何在平面上应用这些查询。我们将在一个测试地图应用程序中使用它。
- en: Geospatial queries can be executed on data in which geospatial indexes are created.
    There are two types of geospatial indexes. The first one is called the 2D indexes
    and is the simpler of the two, it assumes that the data is given as *x,y* coordinates.
    The second one is called 3D or spherical indexes and is relatively more complicated.
    In this recipe, we will explore the 2D indexes and execute some queries on 2D
    data. The data on which we are going to work upon is a 25 x 25 grid with some
    coordinates representing bus stops, restaurants, hospitals, and gardens.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间查询可以在创建了地理空间索引的数据上执行。有两种类型的地理空间索引。第一种称为2D索引，是两者中较简单的一种，它假定数据以*x,y*坐标的形式给出。第二种称为3D或球面索引，相对更复杂。在这个配方中，我们将探索2D索引，并对2D数据执行一些查询。我们将要处理的数据是一个25
    x 25的网格，其中一些坐标表示公交车站、餐厅、医院和花园。
- en: '![Flat plane 2D geospatial queries in Mongo using geospatial indexes](img/B04831_05_02.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![使用地理空间索引在Mongo中进行平面2D地理空间查询](img/B04831_05_02.jpg)'
- en: Getting ready
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Connecting to the single node using a Java client* from
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server*, for all the necessary setup for this recipe. Download
    the data file `2dMapLegacyData.json` and keep it on the local filesystem ready
    to import. Open a mongo shell connecting to the local MongoDB instance.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此配方的所有必要设置，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的配方*使用Java客户端连接单个节点*，*安装和启动服务器*。下载数据文件`2dMapLegacyData.json`，并将其保存在本地文件系统上以备导入。打开一个连接到本地MongoDB实例的mongo
    shell。
- en: How to do it…
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Execute the following command from the operating system shell to import the
    data into the collection. The file `2dMapLegacyData.json` is present in the current
    directory.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从操作系统shell执行以下命令将数据导入到集合中。文件`2dMapLegacyData.json`位于当前目录中。
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'If we see something like the following on the screen, we can confirm that the
    import has gone through successfully:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在屏幕上看到类似以下内容，我们可以确认导入已成功进行：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'After the successful import, from the opened mongo shell, verify the collection
    and its content by executing the following query:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功导入后，从打开的mongo shell中，通过执行以下查询验证集合及其内容：
- en: '[PRE69]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This should give you the feel of the data in the collection.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该让你感受到集合中的数据。
- en: 'The next step is to create 2D geospatial index on this data. Execute the following
    to create a 2D index:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是在这些数据上创建2D地理空间索引。执行以下命令创建2D索引：
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: With the index created, we will now try to find the nearest restaurant from
    the place an individual is standing. Assuming the person is not fussy about the
    type of cuisine, let's execute the following query assuming that the person is
    standing at location (12, 8), as shown in the image. Also, we are interested in
    just three nearest places.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了索引后，我们现在将尝试找到离一个人所站的地方最近的餐厅。假设这个人对美食不挑剔，让我们执行以下查询，假设这个人站在位置(12, 8)，如图所示。此外，我们只对最近的三个地方感兴趣。
- en: '[PRE71]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This should give us three results, starting with the nearest restaurant with
    the subsequent ones given in increasing distance. If we look at the image given
    earlier, we kind of agree with the results given here.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该给我们三个结果，从最近的餐厅开始，随后的结果按距离递增给出。如果我们看一下之前给出的图像，我们可能会对这里给出的结果表示同意。
- en: 'Let''s add more options to the query. The individual has to walk and thus wants
    the distance to be restricted to a particular value in the results. Let''s rewrite
    the query with the following modification:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们给查询添加更多选项。个人需要步行，因此希望结果中的距离受到限制。让我们使用以下修改重新编写查询：
- en: '[PRE72]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Observe the number of results retrieved this time around.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察这次检索到的结果数量。
- en: How it works…
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Let''s now go through what we did. Before we continue, let''s define what exactly
    we mean by the distance between two points. Suppose on a cartesian plane that
    we have two points (x[1], y[1]) and (x[2], y[2]), the distance between them would
    be computed using the formula:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看我们做了什么。在继续之前，让我们定义一下两点之间的距离是什么意思。假设在笛卡尔平面上我们有两点(x[1], y[1])和(x[2], y[2])，它们之间的距离将使用以下公式计算：
- en: '*√(x[1] – x[2])² + (y[1] – y[2])²*'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '*√(x[1] – x[2])² + (y[1] – y[2])²*'
- en: 'Suppose the two points are (2, 10) and (12, 3), the distance would be: √(2
    – 12)² + (10 – 3)² = √(-10)² + (7)² = √149 =12.207.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 假设两点分别为(2, 10)和(12, 3)，距离将是：√(2 – 12)² + (10 – 3)² = √(-10)² + (7)² = √149 =12.207。
- en: After knowing how calculations for distance calculation are done behind the
    scenes by MongoDB, let's see what we did right from step 1.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了MongoDB在幕后如何进行距离计算的计算方法之后，让我们从第1步开始看看我们做了什么。
- en: We started by importing the data normally into a collection, `areaMap` in the
    `test` database and created an index as `db.areaMap.ensureIndex({co:'2d'})`. The
    index is created on the field `co` in the document and the value is a special
    value, `2d`, which denotes that this is a special type of index called 2D geospatial
    index. Usually, we give this value as `1` or `-1` in other cases denoting the
    order of the index.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据正常导入到`test`数据库中的一个集合`areaMap`中，并创建了一个索引`db.areaMap.ensureIndex({co:'2d'})`。索引是在文档中的字段`co`上创建的，其值是一个特殊值`2d`，表示这是一种特殊类型的索引，称为2D地理空间索引。通常，在其他情况下，我们会给出值`1`或`-1`，表示索引的顺序。
- en: There are two types of indexes. The first is a 2D index that is commonly used
    for planes whose span is less and do not involve spherical surfaces. It could
    be something like a map of the building, a locality, or even a small city where
    the curvature of the earth covering the portion of the land is not really significant.
    However, once the span of the map increases and covers the globe, 2D indexes will
    be inaccurate for predicting the values as the curvature of the earth needs to
    be considered in the calculations. In such cases, we go for spherical indexes,
    which we will discuss soon.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的索引。第一种是2D索引，通常用于跨度较小且不涉及球面的平面。它可能是建筑物的地图，一个地区，甚至是一个小城市，其中地球的曲率覆盖的土地部分并不真正重要。然而，一旦地图的跨度增加并覆盖全球，2D索引将不准确地预测值，因为需要考虑地球的曲率在计算中。在这种情况下，我们将使用球形索引，我们将很快讨论。
- en: 'Once the 2D index is created, we can use it to query the collection and find
    some points near the point queried. Execute the following query:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 创建2D索引后，我们可以使用它来查询集合并找到一些接近查询点的点。执行以下查询：
- en: '[PRE73]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'It will query for documents that are of the type R, which are of type `restaurants`
    and closes to the co-ordinates (12,8). The results returned by this query will
    be in the increasing order of the distance from the point in question, (12, 8)
    in this case. The limit just limits the result to top three documents. We may
    also provide the `$maxDistance` in the query, which will restrict the results
    with a distance less than or equal to the provided value. We queried for locations
    not more than four units away, as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 它将查询类型为R的文档，这些文档的类型是`restaurants`，并且接近坐标（12,8）。此查询返回的结果将按照与所查询点（在本例中为（12,8））的距离递增的顺序排列。限制只是将结果限制为前三个文档。我们还可以在查询中提供`$maxDistance`，它将限制距离小于或等于提供的值的结果。我们查询的位置不超过四个单位，如下所示：
- en: '[PRE74]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Spherical indexes and GeoJSON compliant data in Mongo
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mongo中的球形索引和GeoJSON兼容数据
- en: 'Before we continue with this recipe, we need to look at the previous recipe
    *Flat plane 2D geospatial queries in Mongo using geospatial indexes* to get an
    understanding of what geospatial indexes are in MongoDB and how to use the 2D
    indexes. So far, we have imported the JSON documents in a non-standard format
    in MongoDB collection, created geospatial indexes, and queried them it. This approach
    works perfectly fine and in fact, it was the only option available until MongoDB
    2.4\. version 2.4 of MongoDB supports an additional way to store, index, and query
    the documents in the collections. There is a standard way to represent geospatial
    data particularly meant for geodata exchange in JSON and the specification of
    GeoJSON mentions it in detail in the following link: [http://geojson.org/geojson-spec.html](http://geojson.org/geojson-spec.html).
    We can now store the data in this format.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本食谱之前，我们需要查看之前的食谱*使用地理空间索引在Mongo中进行平面2D地理空间查询*，以了解MongoDB中的地理空间索引是什么，以及如何使用2D索引。到目前为止，我们已经在MongoDB集合中以非标准格式导入了JSON文档，创建了地理空间索引，并对其进行了查询。这种方法完全有效，实际上，直到MongoDB
    2.4版本之前，这是唯一可用的选项。MongoDB 2.4版本支持一种额外的方式来存储、索引和查询集合中的文档。有一种标准的方式来表示地理空间数据，特别是用于JSON中的地理数据交换，并且GeoJSON的规范在以下链接中详细说明：[http://geojson.org/geojson-spec.html](http://geojson.org/geojson-spec.html)。我们现在可以以这种格式存储数据。
- en: There are various geographic figure types supported by this specification. However,
    for our use case, we will be using the type `Point`. First let's see how the document
    we imported before using a non-standard format looked and how the one using GeoJSON
    format looks.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 此规范支持各种地理图形类型。但是，对于我们的用例，我们将使用类型`Point`。首先让我们看看我们之前使用非标准格式导入的文档是什么样子的，以及使用GeoJSON格式的文档是什么样子的。
- en: 'Document in non-standard format:'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非标准格式的文档：
- en: '[PRE75]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Document in GeoJSON format:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeoJSON格式的文档：
- en: '[PRE76]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: It looks more complicated than the non-standard format and for our particular
    case I do agree. However, when representing polygons and other lines, the non-standard
    format might have to store multiple documents. In this case, it can be stored
    in a single document just by changing the value of the `type` field. Refer to
    the specification for more details.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的特定情况来说，它看起来比非标准格式更复杂，我同意。然而，当表示多边形和其他线时，非标准格式可能必须存储多个文档。在这种情况下，只需更改`type`字段的值，就可以将其存储在单个文档中。有关更多详细信息，请参阅规范。
- en: Getting ready
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The prerequisites for this recipe are same as the prerequisites for the previous
    recipe except that the file to be imported would be `2dMapGeoJSONData.json` and
    `countries.geo.json`. Download these files from the Packt site and keep them on
    the local filesystem for importing them later.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的先决条件与上一个食谱的先决条件相同，只是要导入的文件将是`2dMapGeoJSONData.json`和`countries.geo.json`。从Packt网站下载这些文件，并将它们保存在本地文件系统中，以便稍后导入它们。
- en: Note
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Special thanks to Johan Sundström for sharing the world data. The GeoJSON for
    the world is taken from [https://github.com/johan/world.geo.json](https://github.com/johan/world.geo.json).
    The file is massaged to enable importing and index creation in Mongo. Version
    2.4 doesn't support MultiPolygon and thus all MultiPolygon type of shapes are
    omitted. The shortcoming seems to be fixed in Version 2.6 though.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢Johan Sundström分享世界数据。世界的GeoJSON取自[https://github.com/johan/world.geo.json](https://github.com/johan/world.geo.json)。该文件经过处理，以便在Mongo中进行导入和索引创建。2.4版本不支持MultiPolygon，因此所有MultiPolygon类型的形状都被省略了。然而，这个缺点似乎在2.6版本中得到了修复。
- en: How to do it…
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Import the GeoJSON compatible data in a new collection as follows. This contains
    26 documents similar to what we imported last time around, except that they are
    formatted using the GeoJSON format.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式将GeoJSON兼容数据导入新集合。这包含了26个类似于我们上次导入的文档，只是它们是使用GeoJSON格式进行格式化的。
- en: '[PRE77]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Create a Geospatial index on this collections as follows:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些集合上创建一个地理空间索引，如下所示：
- en: '[PRE78]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We will now first query the collection `areaMapGeoJSON` collection as follows:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将首先查询`areaMapGeoJSON`集合，如下所示：
- en: '[PRE79]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Next, we will try to find all the restaurants that fall within the square drawn
    between the points (0, 0), (0, 11), (11, 11), and (11, 0). Refer to the figure
    given in the introduction of the previous recipe for getting a clear visual of
    the points and the results to expect.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试找到所有落在由点(0, 0)、(0, 11)、(11, 11)和(11, 0)之间的正方形内的餐馆。请参考上一个食谱介绍中给出的图形，以清晰地看到点和预期结果。
- en: 'Write the following query and observe the results:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写以下查询并观察结果：
- en: '[PRE80]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Check if it contains the three restaurants at coordinates (2, 6), (10, 5), and
    (10, 1) as expected.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 检查它是否包含预期的坐标(2, 6)、(10, 5)和(10, 1)处的三家餐馆。
- en: 'We will next try and perform some operations that would find all the matching
    objects that lie completely within another enclosing polygon. Suppose that we
    want to find some bus stops that lie within a given square block. Such use cases
    can be addressed using the `$geoWithin` operator, and the query to achieve it
    is as follows:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试执行一些操作，找到完全位于另一个封闭多边形内的所有匹配对象。假设我们想找到一些位于给定正方形街区内的公交车站。可以使用`$geoWithin`操作符来解决这类用例，实现它的查询如下：
- en: '[PRE81]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Verify the results; we should have three bus stops in the result. Refer to the
    image of the map in the previous recipe's introduction to get the expected results
    of the query.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证结果；我们应该在结果中有三个公交车站。参考上一个食谱介绍中的地图图像，以获取查询的预期结果。
- en: 'When we execute the above commands, they just print the documents in ascending
    order of the distance. However, we don''t see the actual distance in the result.
    Let''s execute the same query as in point number 3 and additionally, get the calculated
    distances as following:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们执行上述命令时，它们只是按距离升序打印文档。但是，我们在结果中看不到实际的距离。让我们执行与第3点中相同的查询，并额外获取计算出的距离如下：
- en: '[PRE82]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The query returns one document with an array within the field called results
    containing the matching documents and the calculated distances. The result also
    contains some additional stats giving the maximum distance, the average of the
    distances in the result, the total documents scanned, and the time taken in milliseconds.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询返回一个文档，其中包含一个名为results的字段内的数组，其中包含匹配的文档和计算出的距离。结果还包含一些额外的统计信息，包括最大距离，结果中距离的平均值，扫描的总文档数以及以毫秒为单位的所用时间。
- en: 'We will finally query on the world map collection to find which country the
    provided coordinate lies in. Execute the following query as follows from the mongo
    shell:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将在世界地图集合上查询，找出提供的坐标位于哪个国家。从mongo shell执行以下查询：
- en: '[PRE83]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: All the possible operations we can perform with the `worldMap` collection are
    numerous and not all are practically possible to cover in this recipe. I would
    encourage you to play around with this collection and try out different use cases.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对`worldMap`集合执行的所有可能操作都很多，而且并非所有操作都在这个食谱中都能实际覆盖到。我鼓励你尝试使用这个集合并尝试不同的用例。
- en: How it works…
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Starting from version MongoDB 2.4, the standard way for storing geospatial data
    in JSON is also supported. Note that the legacy approach that we saw is also supported.
    However, if you are starting afresh, it is recommended to go ahead with this approach
    for the following reasons.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 2.4版本开始，JSON中存储地理空间数据的标准方式也得到了支持。请注意，我们看到的传统方法也得到了支持。但是，如果你是从头开始的，建议出于以下原因采用这种方法。
- en: It is a standard and anybody aware of the specification would easily be able
    to understand the structure of the document
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个标准的，任何了解规范的人都可以轻松理解文档的结构
- en: It makes storing complex shapes, polygons, and multiple lines easy
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使存储复杂形状、多边形和多条线变得容易。
- en: It also lets us query easily for the intersection of the shapes using the `$geoIntersect`
    and other new set of operators
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还让我们可以使用`$geoIntersect`和其他一组新的操作符轻松查询形状的交集
- en: 'For using GeoJSON-compatible documents, we import JSON documents in the file
    `2dMapGeoJSONData.json` into the collection `areaMapGeoJSON` and create the index
    as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用GeoJSON兼容的文档，我们将JSON文档导入到`areaMapGeoJSON`集合中，并按以下方式创建索引：
- en: '[PRE84]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The collection has data similar to what we had imported into the `areaMap` collection
    in the previous recipe but with a different structure that is compatible to JSON
    format. The type here used is 2Dsphere and not 2D. The 2Dsphere type of index
    also considers the spherical surfaces in calculations. Note that the field `co`,
    on which we are creating the geospatial index, is not an array of coordinates
    but a document itself that is GeoJSON compatible.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 集合中的数据与我们在上一个食谱中导入到`areaMap`集合中的数据类似，但结构不同，与JSON格式兼容。这里使用的类型是2Dsphere而不是2D。2Dsphere类型的索引还考虑了球面表面的计算。请注意，我们正在创建地理空间索引的字段`co`不是坐标数组，而是一个符合GeoJSON的文档本身。
- en: We query where the value of the `$near` operator is not an array of the coordinates,
    as we did in our previous recipe, but a document with the key `$geometry` and
    the value is a GeoJSON-compatible document for a point with the coordinates. The
    results, irrespective of the query we use are identical. Refer to point 3 in this
    recipe and point 5 in the previous recipe to see the difference in the query.
    The approach using GeoJSON looks more complicated but it has some advantages which
    we will soon see.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们查询`$near`操作符的值不是坐标数组，而是一个带有`$geometry`键的文档，其值是一个具有坐标的GeoJSON兼容文档。无论我们使用的查询是什么，结果都是相同的。参考本食谱中的第3点和上一个食谱中的第5点，以查看查询中的差异。使用GeoJSON的方法看起来更复杂，但它有一些优势，我们很快就会看到。
- en: It is important to note that we cannot mix two approaches. Try executing the
    query in the GeoJSON format that we just executed on the collection `areaMap`
    and see that although we do not get any errors, the results are not correct.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们不能混合两种方法。尝试在`areaMap`集合上执行我们刚刚执行的GeoJSON格式的查询，尽管我们不会收到任何错误，但结果是不正确的。
- en: 'We used the `$geoIntersects` operator in point 5 of this recipe. This is only
    possible when the documents are stored in GeoJSON format in the database. The
    query simply finds all the points in our case that intersect any shape we create.
    We create a polygon using the GeoJSON format as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本示例的第5点中使用了`$geoIntersects`运算符。这只有在数据库中以GeoJSON格式存储文档时才可能。查询简单地找到我们的情况下与我们创建的任何形状相交的所有点。我们使用GeoJSON格式创建多边形如下：
- en: '[PRE85]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The coordinates are for the square, giving the four corners in a clockwise direction
    with the last coordinate the same as the first denoting it to be complete. The
    query executed is the same as `$near`, apart from the fact that the `$near` operator
    is replaced by the `$geoIntersects` and the value of the `$geometry` field is
    the GeoJSON document of the polygon with which we wish to find the intersecting
    points in the `areaMapGeoJSON` collection. If we look at the results obtained
    and look at the figure in the introduction section or previous recipe, they indeed
    are what we expected.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这些坐标是正方形的，按顺时针方向给出四个角，最后一个坐标与第一个坐标相同，表示它是完整的。执行的查询与`$near`相同，除了`$near`运算符被`$geoIntersects`替换，`$geometry`字段的值是我们希望在`areaMapGeoJSON`集合中找到相交点的多边形的GeoJSON文档。如果我们看一下得到的结果，并查看介绍部分或上一个示例中的图形，它们确实是我们期望的。
- en: We also saw what the `$geoWithin` operator is in point number 12, which is pretty
    handy to use when we want to find the points or even within another polygon. Note
    that only shapes completely inside the given polygon will be returned. Suppose
    that, similar to our `worldMap` collection, we have a `cities` collection with
    their coordinates specified in a similar manner. We can then use the polygon of
    a country to query all the polygons that lie within it in the `cities` collection,
    thus giving us the cities. Obviously, an easier and faster way would be to store
    the country code in the city document. Alternatively, if we have some data missing
    in the city's collection and the country is not present, one point anywhere within
    the city's polygon (since a city entirely lies in one country) can be used and
    a query can be executed on the `worldMap` collection to get its country, which
    we have demonstrated in point number 12.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在第12点看到了`$geoWithin`运算符，当我们想要找到点或者在另一个多边形内部时，这是非常方便的。请注意，只有完全在给定多边形内部的形状才会被返回。假设，类似于我们的`worldMap`集合，我们有一个`cities`集合，其中的坐标以类似的方式指定。然后，我们可以使用一个国家的多边形来查询在`cities`集合中位于其中的所有多边形，从而给出城市。显然，一个更简单和更快的方法是在城市文档中存储国家代码。或者，如果城市集合中有一些数据缺失，而且国家不存在，可以使用城市多边形内的任意一点（因为一个城市完全位于一个国家内），并在`worldMap`集合上执行查询来获取它的国家，这是我们在第12点中演示的。
- en: A combination of what we saw previously can be put to good use to compute the
    distances between two points or even execute some geometric operation.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的一些组合可以很好地用于计算两点之间的距离，甚至执行一些几何操作。
- en: Some of the functionalities like getting the centroid of a polygon figure stored
    as GeoJSON in the collection or even the area of a polygon are not supported out
    of the box and there should have been some utility functions to help compute these
    given the coordinates. These features are good and are commonly required, and
    perhaps we might have some support in future release; such operations are to be
    implemented by developers themselves. Also, there is no straightforward way to
    find if there is an overlap between two polygons, what the coordinates are, where
    they overlap, the area of overlap, and so on. The `$geoIntersects` operator we
    saw does tell us what polygons do intersect with the given polygon, point, or
    line.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一些功能，比如获取存储在集合中的GeoJSON多边形图形的质心，甚至是多边形的面积，都不是开箱即用的，应该有一些实用函数来帮助计算这些坐标。这些功能很好，通常是必需的，也许在将来的版本中我们可能会有一些支持；这些操作需要开发人员自己实现。此外，没有直接的方法来查找两个多边形之间是否有重叠，它们的坐标在哪里重叠，重叠的面积等等。我们看到的`$geoIntersects`运算符告诉我们哪些多边形与给定的多边形、点或线相交。
- en: Though nothing related to Mongo, the GeoJSON format doesn't have support for
    circles, and hence storing circles in Mongo using GeoJSON format is not possible.
    Refer to the following link [http://docs.mongodb.org/manual/reference/operator/query-geospatial/](http://docs.mongodb.org/manual/reference/operator/query-geospatial/)
    for more details on geospatial operators.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与Mongo无关，但GeoJSON格式不支持圆，因此无法使用GeoJSON格式在Mongo中存储圆。有关地理空间运算符的更多详细信息，请参考以下链接[http://docs.mongodb.org/manual/reference/operator/query-geospatial/](http://docs.mongodb.org/manual/reference/operator/query-geospatial/)。
- en: Implementing full text search in Mongo
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Mongo中实现全文搜索
- en: 'Many of us (I won''t be wrong to say all of us) use Google every day to search
    content on the web. To explain in short: the text that we provide in the text
    box on Google''s page is used to search the pages on the web it has indexed. The
    search results are then returned to us in some order determined by Google''s page
    rank algorithm. We might want to have a similar functionality in our database
    that lets us search for some text content and give the corresponding search results.
    Note that this text search is not same as finding the text as part of the sentence,
    which can easily be done using regex. It goes way beyond that and can be used
    to get results that contain the same word, a similar sounding word, have a similar
    base word, or even a synonym in the actual sentence.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的许多人（我可以毫不夸张地说所有人）每天都使用Google在网上搜索内容。简单来说：我们在Google页面的文本框中提供的文本用于搜索它所索引的网页。搜索结果然后以一定顺序返回给我们，这个顺序是由Google的页面排名算法确定的。我们可能希望在我们的数据库中有类似的功能，让我们搜索一些文本内容并给出相应的搜索结果。请注意，这种文本搜索与查找作为句子的一部分的文本不同，后者可以很容易地使用正则表达式来完成。它远远超出了那个范围，可以用来获取包含相同单词、类似发音的单词、具有相似基本单词，甚至是实际句子中的同义词的结果。
- en: Since MongoDB Version 2.4, text indexes have been introduced, which let us create
    text indexes on a particular field in the document and enable text search on those
    words. In this recipe, we will be importing some documents and creating text indexes
    on them, which we will later query to retrieve the results.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 自MongoDB 2.4版本以来，引入了文本索引，它让我们可以在文档的特定字段上创建文本索引，并在这些单词上启用文本搜索。在这个示例中，我们将导入一些文档，并在它们上创建文本索引，然后查询以检索结果。
- en: Getting ready
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: A simple, single node is what we would need for the test. Refer to the recipe
    *Installing single node MongoDB* from [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*, for how to start
    the server. However, do not start the server yet. There would be an additional
    flag provided during the startup to enable text search. Download the file `BlogEntries.json`
    from the Packt site and keep it on your local drive ready to be imported.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 测试需要一个简单的单节点。参考[第1章](ch01.html "第1章. 安装和启动服务器")的*安装单节点MongoDB*一节，了解如何启动服务器。但是，不要立即启动服务器。在启动过程中将提供一个额外的标志来启用文本搜索。从Packt网站下载文件`BlogEntries.json`，并将其保存在本地驱动器上以备导入。
- en: How to do it…
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'Start the MongoDB server listening to port `27017` as follows. Once the server
    is started, we will be creating the test data in a collection as follows. With
    the file `BlogEntries.json` placed in the current directory, we will be creating
    the collection `userBlog` as follows using `mongoimport`:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动MongoDB服务器监听端口`27017`，如下所示。一旦服务器启动，我们将按以下方式在集合中创建测试数据。将文件`BlogEntries.json`放在当前目录中，我们将使用`mongoimport`创建`userBlog`集合，如下所示：
- en: '[PRE86]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, connect to the `mongo` process from a mongo shell by typing the following
    command from the operating system shell:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过在操作系统shell中输入以下命令，从mongo shell连接到`mongo`进程：
- en: '[PRE87]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Once connected, get a feel of the documents in the `userBlog` collection as
    follows:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接后，按照以下步骤对`userBlog`集合中的文档有所了解：
- en: '[PRE88]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The field `blog_text` is of our interest and this is the one on which we will
    be creating a text search index.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们感兴趣的字段是`blog_text`，这是我们将创建文本搜索索引的字段。
- en: 'Create a text index on the field `blog_text` of the document as follows:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤在文档的`blog_text`字段上创建文本索引：
- en: '[PRE89]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now, execute the following search on the collection from the mongo shell:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在mongo shell中对集合执行以下搜索：
- en: '[PRE90]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Look at the results obtained.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所得到的结果。
- en: 'Execute another search as follows:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行另一个搜索，如下所示：
- en: '[PRE91]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: How it works…
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Let''s now see how it all works. A text search is done by a process called
    reverse indexes. In simple terms, this is a mechanism where the sentences are
    broken up into words and then those individual words point back to the document
    which they belong to. The process is not straightforward though, so let''s see
    what happens in this process step by step at a high level:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看它是如何工作的。文本搜索是通过一个称为反向索引的过程来完成的。简单来说，这是一个机制，将句子分解为单词，然后这些单词分别指向它们所属的文档。然而，这个过程并不是直接的，所以让我们高层次地逐步看看这个过程中发生了什么：
- en: Consider the following input sentence, `I played cricket yesterday`. The first
    step is to break this sentence into tokens and they become [`I`, `played`, `cricket`,
    `yesterday`].
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑以下输入句子，`I played cricket yesterday`。第一步是将这个句子分解为标记，它们变成了[`I`, `played`, `cricket`,
    `yesterday`]。
- en: Next, the stop words from the broken down sentence are removed and we are left
    with a subset of these. Stop words are a list of very common words that are eliminated
    as it makes no sense to index them as they can potentially affect the accuracy
    of the search when used in the search query. In this case, we will be left with
    the following words [`played`, `cricket`, `yesterday`]. Stop words are language
    specific and will be different for different languages.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从拆分的句子中删除停用词，我们将得到这些词的子集。停用词是一组非常常见的词，它们被排除在外是因为将它们索引化没有意义，因为它们在搜索查询中使用时可能会影响搜索的准确性。在这种情况下，我们将得到以下单词[`played`,
    `cricket`, `yesterday`]。停用词是与语言相关的，对于不同的语言将会有不同的停用词。
- en: Finally, these words are stemmed to their base words, in this case it will be
    [`play`, `cricket`, `yesterday`]. Stemming is process of reduction of a word to
    its root. For instance, all the words `play`, `playing`, `played`, and `plays`
    have the same root word, `play`. There are a lot of algorithms and frameworks
    present for stemming a word to its root form. Refer to the Wikipedia [http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming)
    page for more information on stemming and the algorithms used for this purpose.
    Similar to eliminating stop words, the stemming algorithm is language dependent.
    The examples given here were for the English language.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这些单词被转换为它们的基本词，这种情况下将会是[`play`, `cricket`, `yesterday`]。词干提取是将一个词减少到其词根的过程。例如，所有的单词`play`,
    `playing`, `played`, 和 `plays`都有相同的词根词`play`。有很多算法和框架用于将一个词提取为其词根形式。参考维基百科[http://en.wikipedia.org/wiki/Stemming](http://en.wikipedia.org/wiki/Stemming)页面，了解更多关于词干提取和用于此目的的算法的信息。与消除停用词类似，词干提取算法是与语言相关的。这里给出的例子是针对英语的。
- en: 'If we look at the index creation process, it is created as follows `db.userBlog.ensureIndex({''blog_text'':''text''})`.
    The key given in the JSON argument is the name of the field on which the text
    index is to be created and the value will always be the text denoting that the
    index to be created is a text index. Once the index is created, at a high level,
    the preceding three steps get executed on the content of the field on which the
    index is created in each document and a reverse index is created. You can also
    choose to create a text index on more than one field. Suppose that we had two
    fields, `blog_text1` and `blog_text2`; we can create the index as `{''blog_text1'':
    ''text'', ''blog_text2'':''text''}`. The value `{''$**'':''text''}` creates an
    index on all fields of the document.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们查看索引创建过程，它是如下创建的`db.userBlog.ensureIndex({''blog_text'':''text''})`。JSON参数中给定的键是要在其上创建文本索引的字段的名称，值将始终是表示要创建的索引是文本索引的文本。创建索引后，在高层次上，前面的三个步骤在每个文档中所创建的索引字段的内容上执行，并创建反向索引。您还可以选择在多个字段上创建文本索引。假设我们有两个字段，`blog_text1`和`blog_text2`；我们可以将索引创建为`{''blog_text1'':
    ''text'', ''blog_text2'':''text''}`。值`{''$**'':''text''}`在文档的所有字段上创建索引。'
- en: 'Finally, we executed the search operation by invoking the following: `db.userBlog.find({$text:
    {$search : ''plot zoo''}})`.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们通过调用以下命令执行了搜索操作：`db.userBlog.find({$text: {$search : ''plot zoo''}})`。'
- en: 'This command runs the text search on the collection `userBlog` and the search
    string used is `plot zoo`. This searches for the value `plot` or `zoo` in the
    text in any order. If we look at the results, we see that we have two documents
    matched and the documents are ordered by the score. This score tells us how relevant
    the document searched is, and the higher the score, the more relevant it is. In
    our case, one of the documents had both the words plot and zoo in it, and thus
    got a higher score than a document, as we see here:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令在集合`userBlog`上运行文本搜索，使用的搜索字符串是`plot zoo`。这会按任意顺序在文本中搜索值`plot`或`zoo`。如果我们查看结果，我们会看到有两个匹配的文档，并且文档按得分排序。得分告诉我们所搜索的文档的相关性如何，得分越高，相关性越大。在我们的情况下，一个文档中同时包含单词plot和zoo，因此得分比另一个文档高。
- en: 'To get the scores in the result, we need to modify the query a bit, as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 要在结果中获取得分，我们需要稍微修改查询，如下所示：
- en: '[PRE92]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We now have an additional document provided in the `find` method that asks
    for the score calculated for the text match. The results still are not ordered
    in descending order of score. Let''s see how to sort the results by score:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在`find`方法中提供了一个额外的文档，询问文本匹配的计算得分。结果仍然没有按得分降序排序。让我们看看如何按得分排序：
- en: '[PRE93]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: As we can see, the query is same as before, it's just the additional `sort`
    function that we have added, which will sort the results by descending order of
    score.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，查询与以前相同，只是我们添加了额外的`sort`函数，它将按得分降序对结果进行排序。
- en: When the search is executed as `{$text:{$search:'Zoo -plot'}`, it searches for
    all the documents that contain the word `zoo` and do not contain the word `plot`,
    thus we get only one result. The `-` sign is for negation and leaves out the document
    from the search result containing that word. However, do not expect to find all
    documents without the word plot by just giving `-plot` in the search.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 当搜索执行为`{$text:{$search:'Zoo -plot'}`时，它会搜索包含单词`zoo`但不包含单词`plot`的所有文档，因此我们只得到一个结果。`-`符号用于否定，并且将包含该单词的文档排除在搜索结果之外。但是，不要期望通过在搜索中只给出`-plot`来找到所有不包含单词plot的文档。
- en: 'If we look at the contents returned as the result of the search, it contains
    the entire matched document in the result. If we are not interested in the entire
    document, but only a few documents, we can use projection to get the desired fields
    of the document. The following query, for instance, `db.userBlog.find({$text:
    {$search : ''plot zoo''}},{_id:1})` will be same as finding all the documents
    in the `userBlog` collection containing the words zoo or plot, but the results
    will contain the `_id` field from the resulting documents.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们查看作为搜索结果返回的内容，它包含了整个匹配的文档。如果我们对整个文档不感兴趣，而只对其中的一些文档感兴趣，我们可以使用投影来获取文档的所需字段。例如，以下查询`db.userBlog.find({$text:
    {$search : ''plot zoo''}},{_id:1})`将与在`userBlog`集合中查找包含单词zoo或plot的所有文档相同，但结果将包含所得文档的`_id`字段。'
- en: 'If multiple fields are used for creation of index, then we may have different
    weights for different fields in the document. For instance, suppose blog_text1
    and blog_text2 are two fields of a collection. We can create an index where `blog_text1`
    has higher weight than `blog_text2` as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个字段用于创建索引，则文档中的不同字段可能具有不同的权重。例如，假设`blog_text1`和`blog_text2`是集合的两个字段。我们可以创建一个索引，其中`blog_text1`的权重高于`blog_text2`，如下所示：
- en: '[PRE94]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: This gives the content in `blog_text1` twice as much weight as that in `blog_text2`.
    Thus, if a word is found in two documents but is present in the `blog_text1` field
    of the first document and `blog_text2` of second document, then the score of first
    document will be more than the second. Note that we also have provided the name
    of the index using the name field as `MyCustomIndexName`.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得`blog_text1`中的内容的权重是`blog_text2`的两倍。因此，如果一个词在两个文档中被找到，但是在第一个文档的`blog_text1`字段和第二个文档的`blog_text2`中出现，那么第一个文档的得分将比第二个文档更高。请注意，我们还使用`MyCustomIndexName`字段提供了索引的名称。
- en: We also see from the language key that the language in this case is English.
    MongoDB supports various languages for implementing text search. Languages are
    important when indexing the content as they decide the stop words, and stemming
    of words is language specific too.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还从语言键中看到，这种情况下的语言是英语。MongoDB支持各种语言来实现文本搜索。语言在索引内容时很重要，因为它们决定了停用词，并且词干提取也是特定于语言的。
- en: Visit the link [http://docs.mongodb.org/manual/reference/command/text/#text-search-languages](http://docs.mongodb.org/manual/reference/command/text/#text-search-languages)
    for more details on the languages supported by Mongo for text search.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 访问链接[http://docs.mongodb.org/manual/reference/command/text/#text-search-languages](http://docs.mongodb.org/manual/reference/command/text/#text-search-languages)以获取Mongo支持的文本搜索语言的更多详细信息。
- en: 'So, how do we choose the language while creating the index? By default, if
    nothing is provided, the index is created assuming the language is English. However,
    if we know the language is French, we create the index as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在创建索引时如何选择语言呢？默认情况下，如果没有提供任何内容，索引将被创建，假定语言是英语。但是，如果我们知道语言是法语，我们将如下创建索引：
- en: '[PRE95]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Suppose that we had originally created the index using the French language,
    the `getIndexes` method would return the following document:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们最初是使用法语创建索引的，`getIndexes`方法将返回以下文档：
- en: '[PRE96]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'However, if the language was different per document basis, which is pretty
    common in scenarios like blogs, we have a way out. If we look at the document
    above, the value of the `language_override` field is language. This means that
    we can store the language of the content using this field on a per document basis.
    In its absence, the value will be assumed as the default value, `french` in the
    preceding case. Thus, we can have the following:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果每个文档的语言不同，这在博客等场景中非常常见，我们有一种方法。如果我们查看上面的文档，`language_override`字段的值是language。这意味着我们可以使用此字段在每个文档的基础上存储内容的语言。如果没有，该值将被假定为默认值，在前面的情况下为`french`。因此，我们可以有以下内容：
- en: '[PRE97]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: There's more…
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: To use MongoDB text search in production, you would need version 2.6 or higher.
    Integrating MongoDB with other systems like Solr and Elasticsearch is also an
    option. In the next recipe, we will see how to integrate Mongo with Elasticsearch
    using the mongo-connector.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 要在生产中使用MongoDB文本搜索，您需要2.6或更高版本。还可以将MongoDB与Solr和Elasticsearch等其他系统集成。在下一个配方中，我们将看到如何使用mongo-connector将Mongo集成到Elasticsearch中。
- en: See also
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For more information on the `$text` operator, visit [http://docs.mongodb.org/manual/reference/operator/query/text/](http://docs.mongodb.org/manual/reference/operator/query/text/)
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关`$text`运算符的更多信息，请访问[http://docs.mongodb.org/manual/reference/operator/query/text/](http://docs.mongodb.org/manual/reference/operator/query/text/)
- en: Integrating MongoDB for full text search with Elasticsearch
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将MongoDB集成到Elasticsearch进行全文搜索
- en: 'MongoDB has integrated text search features, as we saw in the previous recipe.
    However, there are multiple reasons why one would not use the Mongo text search
    feature and fall back to a conventional search engine like Solr or Elasticsearch,
    and the following are few of them:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB已经集成了文本搜索功能，就像我们在上一个配方中看到的那样。但是，有多种原因会导致人们不使用Mongo文本搜索功能，而是退回到Solr或Elasticsearch等传统搜索引擎，以下是其中的一些原因：
- en: The text search feature is production ready in version 2.6\. In version 2.4,
    it was introduced in beta and not suitable for production use cases.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本搜索功能在2.6版本中已经准备就绪。在2.4版本中，它是以测试版引入的，不适用于生产用例。
- en: Products like Solr and Elasticsearch are built on top of Lucene, which has proven
    itself in the search engine arena. Solr and Elasticsearch are pretty stable products
    too.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Solr和Elasticsearch这样的产品是建立在Lucene之上的，它在搜索引擎领域已经证明了自己。Solr和Elasticsearch也是相当稳定的产品。
- en: You might already have expertise on products like Solr and Elasticsearch and
    would like to use it as a full text search engine rather than MongoDB.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能已经对Solr和Elasticsearch等产品有所了解，并希望将其作为全文搜索引擎，而不是MongoDB。
- en: Some particular feature that you might find missing in MongoDB search which
    your application might require, for example, facets.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能会发现在MongoDB搜索中缺少一些特定功能，而您的应用程序可能需要这些功能，例如facets。
- en: Setting up a dedicated search engine does need additional efforts to integrate
    it with a MongoDB instance. In this recipe, we will see how to integrate a MongoDB
    instance with a search engine, Elasticsearch.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 设置专用搜索引擎确实需要额外的工作来将其与MongoDB实例集成。在这个配方中，我们将看到如何将MongoDB实例与搜索引擎Elasticsearch集成。
- en: We will be using the mongo-connector for integration purpose. It is an open
    source project that is available at [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用mongo-connector进行集成。这是一个开源项目，可以在[https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)上找到。
- en: Getting ready
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the recipe *Connecting to a single node using a Python client*, in
    [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for installing and setting up Python client. The tool
    pip is used for getting the mongo-connector. However, if you are working on a
    Windows platform, the steps to install pip was not mentioned earlier. Visit the
    URL [https://sites.google.com/site/pydatalog/python/pip-for-windows](https://sites.google.com/site/pydatalog/python/pip-for-windows)
    to get pip for windows.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用Python客户端连接单节点的配方，请参阅[第1章](ch01.html "第1章。安装和启动服务器")中的*安装和启动服务器*。pip工具用于获取mongo-connector。但是，如果您在Windows平台上工作，之前没有提到安装pip的步骤。访问网址[https://sites.google.com/site/pydatalog/python/pip-for-windows](https://sites.google.com/site/pydatalog/python/pip-for-windows)以获取Windows版的pip。
- en: The prerequisites for starting the single instance are all we need for this
    recipe. We would, however, start the server as a one node replica set for demonstration
    purpose in this recipe.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 开始单实例所需的先决条件是我们在这个配方中所需要的。然而，为了演示目的，我们将作为一个节点副本集启动服务器。
- en: Download the file `BlogEntries.json` from the Packt site and keep it on your
    local drive ready to be imported.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 从Packt网站下载文件`BlogEntries.json`，并将其保存在本地驱动器上，准备导入。
- en: 'Download elastic search from the following URL for your target platform: [http://www.elasticsearch.org/overview/elkdownloads/](http://www.elasticsearch.org/overview/elkdownloads/).
    Extract the downloaded archive and from the shell, go to the `bin` directory of
    the extraction.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下URL下载您的目标平台的elastic search：[http://www.elasticsearch.org/overview/elkdownloads/](http://www.elasticsearch.org/overview/elkdownloads/)。提取下载的存档，并从shell中转到提取的`bin`目录。
- en: We will get the mongo-connector source from GitHub.com and run it. A Git client
    is needed for this purpose. Download and install the Git client on your machine.
    Visit the URL [http://git-scm.com/downloads](http://git-scm.com/downloads) and
    follow the instructions for installing Git on your target operating system. If
    you are not comfortable installing Git on your operating system, then there is
    an alternative available that lets you download the source as an archive.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从GitHub.com获取mongo-connector源代码并运行它。为此需要Git客户端。在您的计算机上下载并安装Git客户端。访问URL[http://git-scm.com/downloads](http://git-scm.com/downloads)并按照说明在目标操作系统上安装Git。如果您不愿意在操作系统上安装Git，则有另一种选择，可以让您将源代码作为存档下载。
- en: 'Visit the following URL [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector).
    Here, we will get an option that lets us download the current source as an archive,
    which we can then extract on our local drive. The following image shows that the
    download option available on the bottom-right corner:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下URL[https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)。在这里，我们将获得一个选项，让我们将当前源代码作为存档下载，然后我们可以在本地驱动器上提取它。以下图片显示了下载选项位于右下角：
- en: '![Getting ready](img/B04831_05_03.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/B04831_05_03.jpg)'
- en: Note
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that we can also install mongo-connector in a very easy way using pip
    as follows:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使用pip以非常简单的方式安装mongo-connector，如下所示：
- en: '[PRE98]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: However, the version in PyPi is a very old with not many features supported
    and thus using the latest from the repository is recommended.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，PyPi中的版本非常旧，不支持许多功能，因此建议使用存储库中的最新版本。
- en: Similar to the previous recipe, where we saw text search in Mongo, we will use
    the same five documents to test our simple search. Download and keep the `BlogEntries.json`
    file.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的配方类似，在那里我们在Mongo中看到了文本搜索，我们将使用相同的五个文档来测试我们的简单搜索。下载并保留`BlogEntries.json`文件。
- en: How to do it…
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'At this point, it is assumed that Python and PyMongo are installed and pip
    for your operating system platform is installed. We will now get mongo-connector
    from source. If you have already installed the Git client, we will be executing
    the following on the operating system shell. If you have decided to download the
    repository as an archive, you may skip this step. Go to the directory where you
    would like to clone the connector repository and execute the following:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，假设Python和PyMongo已安装，并且为您的操作系统平台安装了pip。我们现在将从源代码获取mongo-connector。如果您已经安装了Git客户端，我们将在操作系统shell上执行以下操作。如果您决定将存储库下载为存档，则可以跳过此步骤。转到您想要克隆连接器存储库的目录，并执行以下操作：
- en: '[PRE99]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: The preceding setup will also install the Elasticsearch client that will be
    used by this application.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述设置还将安装将被此应用程序使用的Elasticsearch客户端。
- en: 'We will now start a single mongo instance but as a replica set. From the operating
    system console, execute the following:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将启动单个mongo实例，但作为副本集。从操作系统控制台执行以下操作：
- en: '[PRE100]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Start a mongo shell and connect to the started instance:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动mongo shell并连接到已启动的实例：
- en: '[PRE101]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'From the mongo shell initiate the replica set as follows:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo shell开始初始化副本集如下：
- en: '[PRE102]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: The replica set will be initiated in a few moments. Meanwhile, we can proceed
    to starting the `elasticsearch` server instance.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 副本集将在几分钟内初始化。与此同时，我们可以继续启动`elasticsearch`服务器实例。
- en: 'Execute the following from the command after going to the `bin` directory of
    the extracted `elasticsearch` archive:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提取的`elasticsearch`存档的`bin`目录中执行以下命令：
- en: '[PRE103]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We won't be getting into the Elasticsearch settings, and we will start it in
    the default mode.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不会涉及Elasticsearch设置，我们将以默认模式启动它。
- en: Once started, enter the following URL in the browser `http://localhost:9200/_nodes/process?pretty`.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦启动，输入以下URL到浏览器`http://localhost:9200/_nodes/process?pretty`。
- en: If we see a JSON document as the following, giving the process details, we have
    successfully started `elasticsearch`.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们看到以下JSON文档，给出了进程详细信息，我们已成功启动了`elasticsearch`。
- en: '[PRE104]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Once the `elasticsearch` server and mongo instance are up and running, and the
    necessary Python libraries are installed, we will start the connector that will
    sync the data between the started mongo instance and the `elasticsearch` server.
    For the sake of this test, we will be using the collection `user_blog` in the
    `test` database. The field on which we would like to have text search implemented
    is the field `blog_text` in the document.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`elasticsearch`服务器和mongo实例启动并运行，并且安装了必要的Python库，我们将启动连接器，该连接器将在启动的mongo实例和`elasticsearch`服务器之间同步数据。出于这个测试的目的，我们将在`test`数据库中使用`user_blog`集合。我们希望在文档中实现文本搜索的字段是`blog_text`。
- en: Start the mongo-connector from the operating system shell as follows. The following
    command was executed with the mongo-connector's directory as the current directory.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从操作系统shell启动mongo-connector如下。以下命令是在mongo-connector的目录中执行的。
- en: '[PRE105]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Import the `BlogEntries.json` file into the collection using `mongoimport` utility
    as follows. The command is executed with the `.json` file present in the current
    directory.
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mongoimport`实用程序将`BlogEntries.json`文件导入集合如下。该命令是在当前目录中执行的`.json`文件。
- en: '[PRE106]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Open a browser of your choice and enter the following URL in it: `http://localhost:9200/_search?q=blog_text:facebook`.'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您选择的浏览器，并在其中输入以下URL：`http://localhost:9200/_search?q=blog_text:facebook`。
- en: You should see something like the following in the browser:![How to do it…](img/B04831_05_04.jpg)
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该在浏览器中看到类似以下内容的内容：![如何做…](img/B04831_05_04.jpg)
- en: How it works…
  id: totrans-490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Mongo-connector basically tails the oplog to find new updates that it publishes
    to another endpoint. We used elasticsearch in our case, but it could be even be
    Solr. You may choose to write a custom DocManager that would plugin with the connector.
    Refer to the wiki [https://github.com/10gen-labs/mongo-connector/wiki](https://github.com/10gen-labs/mongo-connector/wiki)
    for more details, and the readme for [https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)
    gives some detailed information too.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo-connector基本上是尾随oplog以查找它发布到另一个端点的新更新。在我们的情况下，我们使用了elasticsearch，但也可以是Solr。您可以选择编写一个自定义的DocManager，它将插入连接器。有关更多详细信息，请参阅维基[https://github.com/10gen-labs/mongo-connector/wiki](https://github.com/10gen-labs/mongo-connector/wiki)，自述文件[https://github.com/10gen-labs/mongo-connector](https://github.com/10gen-labs/mongo-connector)也提供了一些详细信息。
- en: 'We gave the connector the options `-m`, `-t`, `-n`, `--fields`, and `-d` and
    what they mean is explained in the table as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给连接器提供了选项`-m`，`-t`，`-n`，`--fields`和`-d`，它们的含义如下表所述：
- en: '| Option | Description |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: 选项 | 描述
- en: '| --- | --- |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `-m` | URL of the MongoDB host to which the connector connects to get the
    data to be synchronized. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '`-m` | 连接器连接到以获取要同步的数据的MongoDB主机的URL。'
- en: '| `-t` | The target URL of the system with which the data is to be synchronized
    with. Elasticsearch in this case. The URL format will depend on the target system.
    Should you choose to implement your own DocManager, the format will be one that
    your DocManager understands. |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '`-t` | 要将数据与之同步的系统的目标URL。在本例中是elasticsearch。URL格式将取决于目标系统。如果选择实现自己的DocManager，则格式将是您的DocManager理解的格式。'
- en: '| `-n` | This is the namespace that we would like keep synchronized with the
    external system. The connector will just be looking for changes in these namespaces
    while tailing the oplog for data. The value will be comma separated if more than
    one namespaces are to be synchronized. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '`-n` | 这是我们希望与外部系统保持同步的命名空间。连接器将在oplog中寻找这些命名空间的更改以获取数据。如果要同步多个命名空间，则值将以逗号分隔。'
- en: '| `--fields` | These are the fields from the document that will be sent to
    the external system. In our case, it doesn''t make sense to index the entire document
    and waste resources. It is recommended to add to the index just the fields that
    you would like to add text search support. The identifier `_id` and the namespace
    of the source is also present in the result, as we can see in the preceding screenshot.
    The `_id` field can then be used to query the target collection. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '`--fields` | 这些是将发送到外部系统的文档字段。在我们的情况下，索引整个文档并浪费资源是没有意义的。建议只向索引中添加您希望添加文本搜索支持的字段。在结果中还包括标识符`_id`和源的命名空间，正如我们在前面的屏幕截图中所看到的。然后可以使用`_id`字段来查询目标集合。'
- en: '| `-d` | This is the document manager to be used, in our case we have used
    the elasticsearch''s document manager. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '`-d` | 这是要使用的文档管理器，在我们的情况下，我们使用了elasticsearch的文档管理器。'
- en: For more supported options, refer to the readme of the connector's page on GitHub.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多支持的选项，请参阅GitHub上连接器页面的自述文件。
- en: Once the insert is executed on the MongoDB server, the connector detects the
    newly added documents to the collection of its interest, `user_blog`, and starts
    sending the data to be indexed from the newly documents to the elasticsearch.
    To confirm the addition, we execute a query in the browser to view the results.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在MongoDB服务器上执行插入操作，连接器就会检测到其感兴趣的集合`user_blog`中新添加的文档，并开始从新文档中发送要索引的数据到elasticsearch。为了确认添加，我们在浏览器中执行查询以查看结果。
- en: Elasticsearch will complain that the index names have upper case characters
    in them. The mongo-connector doesn't take care of this, and thus the name of the
    collection has to be in lower case. For example, the name `userBlog` will fail.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch将抱怨索引名称中有大写字符。mongo-connector没有处理这个问题，因此集合的名称必须是小写。例如，名称`userBlog`将失败。
- en: There's more…
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We have not done any additional configuration on elasticsearch as that was not
    the objective of the recipe. We were more interested in integrating MongoDB and
    elasticsearch. You will have to refer to elasticsearch documentation for more
    advanced config options. If integrating with elasticsearch is required, there
    is a concept called rivers in elasticsearch that can be used as well. Rivers are
    elasticsearch's way to get data from another data source. For MongoDB, the code
    for the river can be found at [https://github.com/richardwilly98/elasticsearch-river-mongodb/](https://github.com/richardwilly98/elasticsearch-river-mongodb/).
    The readme in this repository has steps on how to set it up.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有对elasticsearch进行任何额外的配置，因为这不是本教程的目标。我们更感兴趣的是集成MongoDB和elasticsearch。您需要参考elasticsearch文档以获取更高级的配置选项。如果需要与elasticsearch集成，elasticsearch中还有一个称为rivers的概念可以使用。Rivers是elasticsearch从另一个数据源获取数据的方式。对于MongoDB，可以在[https://github.com/richardwilly98/elasticsearch-river-mongodb/](https://github.com/richardwilly98/elasticsearch-river-mongodb/)找到river的代码。此存储库中的自述文件中有关于如何设置的步骤。
- en: In this chapter, we saw a recipe, *Implementing triggers in Mongo using oplog*,
    on how to implement trigger-like functionalities using Mongo. This connector and
    MongoDB river for elasticsearch rely on the same logic for getting the data out
    of Mongo as and when it is needed.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一个名为*在Mongo中使用oplog实现触发器*的教程，介绍了如何使用Mongo实现类似触发器的功能。这个连接器和elasticsearch的MongoDB
    river依赖于相同的逻辑，以在需要时从Mongo中获取数据。
- en: See also
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: You may find additional elasticsearch documentation at [http://www.elasticsearch.org/guide/en/elasticsearch/reference/](http://www.elasticsearch.org/guide/en/elasticsearch/reference/)
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在[http://www.elasticsearch.org/guide/en/elasticsearch/reference/](http://www.elasticsearch.org/guide/en/elasticsearch/reference/)找到更多的elasticsearch文档。
