- en: Appendix A. Concepts for Reference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. 参考概念
- en: This appendix contains some additional information that will help you understand
    the recipes better. We will discuss write concern and read preference in as much
    detail as possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录包含一些额外信息，将帮助您更好地理解配方。我们将尽可能详细地讨论写入关注和读取偏好。
- en: Write concern and its significance
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写入关注及其重要性
- en: Write concern is the minimum guarantee that the MongoDB server provides with
    respect to the write operation done by the client. There are various levels of
    write concern that are set by the client application, to get a guarantee from
    the server that a certain stage will be reached in the write process on the server
    side.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 写入关注是MongoDB服务器提供的关于客户端执行的写入操作的最低保证。客户端应用程序设置了各种级别的写入关注，以便从服务器获取在服务器端写入过程中达到某个阶段的保证。
- en: The stronger the requirement for a guarantee, the greater the time taken (potentially)
    to get a response from the server. With write concern, we don't always need to
    get an acknowledgement from the server about the write operation being completely
    successful. For some less crucial data such as logs, we might be more interested
    in sending more writes per second over a connection. On the other hand, when we
    are looking to update sensitive information, such as customer details, we want
    to be sure of the write being successful (consistent and durable); data integrity
    is crucial and takes precedence over the speed of the writes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于保证的要求越强，从服务器获取响应的时间就越长（可能）。在写入关注中，我们并不总是需要从服务器获取关于写入操作完全成功的确认。对于一些不太关键的数据，比如日志，我们可能更感兴趣地通过连接发送更多的写入。另一方面，当我们试图更新敏感信息，比如客户详细信息时，我们希望确保写入成功（一致和持久）；数据完整性至关重要，优先于写入速度。
- en: 'An extremely useful feature of write concern is the ability to compromise between
    one of the factors: the speed of write operations and the consistency of the data
    written, on a case-to-case basis. However, it needs a deep understanding of the
    implications of setting up a particular write concern. The following diagram runs
    from the left and goes to the right, and shows the increasing level of write guarantees:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 写入关注的一个极其有用的特性是在特定情况下在写入操作的速度和数据一致性之间进行权衡。然而，这需要对设置特定写入关注的影响有深入的理解。下图从左到右运行，并显示了写入保证水平的增加：
- en: '![Write concern and its significance](img/1943OS_Appendix_01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![写入关注及其重要性](img/1943OS_Appendix_01.jpg)'
- en: 'As we move from **I** to **IV**, the guarantee for the performed write gets
    stronger and stronger, but the time taken to execute the write operation is longer
    from a client''s perspective. All write concerns are expressed here as JSON objects,
    using three different keys, namely, `w`, `j`, and `fsync`. Additionally, another
    key called `wtimeout` is used to provide timeout values for the write operation.
    Let''s see the three keys in detail:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从**I**到**IV**，执行的写入保证越来越强，但从客户端的角度来看，执行写入操作所需的时间也越来越长。所有写入关注都以JSON对象的形式表示，使用三个不同的键，即`w`、`j`和`fsync`。另外，还使用了一个名为`wtimeout`的键，用于提供写入操作的超时值。让我们详细看一下这三个键：
- en: '`w`: This is used to indicate whether to wait for the server''s acknowledgement
    or not, whether to report write errors due to data issues or not, and about the
    data being replicated to secondary. Its value is usually a number and a special
    case where the value can be `majority`, which we will see later.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w`：用于指示是否等待服务器的确认，是否报告由于数据问题而导致的写入错误，以及数据是否被复制到次要位置。其值通常是一个数字，还有一个特殊情况，值可以是`majority`，我们稍后会看到。'
- en: '`j`: This is related to journaling and its value can be a Boolean (true/false
    or 1/0).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`j`：这与日志记录有关，其值可以是布尔值（true/false或1/0）。'
- en: '`fsync`: This is a Boolean value and is related to whether the write should
    wait till the data is flushed to disk or not before responding.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsync`：这是一个布尔值，与写入是否等待数据刷新到磁盘有关。'
- en: '`wtimeout`: This specifies the timeout for write operations, whereby the driver
    throws an exception to the client if the server doesn''t respond back in seconds
    within the provided time. We will see the option in some detail soon.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wtimeout`：指定写入操作的超时时间，如果服务器在提供的时间内没有在几秒内回复客户端，驱动程序将向客户端抛出异常。我们很快会详细了解该选项。'
- en: In part **I**, which we have demarcated till driver, we have two write concerns,
    namely, `{w:-1}` and `{w:0}`. Both these write concerns are common, in a sense
    that they neither wait for the server's acknowledgement upon receiving the write
    operation, nor do they report any exception on the server side caused by unique
    index violation. The client will get an `ok` response and will discover the write
    failure only when they query the database at some later point of time and find
    the data missing. The difference is in the way both these respond on the network
    error. When we set `{w:-1}`, the operation doesn't fail and a write response is
    received by the user. However, it will contain a response stating that a network
    error prevented the write operation from succeeding and no retries for write must
    be attempted. On the other hand, with `{w:0}`, if a network error occurs, the
    driver might choose to retry the operation and throw an exception to the client
    if the write fails due to network error. Both these write concerns give a quick
    response back to the invoking client at the cost of data consistency. These write
    concerns are ok for use cases such as logging, where occasional log write misses
    are fine. In older versions of MongoDB, `{w:0}` was the default write concern
    if none was mentioned by the invoking client. At the time of writing this book,
    this has changed to `{w:1}` by default and the option `{w:0}` is deprecated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们划分到驱动程序的**I**部分中，我们有两种写入关注点，分别是`{w:-1}`和`{w:0}`。这两种写入关注点都很常见，它们既不等待服务器对写入操作的确认，也不会报告由于唯一索引违规而在服务器端引起的任何异常。客户端将收到一个`ok`响应，并且只有在以后查询数据库时发现数据丢失时才会发现写入失败。两者的区别在于它们在网络错误时的响应方式。当我们设置`{w:-1}`时，操作不会失败，并且用户将收到写入响应。但是，它将包含一个响应，指出网络错误阻止了写入操作的成功，并且不应尝试重新写入。另一方面，对于`{w:0}`，如果发生网络错误，驱动程序可能选择重试操作，并且如果由于网络错误导致写入失败，则向客户端抛出异常。这两种写入关注点以牺牲数据一致性为代价，快速向调用客户端返回响应。这些写入关注点适用于日志记录等用例，其中偶尔的日志写入丢失是可以接受的。在较早版本的MongoDB中，如果调用客户端没有提及任何写入关注点，则`{w:0}`是默认的写入关注点。在撰写本书时，这已更改为默认的`{w:1}`选项，而`{w:0}`选项已被弃用。
- en: In part **II** of the diagram, which falls between the driver and the server,
    the write concern we are talking about is `{w:1}`. The driver waits for an acknowledgement
    from the server for the write operation to complete. Note that the server responding
    doesn't mean that the write operation was made durable. It means that the change
    just got updated into the memory, all the constraints were checked, and any exception
    will be reported to the client, unlike the previous two write concerns we saw.
    This is a relatively safe write concern mode, which will be fast, but there is
    still a slim chance of the data being lost if it crashes in those few milliseconds
    when the data was written to the journal from the memory. For most use cases,
    this is a good option to set. Hence, this is the default write concern mode.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的**II**部分，位于驱动程序和服务器之间，我们讨论的写入关注点是`{w:1}`。驱动程序等待服务器对写入操作的确认完成。请注意，服务器的响应并不意味着写入操作已经持久化。这意味着更改刚刚更新到内存中，所有约束都已经检查，并且任何异常都将报告给客户端，与我们之前看到的两种写入关注点不同。这是一个相对安全的写入关注点模式，将会很快，但如果在数据从内存写入日志时发生崩溃，仍然有一些数据丢失的可能性。对于大多数用例来说，这是一个不错的选择。因此，这是默认的写入关注点模式。
- en: Moving on, we come to part **III** of the diagram, which is from the entry point
    into the server as far as the journal. The write concern we are looking for here
    is at `{j:1}` or `{j:true}`. This write concern ensures a response to the invoking
    client only when the write operation is written to the journal. What is a journal
    though? This is something that we saw in depth in [Chapter 4](ch04.html "Chapter 4. Administration"),
    *Administration*, but for now, we will just look at a mechanism that ensures that
    the writes are made durable and the data on the disk doesn't get corrupted in
    the event of server crashes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来到图表的**III**部分，从服务器的入口点到日志。我们在这里寻找的写入关注点是`{j:1}`或`{j:true}`。这种写入关注点确保只有当写入操作写入日志时，才会向调用客户端返回响应。但是什么是日志呢？这是我们在[第4章](ch04.html
    "第4章。管理")中深入了解的内容，但现在，我们只看一种机制，确保写入是持久的，数据在服务器崩溃时不会损坏。
- en: Finally, let's come to part **IV** of the diagram; the write concern we are
    talking about is `{fsync:true}`. This requires that the data be flushed to disk
    to get before sending the response back to the client. In my opinion, when journaling
    is enabled, this operation doesn't really add any value, as journaling ensures
    data persistence even on server crash. Only when journaling is disabled does this
    option ensure that the write operation is successful when the client receives
    a success response. If the data is really important, journaling should never be
    disabled in the first place as it also ensures that the data on the disk doesn't
    get corrupted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来到图表的**IV**部分；我们讨论的写入关注点是`{fsync:true}`。这要求在向客户端发送响应之前将数据刷新到磁盘。在我看来，当启用日志记录时，这个操作实际上并没有增加任何价值，因为日志记录确保即使在服务器崩溃时也能保持数据持久性。只有在禁用日志记录时，此选项才能确保客户端接收到成功响应时写入操作成功。如果数据真的很重要，首先不应该禁用日志记录，因为它还确保磁盘上的数据不会损坏。
- en: We have seen some basic write concerns for a single-node server or those relevant
    to the primary node only in a replica set.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了单节点服务器的一些基本写入关注点，或者仅适用于复制集中的主节点的写入关注点。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An interesting thing to discuss is, what if we have a write concern such as
    `{w:0, j:true}`? We do not wait for the server's acknowledgement and also ensure
    that the write has been made to the journal. In this case, journaling flag takes
    precedence and the client waits for the acknowledgement of the write operation.
    One should avoid setting such ambiguous write concerns to avoid unpleasant surprises.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论一个有趣的事情是，如果我们有一个写关注，比如`{w:0, j:true}`？我们不等待服务器的确认，同时确保写入已经被记录到日志中。在这种情况下，日志标志优先，并且客户端等待写操作的确认。应该避免设置这种模棱两可的写关注，以避免不愉快的惊喜。
- en: 'We will now talk about write concern when it involves secondary nodes of a
    replica set as well. Let''s take a look at the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论涉及副本集辅助节点的写关注。让我们看一下下面的图表：
- en: '![Write concern and its significance](img/1943OS_Appendix_02.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![写关注及其重要性](img/1943OS_Appendix_02.jpg)'
- en: Any write concern with a `w` value greater than one indicates that secondary
    nodes too need to acknowledge before sending a response back. As seen in the preceding
    diagram, when a primary node gets a write operation, it propagates that operation
    to all secondary nodes. As soon as it gets a response from a predetermined number
    of secondary nodes, it acknowledges the client that the write has been successful.
    For example, when we have a write concern `{w:3}`, it means that the client should
    be sent a response only when three nodes in the cluster acknowledge the write.
    These three nodes include the primary node. Hence, it is now down to two secondary
    nodes to respond back for a successful write operation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任何`w`值大于一的写关注都表示在发送响应之前，辅助节点也需要确认。如前图所示，当主节点接收写操作时，它将该操作传播到所有辅助节点。一旦它从预定数量的辅助节点收到响应，它就向客户端确认写操作已成功。例如，当我们有一个写关注`{w:3}`时，这意味着只有当集群中的三个节点确认写操作时，客户端才会收到响应。这三个节点包括主节点。因此，现在只有两个辅助节点需要对成功的写操作做出响应。
- en: 'However, there is a problem with providing a number for the write concern.
    We need to know the number of nodes in the cluster and accordingly set the value
    of `w`. A low value will send an acknowledgement to a few nodes replicating the
    data. A value too high may unnecessarily slow the response back to the client,
    or in some cases, might not send a response at all. Suppose you have a three-node
    replica set and we have `{w:4}` as the write concern, the server will not send
    an acknowledgement till the data is replicated to three secondary nodes, which
    do not exist as we have just two secondary nodes. Thus, the client waits for a
    very long time to hear from the server about the write operation. There are a
    couple of ways to address this problem:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为写关注提供一个数字存在问题。我们需要知道集群中节点的数量，并相应地设置`w`的值。较低的值将向复制数据的少数节点发送确认。值太高可能会不必要地减慢向客户端的响应，或者在某些情况下可能根本不发送响应。假设您有一个三节点副本集，我们的写关注是`{w:4}`，服务器将在数据复制到三个不存在的辅助节点时才发送确认，因为我们只有两个辅助节点。因此，客户端需要很长时间才能从服务器那里得知写操作的情况。解决这个问题有几种方法：
- en: Use the `wtimeout` key and specify the timeout for the write concern. This will
    ensure that a write operation will not block for longer than the time specified
    (in milliseconds) for the `wtimeout` field of the write concern. For example,
    `{w:3, wtimeout:10000}` ensures that the write operation will not block more than
    10 seconds (10,000 ms), after which an exception will be thrown to the client.
    In the case of Java, a `WriteConcernException` will be thrown with the root cause
    message stating the reason as timeout. Note that this exception does not rollback
    the write operation. It just informs the client that the operation did not get
    completed in the specified amount of time. It might later be completed on the
    server side, some time after the client receives the timeout exception. It is
    up to the application program to deal with the exception and programmatically
    take the corrective steps. The message for the timeout exception does convey some
    interesting details, which we will see on executing the test program for the write
    concern.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`wtimeout`键并指定写关注的超时时间。这将确保写操作不会阻塞超过`wtimeout`字段指定的时间（以毫秒为单位）。例如，`{w:3, wtimeout:10000}`确保写操作不会阻塞超过10秒（10,000毫秒），之后将向客户端抛出异常。在Java的情况下，将抛出`WriteConcernException`，根本原因消息将说明超时的原因。请注意，此异常不会回滚写操作。它只是通知客户端操作在指定的时间内未完成。它可能在客户端收到超时异常后的一段时间内在服务器端完成。由应用程序来处理异常并以编程方式采取纠正措施。超时异常的消息传达了一些有趣的细节，我们将在执行写关注的测试程序时看到。
- en: A better way to specify the value of `w`, in the case of replica sets, is by
    specifying the value as `majority`. This write concern automatically identifies
    the number of nodes in a replica set and sends an acknowledgement back to the
    client when the data is replicated to a majority of nodes. For example, if the
    write concern is `{w:"majority"}` and the number of nodes in a replica set is
    three, then `majority` will be `2`. Whereas, at the later point in time, when
    we change the number of nodes to five, the `majority` will be `3` nodes. The number
    of nodes to form a majority automatically gets computed when the write concern's
    value is given as `majority`.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在副本集的情况下，指定`w`的更好方法是将值指定为`majority`。这种写关注会自动识别副本集中的节点数，并在数据复制到大多数节点时向客户端发送确认。例如，如果写关注是`{w:"majority"}`，并且副本集中的节点数为三，则`majority`将是`2`。而在以后，当我们将节点数更改为五时，`majority`将是`3`个节点。当写关注的值给定为`majority`时，自动计算形成大多数所需的节点数。
- en: Now, let us put the concepts we discussed into use and execute a test program
    that will demonstrate some of the concepts we just saw.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们讨论的概念付诸实践，并执行一个测试程序，演示我们刚刚看到的一些概念。
- en: Setting up a replica set
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立副本集
- en: To set up a replica set, you should know how to start the basic replica set
    with three nodes. Refer to the *Starting multiple instances as part of a replica
    set* recipe in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*. This recipe is built on that recipe because
    it needs an additional configuration while starting the replica set, which we
    will discuss in the next section. Note that the replica used here has a slight
    change in configuration to the one you have used earlier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置副本集，您应该知道如何启动具有三个节点的基本副本集。参考[第1章](ch01.html "第1章。安装和启动服务器") *安装和启动服务器*中的*作为副本集的一部分启动多个实例*配方。这个配方是基于那个配方构建的，因为在启动副本集时需要额外的配置，我们将在下一节中讨论。请注意，此处使用的副本与您之前使用的副本在配置上有轻微变化。
- en: Here, we will use a Java program to demonstrate various write concerns and their
    behavior. The *Connecting to a single node using a Java client* recipe in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server*, should be visited until Maven is set up. This can be a bit
    inconvenient if you are coming from a non-Java background.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一个Java程序来演示各种写入关注点及其行为。在[第1章](ch01.html "第1章。安装和启动服务器") *安装和启动服务器*中的*使用Java客户端连接单个节点*配方中，直到设置Maven之前，应该被访问。如果您来自非Java背景，这可能有点不方便。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Java project named `Mongo Java` is available for download at the book''s
    website. If the setup is complete, you can test the project just by executing
    the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Java项目名为`Mongo Java`可在该书的网站上下载。如果设置完成，只需执行以下命令即可测试该项目：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code for this project is available for download at the book's website. Download
    the project named `WriteConcernTest` and keep it on a local drive ready for execution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码可在该书的网站上下载。下载名为`WriteConcernTest`的项目，并将其保存在本地驱动器上以备执行。
- en: 'So, let''s get started:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们开始吧：
- en: 'Prepare the following configuration file for the replica set. This is identical
    to the config file that we saw in the *Starting multiple instances as part of
    a replica set* recipe in [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server*, where we set up the replica
    set, as follows, with just one difference, `slaveDelay:5`, `priority:0`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为副本集准备以下配置文件。这与我们在[第1章](ch01.html "第1章。安装和启动服务器") *安装和启动服务器*中的*作为副本集的一部分启动多个实例*配方中看到的配置文件相同，我们在那里设置了副本集，只有一个区别，`slaveDelay:5`，`priority:0`：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Use this config to start a three-node replica set, with one node listening to
    port `27000`. The others can be any ports of your choice, but stick to `27001`
    and `27002` if possible (we need to update the config accordingly if we decide
    to use a different port number). Also, remember to set the name of the replica
    set as `replSetTest` for the `replSet` command-line option while starting the
    replica set. Give some time to the replica set to come up before going ahead with
    next step.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此配置启动一个三节点副本集，其中一个节点监听端口`27000`。其他节点可以是您选择的任何端口，但如果可能的话，请坚持使用`27001`和`27002`（如果决定使用不同的端口号，我们需要相应更新配置）。还要记得在启动副本集时，将副本集的名称设置为`replSetTest`，并将其作为`replSet`命令行选项。在继续下一步之前，请给副本集一些时间来启动。
- en: At this point, the replica set with the earlier mentioned specifications should
    be up and running. We will now execute the test code provided in Java, to observe
    some interesting facts and behaviors of different write concerns. Note that this
    program also tries to connect to a port where no Mongo process is listening for
    connections. The port chosen is `20000`; ensure that before running the code,
    no server is up and running and listening to port `20000`.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，具有前述规格的副本集应该已经启动并运行。我们现在将执行Java中提供的测试代码，以观察不同写入关注点的一些有趣事实和行为。请注意，此程序还尝试连接到没有Mongo进程监听连接的端口。选择的端口是`20000`；在运行代码之前，请确保没有服务器正在运行并监听端口`20000`。
- en: 'Go to the root directory of the `WriteConcernTest` project and execute the
    following command:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`WriteConcernTest`项目的根目录并执行以下命令：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It should take some time to execute completely, depending on your hardware configuration.
    Roughly around 35 to 40 seconds were taken on my machine, which has a spinning
    disk drive with a 7200 RPM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些时间才能完全执行，具体取决于您的硬件配置。在我的机器上大约花了35到40秒的时间，我的机器上有一个7200转的传统硬盘。
- en: Before we continue analyzing the logs, let us see what those two additional
    fields added to the config file to set up the replica were. The `slaveDelay` field
    indicates that the particular slave (the one listening on port `27002` in this
    case) will lag behind the primary by 5 seconds. That is, the data being replicated
    currently on this replica node will be the one that was added on to the primary
    5 seconds ago. Secondly, this node can never be a primary and hence, the `priority`
    field has to be added with the value `0`. We have already seen this in detail
    in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续分析日志之前，让我们看看添加到配置文件中设置副本的这两个附加字段是什么。`slaveDelay`字段表示特定的副本（在本例中监听端口`27002`的副本）将比主节点滞后5秒。也就是说，当前在该副本节点上复制的数据是5秒前添加到主节点上的数据。其次，该节点永远不能成为主节点，因此必须添加`priority`字段并赋值为`0`。我们已经在[第4章](ch04.html
    "第4章。管理") *管理*中详细介绍了这一点。
- en: 'Let us now analyze the output from the preceding command''s execution. The
    Java class provided need not be looked at here; the output on the console is sufficient.
    Some of the relevant portions of the output console are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们分析前述命令执行的输出。这里不需要查看提供的Java类；控制台上的输出就足够了。输出控制台的一些相关部分如下：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first statement in the log states that we try to connect to a Mongo process
    listening on port `20000`. As there should not be a Mongo server running and listening
    to this port for client connections, all our write operations to this server should
    not succeed, and this will now give us a chance to see what happens when we use
    the write concerns `{w:-1}` and `{w:0}` and write to this nonexistent server.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 日志中的第一条语句说明我们尝试连接到一个监听端口`20000`的Mongo进程。由于不应该有Mongo服务器在此端口上运行并监听客户端连接，因此我们所有对该服务器的写操作都不应成功，现在我们有机会看看当我们使用写关注`{w:-1}`和`{w:0}`并向这个不存在的服务器写入时会发生什么。
- en: The next two lines in the output show that when we have the write concern `{w:-1}`,
    we do get a write result back, but it contains the error flag set to indicate
    a network error. However, no exception is thrown. In the case of the write concern
    `{w:0}`, we do get an exception in the client application for any network errors.
    Of course, all other write concerns ensuring a strict guarantee will throw an
    exception in this case too.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的下两行显示，当我们有写关注`{w:-1}`时，我们确实得到了写入结果，但其中包含了设置为指示网络错误的错误标志。但是，没有抛出异常。在写关注`{w:0}`的情况下，我们在客户端应用程序中对任何网络错误都会得到异常。当然，在这种情况下，所有其他确保严格保证的写关注也会抛出异常。
- en: 'Now we come to the portion of the code that connects to the replica set where
    one of the nodes is listening to port `27000` (if not, the code will show the
    error on the console and terminate). Now, we attempt to insert a document with
    a duplicate `_id` field (`{''_id'':''a''}`) into a collection, once with the write
    concern `{w:0}` and once with `{w:1}`. As we see in the console, the former (`{w:0}`)
    didn''t throw an exception and the insert went through successfully from the client''s
    perspective, whereas the latter (`{w:1}`) threw an exception to the client, indicating
    a duplicate key. The exception contains a lot of information about the server''s
    hostname and port, at the time when the exception occurred: the field for which
    the unique constraint failed; the client connection ID; error code; and the value
    that was not unique and caused the exception. The fact is that, even when the
    insert was performed using `{w:0}` as the write concern, it failed. However, as
    the driver didn''t wait for the server''s acknowledgement, it was never communicated
    about the failure.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来到连接到副本集的代码部分，其中一个节点正在监听端口`27000`（如果没有，代码将在控制台上显示错误并终止）。现在，我们尝试向集合中插入一个具有重复`_id`字段（`{'_id':'a'}`）的文档，一次使用写关注`{w:0}`，一次使用`{w:1}`。正如我们在控制台中看到的，前者（`{w:0}`）没有抛出异常，从客户端的角度来看插入成功进行了，而后者（`{w:1}`）向客户端抛出了异常，指示重复键。异常包含了关于服务器主机名和端口的大量信息，在异常发生时：唯一约束失败的字段；客户端连接ID；错误代码；以及导致异常的不唯一值。事实是，即使使用`{w:0}`作为写关注进行插入，它也失败了。但是，由于驱动程序没有等待服务器的确认，它从未被通知插入失败。
- en: Moving on, we now try to compute the time taken for the write operation to complete.
    The time shown here is the average of the time taken to execute the same operation
    with a given write concern five times. Note that these times will vary on different
    instances of execution of the program, and this method is just meant to give some
    rough estimates for our study. We can conclude from the output that the time taken
    for the write concern `{w:1}` is less than that of `{w:2}` (asking for an acknowledgement
    from one secondary node) and the time taken for `{w:2}` is less than `{j:true}`,
    which in turn is less than `{fsync:true}`. The next line of the output shows us
    that the average time taken for the write operation to complete is roughly 5 seconds
    when the write concern is `{w:3}`. Any guesses on why that is the case? Why does
    it take so long? The reason is, when `w` is `3`, we send an acknowledgement to
    the client only when two secondary nodes acknowledge the write operation. In our
    case, one of the nodes is delayed from the primary by about 5 seconds, and thus,
    it can acknowledge the write only after 5 seconds, and hence, the client receives
    a response from the server in roughly 5 seconds.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，我们现在尝试计算写操作完成所需的时间。这里显示的时间是执行相同操作的给定写关注五次所需时间的平均值。请注意，这些时间将在程序的不同执行实例上变化，这种方法只是为了给我们的研究提供一些粗略的估计。我们可以从输出中得出结论，写关注`{w:1}`所需的时间少于`{w:2}`（要求从一个辅助节点获得确认），而`{w:2}`所需的时间少于`{j:true}`，而`{j:true}`又少于`{fsync:true}`。输出的下一行告诉我们，当写关注为`{w:3}`时，写操作完成所需的平均时间大约为5秒。你猜为什么会这样吗？为什么会花这么长时间？原因是，当`w`为`3`时，我们只有在两个辅助节点确认写操作时才向客户端发送确认。在我们的情况下，一个节点比主节点延迟约5秒，因此只有在5秒后才能确认写操作，因此客户端大约在5秒后从服务器收到响应。
- en: Let us do a quick exercise here. What do you'll think would be the approximate
    response time when we have the write concern as `{w:'majority'}`? The hint here
    is, for a replica set of three nodes, two is the majority.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里做一个快速练习。当我们的写关注为`{w:'majority'}`时，你认为大约的响应时间会是多少？这里的提示是，对于一个三个节点的副本集，两个是大多数。
- en: 'Finally we see a timeout exception. Timeout is set using the `wtimeout` field
    of the document and is specified in milliseconds. In our case, we gave a timeout
    of 1000 ms, that is 1 second, and the number of nodes in the replica set to get
    an acknowledgement from before sending the response back to the client is 5 (four
    secondary instances). Thus, we have the write concern as `{w:5, wtimeout:1000}`.
    As our maximum number of nodes is three, the operation with the value of `w` set
    to `5` will wait for a very long time until two more secondary instances are added
    to the cluster. With the timeout set, the client returns and throws an error to
    the client, conveying some interesting details. The following is the JSON sent
    as an exception message:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们看到了超时异常。超时是使用文档的`wtimeout`字段设置的，以毫秒为单位。在我们的情况下，我们设置了1000毫秒的超时，即1秒，并且在将响应发送回客户端之前从副本集中获得确认的节点数为5（四个从实例）。因此，我们的写关注是`{w:5,
    wtimeout:1000}`。由于我们的最大节点数为三个，所以将`w`设置为`5`的操作将等待很长时间，直到集群中添加了另外两个从实例。设置超时后，客户端返回并向客户端抛出错误，传达一些有趣的细节。以下是作为异常消息发送的JSON：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let us look at the interesting fields. We start with the `n` field. This indicates
    the number of documents updated. As in this case it is an insert and not an update,
    it stays `0`. The `wtimeout` and `waited` fields tell us whether the transaction
    did timeout and the amount of time for which the client waited for a response;
    in this case 1000 ms. The most interesting field is `writtenTo`. In this case,
    the insert was successful on these two nodes of the replica set when the operation
    timed out, and hence, it is seen in the array. The third node has a `slaveDelay`
    value of 5 seconds and, hence, the data is still not written to it. This proves
    that the timeout doesn't roll back the insert and it does go through successfully.
    In fact, the node with `slaveDelay` will also have the data after 5 seconds, even
    if the operation times out, and this makes perfect sense as it keeps the primary
    and secondary instances in sync. It is the responsibility of the application to
    detect such timeouts and handle them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看有趣的字段。我们从`n`字段开始。这表示更新的文档数量。在这种情况下，它是一个插入而不是更新，所以保持为`0`。`wtimeout`和`waited`字段告诉我们事务是否超时以及客户端等待响应的时间；在这种情况下是1000毫秒。最有趣的字段是`writtenTo`。在这种情况下，插入在超时时成功在副本集的这两个节点上，并且因此在数组中看到。第三个节点的`slaveDelay`值为5秒，因此数据仍未写入。这证明超时不会回滚插入，它确实成功进行。实际上，即使操作超时，具有`slaveDelay`的节点也将在5秒后拥有数据，这是有道理的，因为它保持主节点和从节点同步。应用程序有责任检测此类超时并处理它们。
- en: Read preference for querying
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询的读取偏好
- en: In the previous section, we saw what a write concern is and how it affects the
    write operations (insert, update, and delete). In this section, we will see what
    a read preference is and how it affects query operations. We'll discuss how to
    use a read preference in separate recipes, to use specific programming language
    drivers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了写关注是什么以及它如何影响写操作（插入、更新和删除）。在本节中，我们将看到读取偏好是什么以及它如何影响查询操作。我们将讨论如何在单独的配方中使用读取偏好，以使用特定的编程语言驱动程序。
- en: When connected to an individual node, query operations will be allowed by default
    when connected to a primary, and in case if it is connected to a secondary node,
    we need to explicitly state that it is ok to query from secondary instances by
    executing `rs.slaveOk()` from the shell.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当连接到单个节点时，默认情况下允许查询操作连接到主节点，如果连接到从节点，则需要明确声明可以通过在shell中执行`rs.slaveOk()`来从从实例查询。
- en: However, consider connecting to a Mongo replica set from an application. It
    will connect to the replica set and not a single instance from the application.
    Depending on the nature of the application, it might always want to connect to
    a primary; always to a secondary; prefer connecting to a primary node but would
    be ok to connect to a secondary node in some scenarios and vice versa and finally,
    it might connect to the instance geographically close to it (well, most of the
    time).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑从应用程序连接到Mongo副本集。它将连接到副本集，而不是从应用程序连接到单个实例。根据应用程序的性质，它可能总是想要连接到主节点；总是连接到从节点；更喜欢连接到主节点，但在某些情况下连接到从节点也可以，反之亦然，最后，它可能连接到地理位置靠近它的实例（嗯，大部分时间）。
- en: 'Thus, the read preference plays an important role when connected to a replica
    set and not to a single instance. In the following table, we will see the various
    read preferences that are available and what their behavior is in terms of querying
    a replica set. There are five of them and the names are self-explanatory:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，读取偏好在连接到副本集而不是单个实例时起着重要作用。在下表中，我们将看到各种可用的读取偏好以及它们在查询副本集方面的行为。共有五种，名称不言自明：
- en: '| Read preference | Description |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 读取偏好 | 描述 |'
- en: '| --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `primary` | This is the default mode and it allows queries to be executed
    only on primary instances. It is the only mode that guarantees the most recent
    data, as all writes have to go through a primary instance. Read operations however
    will fail if no primary is available, which happens for a few moments when a primary
    goes down and continues till a new primary is chosen. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `primary` | 这是默认模式，它允许查询仅在主实例上执行。这是唯一保证最新数据的模式，因为所有写操作都必须通过主实例进行。然而，如果没有主实例可用，读操作将失败，这在主机宕机并持续到选择新的主机时会发生一段时间。|'
- en: '| `primaryPreferred` | This is identical to the preceding primary read preference,
    except that during a failover, when no primary is available, it will read data
    from the secondary and those are the times when it possibly doesn''t read the
    most recent data. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `primaryPreferred` | 这与前面的主读取偏好相同，只是在故障切换期间，当没有主机可用时，它将从从节点读取数据，这些时候可能不会读取到最新数据。|'
- en: '| `secondary` | This is exactly the opposite to the default primary read preference.
    This mode ensures that read operations never go to a primary and a secondary is
    chosen always. The chances of reading inconsistent data that is not updated to
    the latest write operation are maximal in this mode. It, however, is ok (in fact,
    preferred) for applications that do not face end users and are used for some instances
    to get hourly statistics and analytics jobs used for in-house monitoring, where
    the accuracy of the data is least important, but not adding a load to the primary
    instance is key. If no secondary instance is available or reachable, and only
    a primary instance is, the read operation will fail. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `secondary` | 这与默认的primary读取偏好完全相反。此模式确保读取操作永远不会转到primary，而总是选择secondary。在这种模式下，读取不一致的数据的机会最大，因为它没有更新到最新的写操作。但是，对于不面向最终用户并且用于某些实例获取每小时统计和分析作业的应用程序来说，这是可以接受的（事实上是首选），其中数据的准确性最不重要，但不会增加对primary实例的负载是关键的。如果没有secondary实例可用或可达，只有primary实例，读取操作将失败。|'
- en: '| `secondaryPreferred` | This is similar to the preceding secondary read preference,
    in all aspects except that if no secondary is available, the read operations will
    go to the primary instance. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `secondaryPreferred` | 这与前面的secondary读取偏好类似，除了如果没有secondary可用，读取操作将转到primary实例。
    |'
- en: '| `nearest` | This, unlike all the preceding read preferences, can connect
    either to a primary or a secondary. The primary objective for this read preference
    is minimum latency between the client and an instance of a replica set. In the
    majority of the cases, owing to the network latency and with a similar network
    between the client and all instances, the instance chosen will be one that is
    geographically close. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `nearest` | 与所有先前的读取偏好不同，这可以连接到primary或secondary。这种读取偏好的主要目标是客户端和副本集实例之间的最小延迟。在大多数情况下，由于网络延迟和客户端与所有实例之间的相似网络，所选择的实例将是地理上接近的实例。
    |'
- en: Similar to how write concerns can be coupled with shard tags, read preferences
    can also be used along with shard tags. As the concept of tags has already been
    introduced in [Chapter 4](ch04.html "Chapter 4. Administration"), *Administration*,
    you can refer to it for more details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与写关注可以与分片标签结合使用类似，读取偏好也可以与分片标签一起使用。由于标签的概念已经在[第4章](ch04.html "第4章。管理")中介绍过，您可以参考它以获取更多详细信息。
- en: 'We just saw what the different types of read preferences are (except for those
    using tags) but the question is, how do we use them? We have covered Python and
    Java clients in this book and will see how to use them in their respective recipes.
    We can set read preferences at various levels: at the client level, collection
    level, and query level, with the one specified at the query level overriding any
    other read preference set previously.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了不同类型的读取偏好（除了使用标签的那些），但问题是，我们如何使用它们？本书中涵盖了Python和Java客户端，并将看到如何在它们各自的示例中使用它们。我们可以在各个级别设置读取偏好：在客户端级别、集合级别和查询级别，查询级别指定的读取偏好将覆盖先前设置的任何其他读取偏好。
- en: 'Let us see what the nearest read preference means. Conceptually, it can be
    visualized as something like the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最近的读取偏好意味着什么。从概念上讲，它可以被可视化为以下图表：
- en: '![Read preference for querying](img/1943OS_Appendix_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![查询的读取偏好](img/1943OS_Appendix_03.jpg)'
- en: A Mongo replica set is set up with one secondary, which can never be a primary,
    in a separate data center and two (one primary and a secondary) in another data
    center. An identical application deployed in both the data centers, with a primary
    read preference, will always connect to the primary instance in **Data Center
    I**. This means, for the application in **Data Center II**, the traffic goes over
    the public network, which will have high latency. However, if the application
    is ok with slightly stale data, it can set the read preference as the nearest,
    which will automatically let the application in **Data Center I** connect to an
    instance in **Data Center I** and will allow an application in **Data Center II**
    to connect to the secondary instance in **Data Center II**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo副本集设置了一个secondary，它永远不会成为primary，在一个单独的数据中心，另一个数据中心有两个（一个primary和一个secondary）。在两个数据中心都部署了相同的应用程序，使用primary读取偏好，将始终连接到**数据中心I**中的primary实例。这意味着，对于**数据中心II**中的应用程序，流量将通过公共网络，这将具有较高的延迟。但是，如果应用程序可以接受略有陈旧的数据，它可以将读取偏好设置为最近，这将自动让**数据中心I**中的应用程序连接到**数据中心I**中的实例，并允许**数据中心II**中的应用程序连接到**数据中心II**中的secondary实例。
- en: But then the next question is, how does the driver know which one is the nearest?
    The term "geographically close" is misleading; it is actually the one with the
    minimum network latency. The instance we query might be geographically further
    than another instance in the replica set, but it can be chosen just because it
    has an acceptable response time. Generally, better response time means geographically
    closer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 但接下来的问题是，驱动程序如何知道哪一个是最近的？术语“地理上接近”是误导的；实际上是具有最小网络延迟的那个。我们查询的实例可能在地理上比副本集中的另一个实例更远，但它可能被选择，只是因为它具有可接受的响应时间。通常，更好的响应时间意味着地理上更接近。
- en: The following section is for those interested in internal details from the driver
    on how the nearest node is chosen. If you are happy with just the concepts and
    not the internal details, you can safely skip the rest of the contents.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分是为那些对驱动程序内部细节感兴趣的人准备的，关于最近节点是如何选择的。如果您只对概念感兴趣而不关心内部细节，可以放心地跳过其余内容。
- en: Knowing the internals
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解内部情况
- en: 'Let us see some pieces of code from a Java client (driver 2.11.3 is used for
    this purpose) and make some sense out of it. If we look at the `com.mongodb.TaggableReadPreference.NearestReadPreference.getNode`
    method, we see the following implementation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下来自Java客户端（用于此目的的驱动程序为2.11.3）的一些代码片段，并对其进行一些解释。如果我们查看`com.mongodb.TaggableReadPreference.NearestReadPreference.getNode`方法，我们会看到以下实现：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For now, if we ignore the contents where tags are specified, all it does is
    execute `set.getAMember()`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，如果我们忽略指定标签的内容，它所做的就是执行`set.getAMember()`。
- en: 'The name of this method tells us that there is a set of replica set members
    and we returned one of them randomly. Then what decides whether the set contains
    a member or not? If we dig a bit further into this method, we see the following
    lines of code in the `com.mongodb.ReplicaSetStatus.ReplicaSet` class:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的名称告诉我们，有一组副本集成员，我们随机返回其中一个。那么是什么决定了集合是否包含成员？如果我们再深入一点研究这个方法，我们会在`com.mongodb.ReplicaSetStatus.ReplicaSet`类中看到以下代码行：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Ok, so all it does is pick one from a list of replica set nodes maintained internally.
    Now, the random pick can be a secondary, even if a primary can be chosen (because
    it is present in the list). Thus, we can now say that when the nearest is chosen
    as a read preference, and even if a primary is in the list of contenders, it might
    not necessarily be chosen randomly.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，它所做的就是从内部维护的副本集节点列表中选择一个。现在，随机选择可以是一个secondary，即使可以选择一个primary（因为它存在于列表中）。因此，我们现在可以说当最近的节点被选择为读取偏好时，即使主节点在候选者列表中，也可能不会被随机选择。
- en: 'The question now is, how is the `acceptableMembers` list initialized? We see
    it is done in the constructor of the `com.mongodb.ReplicaSetStatus.ReplicaSet`
    class as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，`acceptableMembers`列表是如何初始化的？我们看到它是在`com.mongodb.ReplicaSetStatus.ReplicaSet`类的构造函数中完成的，如下所示：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `calculateBestPingTime` line just finds the best ping time of all (we will
    see what this ping time is later).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculateBestPingTime`行只是找到所有ping时间中的最佳时间（稍后我们将看到这个ping时间是什么）。'
- en: 'Another parameter worth mentioning is `acceptableLatencyMS`. This gets initialized
    in `com.mongodb.ReplicaSetStatus.Updater` (this is actually a background thread
    that updates the status of the replica set continuously), and the value for `acceptableLatencyMS`
    is initialized as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的另一个参数是`acceptableLatencyMS`。这在`com.mongodb.ReplicaSetStatus.Updater`中初始化（实际上是一个不断更新副本集状态的后台线程），`acceptableLatencyMS`的值初始化如下：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, this code searches for the system variable called `com.mongodb.slaveAcceptableLatencyMS`,
    and if none is found, it initializes to the value `15`, which is 15 ms.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，这段代码搜索名为`com.mongodb.slaveAcceptableLatencyMS`的系统变量，如果找不到，则初始化为值`15`，即15毫秒。
- en: 'This `com.mongodb.ReplicaSetStatus.Updater` class also has a `run` method that
    periodically updates the replica set stats. Without getting too much into it,
    we can see that it calls `updateAll`, which eventually reaches the `update` method
    in `com.mongodb.ConnectionStatus.UpdatableNode`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`com.mongodb.ReplicaSetStatus.Updater`类还有一个`run`方法，定期更新副本集的统计信息。不深入研究，我们可以看到它调用`updateAll`，最终到达`com.mongodb.ConnectionStatus.UpdatableNode`中的`update`方法。
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: All it does is execute the `{isMaster:1}` command and record the response time
    in nanoseconds. This response time is converted to milliseconds and stored as
    the ping time. So, coming back to the `com.mongodb.ReplicaSetStatus.ReplicaSet`
    class it stores, all `calculateGoodMembers` does is find and add the members of
    a replica set that are no more than `acceptableLatencyMS` milliseconds more than
    the best ping time found in the replica set.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它所做的就是执行`{isMaster:1}`命令并记录响应时间（以纳秒为单位）。这个响应时间转换为毫秒并存储为ping时间。所以，回到`com.mongodb.ReplicaSetStatus.ReplicaSet`类中，`calculateGoodMembers`所做的就是找到并添加副本集中不超过`acceptableLatencyMS`毫秒的成员，这些成员的ping时间不超过副本集中找到的最佳ping时间。
- en: For example, in a replica set with three nodes, the ping times from the client
    to the three nodes (node 1, node 2, and node 3) are 2 ms, 5 ms, and 150 ms respectively.
    As we see, the best time is 2 ms and hence, node 1 goes into the set of good members.
    Now, from the remaining nodes, all those with a latency that is no more than `acceptableLatencyMS`
    more than the best, which is *2 + 15 ms = 17 ms*, as 15 ms is the default that
    will be considered. Thus, node 2 is also a contender, leaving out node 3\. We
    now have two nodes in the list of good members (good in terms of latency).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个有三个节点的副本集中，客户端到三个节点（节点1、节点2和节点3）的ping时间分别为2毫秒、5毫秒和150毫秒。正如我们所见，最佳时间是2毫秒，因此节点1进入了良好成员的集合中。现在，从剩下的节点中，所有延迟不超过最佳时间的`acceptableLatencyMS`的节点也是候选者，即*2
    + 15毫秒 = 17毫秒*，因为15毫秒是默认值。因此，节点2也是一个候选者，剩下的是节点3。现在我们有两个节点在良好成员的列表中（从延迟的角度来看是好的）。
- en: Now, putting together all that we saw on how it would work for the scenario
    we saw in the preceding diagram, the least response time will be from one of the
    instances in the same data center (from the programming language driver's perspective
    in these two data centers), as the instance(s) in other data centers might not
    respond within 15 ms (the default acceptable value) more than the best response
    time due to public network latency. Thus, the acceptable nodes in **Data Center
    I** will be two of the replica set nodes in that data center, and one of them
    will be chosen at random, and for **Data Center II**, only one instance is present
    and is the only option. Hence, it will be chosen by the application running in
    that data center.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将我们在前面的图表中看到的所有内容整合起来，最小的响应时间将来自同一数据中心中的一个实例（从这两个数据中心的编程语言驱动程序的角度来看），因为其他数据中心中的实例可能由于公共网络延迟而无法在15毫秒（默认可接受值）内响应。因此，**数据中心I**中的可接受节点将是该数据中心中的两个副本集节点，其中一个将被随机选择，而对于**数据中心II**，只有一个实例存在，也是唯一的选择。因此，它将由在该数据中心运行的应用程序选择。
