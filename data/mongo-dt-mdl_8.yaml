- en: Chapter 8. Logging and Real-time Analytics with MongoDB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。使用MongoDB进行日志记录和实时分析
- en: Throughout this book, many concepts you already knew were presented to you.
    You have learned how to use them in conjunction with the techniques and tools
    that MongoDB offers to us. The goal of this chapter is to apply these techniques
    in a real-life example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，向您介绍了许多您已经了解的概念。您已经学会了如何将它们与MongoDB提供给我们的技术和工具结合使用。本章的目标是在一个真实的例子中应用这些技术。
- en: The real-life example we will develop in this chapter explains how to use MongoDB
    as a persistent storage for a web server's log data—to be more specific, the data
    from an Nginx web server. By doing this, we will be able to analyze the traffic
    data of a web application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中开发的真实例子解释如何使用MongoDB作为网络服务器日志数据的持久存储，更具体地说，是来自Nginx网络服务器的数据。通过这样做，我们将能够分析Web应用程序的流量数据。
- en: We will start this chapter by analyzing the Nginx log format in order to define
    the information that will be useful for our experiment. After this, we will define
    the type of analysis we want to perform in MongoDB. Finally, we will design our
    database schema and implement, by using code, reading and writing data in MongoDB.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从分析Nginx日志格式开始，以定义对我们实验有用的信息。之后，我们将定义我们想要在MongoDB中执行的分析类型。最后，我们将设计我们的数据库模式，并通过使用代码在MongoDB中读写数据来实现。
- en: For this chapter, we will take into consideration that each host that generates
    this event consumes this information and sends it to MongoDB. Our focus will not
    be on the application's architecture or on the code we will produce in our example.
    So, kind reader, if you do not agree with the code snippets shown here, please,
    feel free to modify them or create a new one yourself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑到每个生成此事件的主机都会消耗这些信息并将其发送到MongoDB。我们的重点不在应用程序的架构或我们在示例中将生成的代码上。因此，亲爱的读者，如果您不同意这里显示的代码片段，请随意修改它们或自己创建一个新的。
- en: 'That said, this chapter will cover:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，本章将涵盖：
- en: Log data analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志数据分析
- en: What we are looking for
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在寻找的内容
- en: Designing the schema
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计模式
- en: Log data analysis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志数据分析
- en: The access log is often ignored by developers, system administrators, or anyone
    who keeps services on the web. But it is a powerful tool when we need prompt feedback
    on what is happening for each request on our web server.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志经常被开发人员、系统管理员或任何在网络上保持服务的人忽视。但是，当我们需要及时了解每个请求在我们的网络服务器上发生了什么时，它是一个强大的工具。
- en: 'The access log keeps information about the server''s activities and performance
    and also tells us about eventual problems. The most common web servers nowadays
    are Apache HTTPD and Nginx. These web servers have by default two log types: error
    logs and access logs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志保存有关服务器活动和性能的信息，还告诉我们有关可能问题。如今最常见的网络服务器是Apache HTTPD和Nginx。这些网络服务器默认情况下有两种日志类型：错误日志和访问日志。
- en: Error logs
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误日志
- en: As the name suggests, the error log is where the web server will store errors
    found during the processing of a received request. In general, this type of log
    is configurable and will write the messages according to the predefined severity
    level.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，错误日志是网络服务器在处理接收到的请求时发现的错误的存储位置。一般来说，这种类型的日志是可配置的，并且会根据预定义的严重级别写入消息。
- en: Access logs
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问日志
- en: The access log is where all the received and processed requests are stored.
    This will be the main object of our study.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志是存储所有接收和处理请求的地方。这将是我们研究的主要对象。
- en: 'The events written in the file are recorded in a predefined layout that can
    be formatted according to the wishes of those who are managing the server. By
    default, both Apache HTTPD and Nginx have a format known as **combined**. An example
    of a log generated in this format is presented as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中记录的事件以预定义的布局记录，可以根据服务器管理者的愿望进行格式化。默认情况下，Apache HTTPD和Nginx都有一个称为**combined**的格式。以这种格式生成的日志示例如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At first sight, it can be a little frightening to see too much information in
    only one log line. However, if we take a look at the pattern that is being applied
    in order to generate this log and try to examine it, we will see that it is not
    so difficult to understand.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，一行日志中包含太多信息可能有点吓人。然而，如果我们看一下生成此日志的模式并尝试对其进行检查，我们会发现它并不难理解。
- en: 'The pattern that generates this line on the Nginx web server is presented as
    follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 生成此行的Nginx网络服务器的模式如下所示：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will describe each part of this pattern so you get a better understanding:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述此模式的每个部分，以便您更好地理解：
- en: '`$remote_addr`: This is the IP address of the client that performed the request
    on the web server. In our example, this value corresponds to 191.32.254.162.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$remote_addr`：这是在网络服务器上执行请求的客户端的IP地址。在我们的示例中，该值对应于191.32.254.162。'
- en: '`$remote_user`: This is the authenticated user, if it exists. When an authenticated
    user is not identified, this field will be filled with a hyphen. In our example,
    the value is `-`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$remote_user`：这是经过身份验证的用户，如果存在的话。当未识别经过身份验证的用户时，此字段将填写连字符。在我们的示例中，该值为`-`。'
- en: '`[$time_local]`: This is the time that the request was received on the web
    server in the format `[day/month/year:hour:minute:second zone]`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[$time_local]`：这是请求在网络服务器上接收的时间，格式为`[day/month/year:hour:minute:second zone]`。'
- en: '`"$request"`: This is the client request itself. Also known as a request line.
    To get a better understanding, we will analyze our request line in the example:
    `"GET /admin HTTP/1.1"`. First, we have the HTTP verb used by the client. In this
    case, it was a `GET` HTTP verb. In the sequence, we have the resource accessed
    by the client. In this case, the resource accessed was `/admin`. And last, we
    have the protocol used by the client. In this case, `HTTP/1.1`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"$request"`：这是客户端请求本身。也称为请求行。为了更好地理解，我们将分析示例中的请求行：`"GET /admin HTTP/1.1"`。首先，我们有客户端使用的HTTP动词。在这种情况下，它是一个`GET`
    HTTP动词。接着，我们有客户端访问的资源。在这种情况下，访问的资源是`/admin`。最后，我们有客户端使用的协议。在这种情况下，是`HTTP/1.1`。'
- en: '`$status`: This is the HTTP status code replied to the client by the web server.
    The possible values for this field are defined in RFC 2616\. In our example, the
    web server returns the status code `200` to the client.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$status`：这是 Web 服务器向客户端返回的 HTTP 状态码。该字段的可能值在 RFC 2616 中定义。在我们的示例中，Web 服务器向客户端返回状态码`200`。'
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about RFC 2616, you can visit [http://www.w3.org/Protocols/rfc2616/rfc2616.txt](http://www.w3.org/Protocols/rfc2616/rfc2616.txt).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关 RFC 2616 的更多信息，您可以访问 [http://www.w3.org/Protocols/rfc2616/rfc2616.txt](http://www.w3.org/Protocols/rfc2616/rfc2616.txt)。
- en: '`$body_bytes_sent`: This is the length in bytes of the response body sent to
    the client. When you do not have a body, the value will be a hyphen. We must observe
    that this value does not include the request headers. In our example, the value
    is 2,529 bytes.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$body_bytes_sent`：这是发送给客户端的响应正文的字节长度。当没有正文时，该值将是一个连字符。我们必须注意，该值不包括请求头。在我们的示例中，该值为
    2,529 字节。'
- en: '`"$http_referer"`: This is the contained value in the header "Referer" of the
    client''s request. This value represents where the requested resource is referenced
    from. When the resource access is performed directly, this field is filled with
    a hyphen. In our example, the value is `-`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"$http_referer"`：这是客户端请求头中“Referer”中包含的值。该值表示所请求资源的引用位置。当直接执行资源访问时，该字段填充一个连字符。在我们的示例中，该值为“-”。'
- en: '`"$http_user_agent"`: This is the information that the client sends in the
    User-Agent header. Normally, it is in this header that we can identify the web
    browser used on the request. In our example, the value for this field is `"Mozilla/5.0
    (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.104
    Safari/537.36"`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"$http_user_agent"`：这是客户端在 User-Agent 头中发送的信息。通常，我们可以在该头中识别请求中使用的网络浏览器。在我们的示例中，该字段的值为`"Mozilla/5.0
    (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.104
    Safari/537.36"`。'
- en: 'Besides these, we have more variables available to create new log formats.
    Among these, we highlight:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们还有更多可用于创建新日志格式的变量。其中，我们要强调：
- en: '`$request_time`: This indicates the total time for the request processing'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$request_time`：这表示请求处理的总时间'
- en: '`$request_length`: This indicates the total length for the client response,
    including the headers'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$request_length`：这表示客户端响应的总长度，包括头部。'
- en: Now that we are familiar with the web server access log, we will define what
    we want to analyze in order to know what the information needs to be logged.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了 Web 服务器访问日志，我们将定义我们想要分析的内容，以了解需要记录哪些信息。
- en: What we are looking for
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们正在寻找的内容
- en: The information extracted from a web server access log is very rich and give
    us good material for infinite possibilities of study. Being simple and direct,
    it is possible to count the number of requests that our web server receives just
    by counting the number of lines that the access log has. But we can expand our
    analysis and try to measure the average of the data traffic in bytes over the
    time, for example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Web 服务器访问日志中提取的信息非常丰富，为我们提供了无限的研究可能性。简单直接，我们可以通过计算访问日志的行数来统计 Web 服务器接收的请求数。但我们可以扩展我们的分析，尝试测量数据流量的平均值，例如。
- en: Recently, one of the most widely used services is the application performance
    management system, also known as **APMs**. Nowadays, these services are commonly
    offered as software-as-a-service and the main goal is to give us a view of an
    application's performance and health.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，最广泛使用的服务之一是应用性能管理系统，也称为 **APM**。如今，这些服务通常作为软件即服务提供，并且主要目标是为我们提供应用程序性能和健康状况的视图。
- en: APMs are a good example of what can be analyzed based on the information extracted
    from the access log, due to the fact that a good part of the information that
    APMs generate is based on the access logs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: APM 是基于从访问日志中提取的信息进行分析的一个很好的例子，因为 APM 生成的大部分信息都是基于访问日志的。
- en: Attention! I am not saying that an APM works based only on the access log, but
    a good part of the information generated by APMs can be extracted from the access
    log. Okay?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！我并不是说 APM 仅基于访问日志工作，但 APM 生成的大部分信息可以从访问日志中提取。好吗？
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more, visit [https://en.wikipedia.org/wiki/Application_performance_management](https://en.wikipedia.org/wiki/Application_performance_management).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请访问 [https://en.wikipedia.org/wiki/Application_performance_management](https://en.wikipedia.org/wiki/Application_performance_management)。
- en: As said at the beginning of this chapter, we do not have any intention of coding
    or creating an entire system, but we will show in practice how we can keep the
    access log information for an eventual analysis using MongoDB.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头所说，我们并没有打算编写或创建整个系统，但我们将在实践中展示如何使用 MongoDB 保留访问日志信息以供可能的分析。
- en: Based on APMs, we will structure our example on an analysis of web server resource
    throughput. It is possible to perform this analysis only with the information
    contained on the web server access log. To do so, what data do we need in our
    access log? And should we use the combined format?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 APM，我们将结构化我们的示例，分析 Web 服务器资源吞吐量。只需使用 Web 服务器访问日志中包含的信息就可以进行此分析。为此，我们的访问日志需要哪些数据？我们应该使用组合格式吗？
- en: Measuring the traffic on the web server
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量 Web 服务器上的流量
- en: The throughput in our web server will be estimated based on the number of requests
    for a given period of time, that is, requests in a day, in an hour, in a minute,
    or in a second. The number of requests per minute is a very reasonable measure
    for a real-time monitoring.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Web 服务器吞吐量将根据一定时间段内的请求数进行估算，即一天内的请求数，一小时内的请求数，一分钟内的请求数或一秒内的请求数。每分钟的请求数是实时监控的一个非常合理的度量。
- en: The throughput is calculated by counting the requests processed in our web server.
    Because of this, it is not necessary to work with specific data from the access
    log. Nevertheless, in order to make possible a richer further analysis of our
    data, we will create a specific log format that will collect request information
    such as the HTTP status code, the request time, and length.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量是通过计算在我们的Web服务器中处理的请求来计算的。因此，不需要使用访问日志中的特定数据。然而，为了使我们的数据能够进行更丰富的进一步分析，我们将创建一个特定的日志格式，其中将收集请求信息，如HTTP状态码、请求时间和长度。
- en: Both Apache HTTP and Nginx allow us to customize the access log or to create
    a new file with a custom format. The second option seems to be perfect. Before
    we start to configure our web server, we will create our log format using the
    variables previously explained. Just remember that we are working on an Nginx
    web server.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Apache HTTP和Nginx都允许我们自定义访问日志或创建一个具有自定义格式的新文件。第二个选项似乎是完美的。在开始配置Web服务器之前，我们将使用先前解释的变量创建我们的日志格式。只需记住我们正在使用Nginx
    Web服务器。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we defined our log format, we can configure our Nginx web server. To do
    so, let''s perform the following steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义了我们的日志格式后，我们可以配置我们的Nginx Web服务器。为此，让我们执行以下步骤：
- en: 'First of all, to define this new format in Nginx, we need to edit the `nginx.conf`
    file, adding a new entry in the HTTP element with the new log format:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在Nginx中定义这种新格式，我们需要编辑`nginx.conf`文件，在HTTP元素中添加一个新的条目，其中包含新的日志格式：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we need to add another entry in `nginx.conf` file that defines in which
    file the new custom log will be written:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要在`nginx.conf`文件中添加另一个条目，定义新的自定义日志将写入哪个文件：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To apply our changes, reload the Nginx web server executing the following command
    in a terminal:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要应用我们的更改，请在终端中执行以下命令重新加载Nginx Web服务器：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After reloading the Nginx server, we can look at our new log file, `/var/log/nginx/custom_access.log`,
    and check whether the lines are like the following lines:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新加载Nginx服务器后，我们可以查看我们的新日志文件`/var/log/nginx/custom_access.log`，并检查行是否像以下行一样：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Log format configured, web server set up; it is time to design our schema.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 配置了日志格式，Web服务器设置好了；现在是设计我们的模式的时候了。
- en: Designing the schema
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式
- en: Something that has been repeated several times during this book is the importance
    of knowing our data and what it will be used for. And now, more than ever, in
    this practical exercise we will design our schema step by step, considering every
    detail involved in this.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中已经多次重复的一件事是了解我们的数据及其用途的重要性。现在，比以往任何时候，在这个实际练习中，我们将逐步设计我们的模式，考虑其中涉及的每个细节。
- en: In the previous section, we defined the data set that we want to extract from
    the web server access logs. The next step is to list the requirements that are
    associated with the clients of the database. As said previously, one process will
    be responsible for capturing the information from the log files and writing them
    in MongoDB, and another process will read the information already persisted in
    the database.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们定义了我们想要从Web服务器访问日志中提取的数据集。下一步是列出与数据库客户端相关的要求。如前所述，一个进程将负责从日志文件中捕获信息并将其写入MongoDB，另一个进程将读取已在数据库中持久化的信息。
- en: A point of concern is the performance during the document writing in MongoDB
    because it is important to ensure that the information will be generated almost
    in realtime. Since we do not have a previous estimation of the data volume per
    second, we will be optimistic. Let's consider that we will have a huge volume
    of data all the time coming from the web server to our MongoDB instance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得关注的问题是在MongoDB中写入文档时的性能，因为重要的是要确保信息几乎实时生成。由于我们没有对每秒数据量的先前估计，我们将持乐观态度。让我们假设我们将一直从Web服务器到我们的MongoDB实例中获得大量数据。
- en: Taking into account this requirement, we will be worried about the data dimensions
    that will increase over time. More events imply that more documents will be inserted.
    These are the main requirements for our system to operate well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一要求，我们将担心随着时间的推移数据维度将增加。更多的事件意味着将插入更多的文档。这些是我们的系统良好运行的主要要求。
- en: At first sight, we can imagine that it is all about defining the document format
    and persisting it into a collection. But thinking in this way is to ignore the
    well-known MongoDB schema flexibility. So we will analyze the problem of throughput
    in order to define where and how to persist the information.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，我们可以想象这一切都是关于定义文档格式并将其持久化到集合中。但这样想忽视了众所周知的MongoDB模式灵活性。因此，我们将分析吞吐量问题，以便定义在哪里以及如何持久化信息。
- en: Capturing an event request
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 捕获事件请求
- en: The web server throughput analysis is maybe the simplest task. In a simple way,
    the measure of the number of events will give us a number that represents the
    web server's throughput.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器吞吐量分析可能是最简单的任务。简单地说，事件数量的测量将给我们一个代表Web服务器吞吐量的数字。
- en: 'So, if for each generated event a write document is performed stating the time
    of this operation, does it mean that we can easily get the throughput? Yes! Thus,
    the easiest way to represent a MongoDB document where we can analyze the throughput
    is the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果对于每个生成的事件都执行了写文档操作，并声明了此操作的时间，这是否意味着我们可以轻松获得吞吐量？是的！因此，表示我们可以分析吞吐量的最简单的MongoDB文档的方式如下：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When executing the `count` method in the document collection, we will get the
    web server''s throughput value. Assuming that we have a collection called `events`,
    in order to find out the throughput, we must execute the following command in
    the mongod shell:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档集合中执行`count`方法时，我们将获得Web服务器的吞吐量值。假设我们有一个名为`events`的集合，为了找出吞吐量，我们必须在mongod
    shell中执行以下命令：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command returns the total of generated events until now on our web server.
    But, is this the number that we want? No. There is no point in having the total
    number of events without placing it in a given period of time. Would it be of
    any use to have 10,000 events processed by the web server until now without knowing
    when we started recording these events or even when the last event was generated?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令返回到目前为止在我们的Web服务器上生成的事件总数。但是，这是我们想要的数字吗？不是。在没有将其放在一定时间段内的情况下，拥有事件的总数是没有意义的。如果我们不知道我们何时开始记录这些事件，甚至最后一个事件生成的时间，那么到目前为止由Web服务器处理的10,000个事件有什么用呢？
- en: 'If we want to count the events in a given period of time, the easiest way to
    do so is by including a field that will represent the event''s creation date.
    An example for this document is shown as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在一定时间段内计算事件的数量，最简单的方法是包括一个代表事件创建日期的字段。这个文档的一个例子如下所示：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a result, we can check the total number of requests in the web server in
    a given period of time by executing a query. The simplest way to perform this
    query is to use the aggregation framework. The execution of the following command
    on the mongod shell will return the total of requests per minute:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过执行查询来检查在给定时间段内Web服务器上的总请求数。执行这个查询的最简单方法是使用聚合框架。在mongod shell上执行以下命令将返回每分钟的总请求数：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The aggregation pipeline has its limits. If the command result returns a single
    document that exceeds the BSON document size, an error is produced. Since MongoDB's
    2.6 release, the `aggregate` command returns a cursor, so it can return result
    sets of any size.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合管道有其限制。如果命令结果返回一个超过BSON文档大小的单个文档，就会产生错误。自MongoDB的2.6版本以来，`aggregate`命令返回一个游标，因此可以返回任意大小的结果集。
- en: You can find more about aggregation pipeline limits in the MongoDB reference
    manual at [http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/](http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在MongoDB参考手册的[http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/](http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/)找到更多关于聚合管道限制的信息。
- en: 'In the command pipeline, we defined the `$group` stage to group the documents
    per day, month, year, hour, and minute. And we count everything using the `$sum`
    operator. From this `aggregate` command''s execution, we will have, as an example,
    documents such as these:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令管道中，我们定义了`$group`阶段来按天、月、年、小时和分钟对文档进行分组。并且使用`$sum`运算符来计算所有内容。通过这个`aggregate`命令的执行，我们将得到如下的文档：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this output, it is possible to know how many requests the web server received
    in a certain time period. This happens due to the `$group` operator behavior,
    which takes documents that match a query and then collects groups of documents
    based on one or more fields. We took each part of our `$date_created` field, such
    as the month, day, year, hour, and minute, to the group stage of the aggregation
    pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，可以知道Web服务器在一定时间段内收到了多少请求。这是由于`$group`运算符的行为，它根据一个或多个字段收集与查询匹配的文档，并收集文档组。我们将聚合管道的组阶段的每个部分，如月、日、年、小时和分钟，都带入了我们的`$date_created`字段。
- en: 'If you want to know which resource is accessed the most often in your web server
    with the higher throughput, none of these options fit this request. However, a
    fast solution for this problem is easily reachable. At first sight, the fastest
    way is to deconstruct the event and create a more complex document, as you can
    see in the following example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道在您的Web服务器上哪个资源的吞吐量最高，但没有一个选项符合这个要求。然而，这个问题的快速解决方案很容易实现。乍一看，最快的方法是拆解事件并创建一个更复杂的文档，就像下面的例子所示：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By using this document design, it is possible to know the resource throughput
    per minute with the help of an aggregation framework:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这种文档设计，可以通过聚合框架知道每分钟的资源吞吐量：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding pipeline, the first step is to group by resource and to count
    how many times a request on the resource occurred during an entire day. The next
    step is to use the operator `$project` and, together with the operator `$divide`,
    use the number of hits in a given resource and calculate the average per minute
    by dividing by 1,440 minutes, that is, the total of minutes in a day or 24 hours.
    Finally, we order the results in descending order to view which resources have
    the higher throughput.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的管道中，第一步是按资源分组，并计算在整个一天内资源请求发生的次数。下一步是使用`$project`运算符，并且与`$divide`运算符一起使用给定资源中的点击次数，并通过1,440分钟进行平均计算，即一天或24小时的总分钟数。最后，我们按降序排序结果，以查看哪些资源具有更高的吞吐量。
- en: 'To keep things clear, we will execute the pipeline step by step and explain
    the results for each step. In the execution of the first phase, we have the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持清晰，我们将逐步执行管道并解释每个步骤的结果。在第一阶段的执行中，我们有以下结果：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This execution groups the event collection documents by field resource and
    counts the number of occurrences of field hits when we use the operator `$sum`
    with the value `1`. The returned result is demonstrated as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个执行通过字段资源对事件集合文档进行分组，并计算使用`$sum`和值`1`时字段点击的发生次数。返回的结果如下所示：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the second phase of the pipeline, we use the operator `$project`, which
    will give us the value of hits per minute:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道的第二阶段，我们使用`$project`运算符，它将给出每分钟的点击次数：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the result of this phase:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个阶段的结果：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The last phase of the pipeline is to order the results by throughput in descending
    order:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的最后阶段是按吞吐量降序排序结果：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output produced is like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的输出如下：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It looks like we succeeded in obtaining a good design for our document. Now
    we can extract the desired analysis and other analyses, as we will see further,
    and for that reason we can stop for now. Wrong! We will review our requirements,
    compare them to the model that we designed, and try to figure out whether it was
    the best solution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们成功地获得了我们文档的良好设计。现在我们可以提取所需的分析和其他分析，正如我们将在后面看到的那样，因此我们现在可以停下来。错了！我们将审查我们的要求，将它们与我们设计的模型进行比较，并尝试弄清楚是否这是最佳解决方案。
- en: Our desire is to know the measure of throughput per minute for all web server
    resources. In the model we designed, one document is created per event in our
    web server, and by using the aggregation framework we can calculate the information
    that we need for our analysis.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的愿望是知道每分钟对所有网络服务器资源的吞吐量。在我们设计的模型中，我们的网络服务器每个事件创建一个文档，通过使用聚合框架，我们可以计算我们分析所需的信息。
- en: What can be wrong in this solution? Well, if you think it's the number of documents
    in the collection, you are right. One document per event can generate huge collections
    depending on the web server traffic. Obviously, we can adopt the strategy of using
    shards, and distribute the collection through many hosts. But first, we will see
    how we can take advantage of the schema flexibility in MongoDB in order to reduce
    the collection size and optimize the queries.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案有什么问题？嗯，如果你认为是集合中的文档数量，那么你是对的。每个事件一个文档可能会生成巨大的集合，这取决于网络服务器的流量。显然，我们可以采用使用分片的策略，并将集合分布到许多主机上。但首先，我们将看看如何利用MongoDB中的模式灵活性来减少集合大小并优化查询。
- en: A one-document solution
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个文档解决方案
- en: Having one document per event may be advantageous if we consider that we will
    have a huge amount of information to create an analysis. But in the example that
    we are trying to solve, it is expensive to persist one document for each HTTP
    request.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑到我们将有大量信息来创建分析，那么每个事件一个文档可能是有利的。但在我们试图解决的例子中，为每个HTTP请求持久化一个文档是昂贵的。
- en: We will take advantage of the schema flexibility in MongoDB that will help us
    to grow documents over time. The following proposal has the main goal of reducing
    the number of persisted documents, also optimizing the queries for read and write
    operations in our collection.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用MongoDB中的模式灵活性，这将帮助我们随着时间的推移增加文档。以下提案的主要目标是减少持久化文档的数量，同时优化我们集合中的读取和写入操作的查询。
- en: 'The document we are looking for should be able to provide us all the information
    needed in order to know the resource throughput in requests per minute; thus we
    can have a document with this structure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找的文档应该能够为我们提供所有所需的信息，以便了解每分钟请求的资源吞吐量；因此我们可以有一个具有这种结构的文档：
- en: A field with the resource
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有资源的字段
- en: A field with the event date
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有事件日期的字段
- en: A field with the minute that the event happened, and the total hits
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件发生的分钟和总点击数
- en: 'The following document implements all the requirements described in the preceding
    list:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文档实现了前面列表中描述的所有要求：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: With this document design, we can retrieve the number of events happening in
    a certain resource every minute. We can also know, by the daily field, the total
    requests during the day and use this to calculate whatever we want, such as requests
    per minute, or requests per hour, for instance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个文档设计，我们可以检索在某个资源上每分钟发生的事件数量。我们还可以通过每日字段知道一天内的总请求数，并使用这个来计算我们想要的任何内容，比如每分钟的请求或每小时的请求。
- en: To demonstrate the write and read operations we could make on this collection,
    we will make use of JavaScript code running on the Node.js platform. So, before
    continuing, we must make sure we have Node.js installed on our machine.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们可以在这个集合上进行的写入和读取操作，我们将利用在Node.js平台上运行的JavaScript代码。因此，在继续之前，我们必须确保我们的机器上已经安装了Node.js。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you need help, you will find more information at [http://nodejs.org](http://nodejs.org).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要帮助，您可以在[http://nodejs.org](http://nodejs.org)找到更多信息。
- en: 'The firs thing we should do is to create a directory where our application
    will live. In a terminal, execute the following command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的第一件事是创建一个我们的应用程序将驻留的目录。在终端中，执行以下命令：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next we navigate to the directory we created and initiate the project:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们转到我们创建的目录并启动项目：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Answer all the questions asked by the wizard to create the initial structure
    of our new project. At the moment, we will have a `package.json` file based on
    the answers we gave.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 回答向导提出的所有问题，以创建我们新项目的初始结构。目前，我们将根据我们给出的答案拥有一个基于`package.json`文件。
- en: 'The next step is to set up the MongoDB driver for our project. We can do this
    by editing the `package.json` file, including the driver reference for its dependencies,
    or by executing the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为我们的项目设置MongoDB驱动程序。我们可以通过编辑`package.json`文件来实现这一点，包括其依赖项的驱动程序引用，或者通过执行以下命令来实现：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding command will install the MongoDB driver for our project and save
    the reference in the `package.json` file. Our file should look like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将为我们的项目安装MongoDB驱动程序并将引用保存在`package.json`文件中。我们的文件应该是这样的：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The last step is to create the `app.js` file with our sample code. The following
    is sample code that shows us how to count an event on our web server and record
    it in our collection:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建带有我们示例代码的`app.js`文件。以下是一个示例代码，向我们展示了如何在我们的网络服务器上计算事件的数量并将其记录在我们的集合中：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding sample code is quite simple. In it, we have the `logDailyHit`
    function, which is responsible for logging an event and incrementing one unit
    in the document `daily` field. The second function is the `logMinuteHit` function,
    which is responsible for logging the occurrence of an event and incrementing the
    document `minute` field that represents the current minute in the day. Both functions
    have an update query that has an `upsert` option with the value `true` if the
    document does not exist, in which case it will be created.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例代码非常简单。其中，我们有`logDailyHit`函数，负责记录事件并在文档`daily`字段中递增一个单位。第二个函数是`logMinuteHit`函数，负责记录事件的发生并递增代表当天当前分钟的文档`minute`字段。这两个函数都有一个更新查询，如果文档不存在，则具有值为`true`的`upsert`选项，那么将创建文档。
- en: 'When we execute the following command, we will record an event on the resource
    `"/"`. To run the code, just navigate to the project directory and execute the
    following command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行以下命令时，将在资源`"/"`上记录一个事件。要运行代码，只需转到项目目录并执行以下命令：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If everything is fine, we should see the following output after running the
    command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，运行命令后我们应该看到以下输出：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To get a feel for this, we will execute a `findOne` command on the mongod shell
    and watch the result:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了感受一下，我们将在mongod shell上执行`findOne`命令并观察结果：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In addition to everything that the previous models can give us, this one has
    some advantages over them. The first thing we notice is that, every time we register
    a new event happening on the web server, we will manipulate only one document.
    The next advantage also lies in how easy we can find the information we are looking
    for, given a specific resource, since we have the information for an entire day
    in one document, which will lead to us to manipulating fewer documents in each
    query.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前的模型可以给我们的一切，这个模型还有一些优点。我们注意到的第一件事是，每当我们在Web服务器上注册一个新事件发生时，我们只会操作一个文档。另一个优点也在于，我们可以很容易地找到我们正在寻找的信息，给定一个特定的资源，因为我们在一个文档中有一整天的信息，这将导致我们在每次查询中操作更少的文档。
- en: The way this schema design deals with time will give us many benefits when we
    think about reports. Both textual and graphical representations can be easily
    extracted from this collection for historical or real-time analysis.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式设计处理时间的方式将在我们考虑报告时给我们带来很多好处。无论是历史还是实时分析，都可以很容易地从这个集合中提取出文本和图形表示。
- en: However, as well as the previous approaches, we will have to deal with a few
    limitations too. As we have seen, we increment both the `daily` field and a `minute`
    field in an event document as they occur on the web server. When no event in a
    resource was reported for that day, then a new document will be created since
    we are using the `upsert` option on the update query. The same thing will happen
    if an event occurs in a resource for the first time in a given minute—the `$inc`
    operator will create the new `minute` field and set `"1"` as the value. This means
    that our document will grow over time, and will exceed the size MongoDB allocated
    initially for it. MongoDB automatically performs a reallocation operation every
    time the space allocated to the document is full. This reallocation operation
    that happens through the entire day has a direct impact on the database's performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与之前的方法一样，我们也将不得不处理一些限制。正如我们所见，我们在事件文档中同时增加了`daily`字段和`minute`字段，因为它们在Web服务器上发生。当某一天没有资源上报事件时，将创建一个新文档，因为我们在更新查询中使用了`upsert`选项。如果在给定的分钟内首次发生资源事件，同样的事情也会发生——`$inc`运算符将创建新的`minute`字段，并将`"1"`设置为值。这意味着我们的文档会随着时间的推移而增长，并且将超过MongoDB最初为其分配的大小。MongoDB在文档分配的空间满时会自动执行重新分配操作。整天发生的这种重新分配操作直接影响了数据库的性能。
- en: What we should do? Live with it? No. We could reduce the impact of a reallocation
    operation by adding a process that preallocates space for our document. In summary,
    we will give the application the responsibility for creating a document with all
    the minutes we can have in a day, and initializing every field with the value
    `0`. By doing this, we will avoid too many reallocation operations by MongoDB
    during the day.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该怎么办？就这样接受吗？不。我们可以通过添加一个预分配空间的过程来减少重新分配操作的影响。总之，我们将让应用程序负责创建一个包含一天内所有分钟的文档，并将每个字段初始化为值`0`。通过这样做，我们将避免MongoDB在一天内进行过多的重新分配操作。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about record allocation strategies, visit the MongoDB reference
    user manual at [http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies](http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于记录分配策略的信息，请访问MongoDB参考用户手册[http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies](http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies)。
- en: 'To give an example of how we can preallocate the document space, we can create
    a new function in our `app.js` file:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们可以在`app.js`文件中创建一个新的函数来预分配文档空间：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Tip
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Downloading the example code**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您在[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载示例代码文件，用于您购买的所有Packt
    Publishing图书。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接发送到您的电子邮件。
- en: 'To preallocate space in the current date to the `"/"` resource, just run the
    following command:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要在当前日期为`"/"`资源预分配空间，只需运行以下命令：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output of the execution is something like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的输出类似于这样：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can run a `findOne` command on the mongod shell to check the new document.
    The document created is very long, so we will show just a piece of it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在mongod shell上运行`findOne`命令来检查新文档。由于创建的文档非常长，所以我们只展示其中的一部分：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: It is recommended that we preallocate the document before midnight to ensure
    smooth functioning of the application. If we schedule this creation with an appropriate
    safety margin, we are not running any risk of creating the document for an event
    occurrence after midnight.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 建议我们在午夜前预先分配文档，以确保应用程序的顺利运行。如果我们安排创建这个文档并留有适当的安全边界，我们就不会有在午夜后发生事件的风险。
- en: 'Well, with the reallocation problem solved, we can go back to the issue that
    initiated our document redesign: the growth of our data.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在问题解决了，我们可以回到引发我们文档重新设计的问题：数据的增长。
- en: 'Even reducing the number of documents in our collection to one document per
    event per day, we can still run into a problem with storage space. This can happen
    when we have too many resources receiving events on our web server, and we cannot
    predict how many new resources we will have in our application''s lifecycle. In
    order to solve this issue, we will use two different techniques: TTL indexes and
    sharding.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使将我们收集的文档数量减少到每天每个事件一个文档，我们仍然可能会遇到存储空间的问题。当我们的网络服务器上有太多资源接收事件时，我们无法预测应用程序生命周期中会有多少新资源。为了解决这个问题，我们将使用两种不同的技术：TTL索引和分片。
- en: TTL indexes
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TTL索引
- en: It is not always the case that we need to have all log information stored on
    our servers forever. It has become a standard practice by operations people to
    limit the number of files stored on disk.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 并不总是需要将所有日志信息永久存储在服务器上。限制存储在磁盘上的文件数量已经成为运维人员的标准做法。
- en: By the same reasoning, we can limit the number of documents we need living in
    our collection. To make this happen, we could create a TTL index on the `date`
    field, specifying how long one document will exist in the collection. Just remember
    that, once we create a TTL index, MongoDB will automatically remove the expired
    documents from the collection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的道理，我们可以限制我们需要存储在集合中的文档数量。为了实现这一点，我们可以在`date`字段上创建一个TTL索引，指定一个文档在集合中存在多长时间。只要记住，一旦创建了TTL索引，MongoDB将自动从集合中删除过期的文档。
- en: Suppose that the event hit information is useful just during one year. We will
    create an index on the `date` field with the property `expireAfterSeconds` with
    `31556926` as the value, which corresponds to one year in seconds.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设事件命中信息只在一年内有用。我们将在`date`字段上创建一个索引，属性为`expireAfterSeconds`，值为`31556926`，对应一年的秒数。
- en: 'The following command, executed on the mongod shell, creates the index on our
    events collection:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在mongod shell上执行以下命令，为我们的事件集合创建索引：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If the index does not exist, the output should look like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引不存在，输出应该是这样的：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Once this is done, our documents will live in our collection for one year, based
    on the date field, and after this MongoDB will remove them automatically.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，我们的文档将根据日期字段在我们的集合中存活一年，之后MongoDB将自动删除它们。
- en: Sharding
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片
- en: If you are one of those people who have infinite resources and would like to
    have a lot of information stored on disk, then one solution to mitigate the storage
    space problem is to distribute the data by sharding your collection.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是那些拥有无限资源并希望在磁盘上存储大量信息的人之一，那么缓解存储空间问题的一个解决方案是通过分片来分布数据。
- en: And, as we stated before, we should increase our efforts when we choose the
    shard key, since it is through the shard key that we will guarantee that our read
    and write operations will be equally distributed by the shards, that is, one query
    will target a single shard or a few shards on the cluster.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，当我们选择分片键时，我们应该增加我们的努力，因为通过分片键，我们将确保我们的读写操作将由分片平均分布，也就是说，一个查询将针对集群上的一个分片或几个分片。
- en: Once we have full control over how many resources (or pages) we have on our
    web server and how this number will grow or decrease, the resource name becomes
    a good choice for a shard key. However, if we have a resource that has more requests
    (or events) than others, then we will have a shard that will be overloaded. To
    avoid this, we will include the date field to compose the shard key, which will
    also give us better performance on query executions that include this field in
    the criteria.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完全控制了我们在网络服务器上拥有多少资源（或页面）以及这个数字将如何增长或减少，资源名称就成为一个很好的分片键选择。然而，如果我们有一个资源比其他资源有更多的请求（或事件），那么我们将有一个负载过重的分片。为了避免这种情况，我们将包括日期字段来组成分片键，这也将使我们在包含该字段的查询执行中获得更好的性能。
- en: 'Remember: our goal is not to explain the setup of a sharded cluster. We will
    present to you the command that shards our collection, taking into account that
    you previously created your sharded cluster.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：我们的目标不是解释分片集群的设置。我们将向您介绍分片我们的集合的命令，考虑到您之前已经创建了分片集群。
- en: 'To shard the events collection with the shard key we choose, we will execute
    the following command on the mongos shell:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用我们选择的分片键对事件集合进行分片，我们将在mongos shell上执行以下命令：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The expected output is:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出是：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'If our events collection has any document in it, we will need to create an
    index where the shard key is a prefix before sharding the collection. To create
    the index, execute the following command:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的事件集合中有任何文档，我们需要在分片集合之前创建一个索引，其中分片键是分片的前缀。要创建索引，请执行以下命令：
- en: '[PRE37]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With the collection with the shard enabled, we will have more capacity to store
    data in the events collection, and a potential gain in performance as the data
    grows.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 启用了分片的集合将使我们能够在事件集合中存储更多数据，并在数据增长时提高性能。
- en: Now that we've designed our document and prepared our collection to receive
    a huge amount of data, let's perform some queries!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经设计好了我们的文档并准备好我们的集合来接收大量数据，让我们执行一些查询！
- en: Querying for reports
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询报告
- en: Until now, we have focused our efforts on storing the data in our database.
    This does not mean that we are not concerned about read operations. Everything
    we did was made possible by outlining the profile of our application, and trying
    to cover all the requirements to prepare our database for whatever comes our way.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直把精力集中在将数据存储在我们的数据库中。这并不意味着我们不关心读取操作。我们所做的一切都是通过概述我们应用程序的概况，并试图满足所有要求，为我们的数据库做好准备，迎接任何可能出现的情况。
- en: So, we will now illustrate some of the possibilities that we have to query our
    collection, in order to build reports based on the stored data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在将说明我们有多少种可能性来查询我们的集合，以便基于存储的数据构建报告。
- en: If what we need is real-time information about the total hits on a resource,
    we can use our daily field to query the data. With this field, we can determine
    the total hits on a resource at a particular time of day, or even the average
    requests per minute on the resource based on the minute of the day.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要的是关于资源总点击量的实时信息，我们可以使用我们的每日字段来查询数据。有了这个字段，我们可以确定在一天中特定时间的资源总点击量，甚至可以根据一天中的分钟数确定资源的平均每分钟请求次数。
- en: 'To query the total hits based on the current time of the day, we will create
    a new function called `getCurrentDayhits` and, to query the average request per
    minute in a day, we will create the `getCurrentMinuteStats` function in the `app.js`
    file:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询基于当天时间的总点击量，我们将创建一个名为`getCurrentDayhits`的新函数，并且为了查询一天内每分钟的平均请求次数，我们将在`app.js`文件中创建`getCurrentMinuteStats`函数：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To see the magic happening, we should run the following command in the terminal:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到魔术发生，我们应该在终端中运行以下命令：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If everything is fine, the output should look like this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，输出应该是这样的：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Another possibility is to retrieve daily information to calculate the average
    requests per minute of a resource, or to get the set of data between two dates
    to build a graph or a table.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能性是每天检索信息，计算资源每分钟的平均请求次数，或者在两个日期之间获取数据集以构建图表或表格。
- en: 'The following code has two new functions, `getAverageRequestPerMinuteStats`,
    which calculates the average number of requests per minute of a resource, and
    `getBetweenDatesDailyStats`, which shows how to retrieve the set of data between
    two dates. Let''s see what the `app.js` file looks like:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码有两个新函数，`getAverageRequestPerMinuteStats`，用于计算资源每分钟的平均请求次数，以及`getBetweenDatesDailyStats`，展示了如何在两个日期之间检索数据集。让我们看看`app.js`文件是什么样子的：
- en: '[PRE41]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, there are many ways to query the data in the `events` collection.
    These were some very simple examples of how to extract the data, but they were
    functional and reliable ones.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，有许多方法可以查询`events`集合中的数据。这些是一些非常简单的提取数据的例子，但它们是功能齐全且可靠的。
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter showed you an example of the process of designing a schema from
    scratch, in order to solve a real-life problem. We began with a detailed problem
    and its requirements and evolved the schema design to have better use of available
    resources. The sample code based on the problem is very simple, but will serve
    as a basis for your life-long learning. Great!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了一个从零开始设计模式的过程的示例，以解决一个现实生活中的问题。我们从一个详细的问题及其要求开始，演变出了更好地利用可用资源的模式设计。基于这个问题的示例代码非常简单，但将为您终身学习提供基础。太棒了！
- en: In this last chapter, we had the opportunity to make, in just a few pages, a
    journey back to the first chapters of this book and apply the concepts that were
    introduced along the way. But, as you must have realized by now, MongoDB is a
    young, full of database that is full of possibilities. Its adoption by the community
    around it—and that includes your own—gets bigger with each new release. Thus,
    if you find yourself faced with a new challenge that you realize that has more
    than one single solution, carry out any test deem necessary or useful. Colleagues
    can also help, so talk to them. And always keep in mind that a good design is
    the one that fits your needs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这最后一章中，我们有机会在短短几页内回顾本书的前几章，并应用沿途介绍的概念。但是，正如你现在可能已经意识到的那样，MongoDB是一个充满可能性的年轻数据库。它在社区中的采用率——包括你自己在内——随着每一次新发布而增加。因此，如果你发现自己面临一个新的挑战，意识到有不止一个解决方案，进行任何你认为必要或有用的测试。同事们也可以帮助，所以和他们交流。并且永远记住，一个好的设计是符合你需求的设计。
