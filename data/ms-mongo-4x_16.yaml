- en: Replication
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: Replication has been one of the most useful features of MongoDB since the very
    early days. In general, replication refers to the process of synchronizing data
    across different servers. The benefits of replication include protection from
    data loss and high availability of data. Replication also provides disaster recovery,
    an avoidance of downtime for maintenance, scaling reads (since we can read from
    multiple servers), and scaling writes (only if we can write to multiple servers).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自从MongoDB的早期以来，复制一直是最有用的功能之一。 一般来说，复制是指在不同服务器之间同步数据的过程。 复制的好处包括防止数据丢失和数据的高可用性。
    复制还提供灾难恢复，避免维护停机时间，扩展读取（因为我们可以从多个服务器读取）和扩展写入（只有我们可以写入多个服务器时）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: An architectural overview, elections, and the use cases for replication
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构概述，选举和复制的用例
- en: Setting up a replica set
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置副本集
- en: Connecting to a replica set
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到副本集
- en: Replica set administration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本集管理
- en: The best practices for deploying replica sets using cloud providers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云提供商部署副本集的最佳实践
- en: Replica set limitations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本集限制
- en: Replication
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: There are different approaches to replication. The approach that MongoDB takes
    is logical replication with a master-slave, which we will explain in more detail
    later in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 复制有不同的方法。 MongoDB采取的方法是主从的逻辑复制，我们将在本章后面更详细地解释。
- en: Logical or physical replication
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑或物理复制
- en: With replication, we synchronize data across multiple servers, providing data
    availability and redundancy. Even if we lose a server due to a hardware or software
    failure, by using replication, we will have multiple copies that we can use to
    restore our data. Another advantage of replication is that we can use one of the
    servers as a dedicated reporting, or backup, server.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过复制，我们在多个服务器之间同步数据，提供数据可用性和冗余。 即使由于硬件或软件故障而丢失服务器，通过使用复制，我们将有多个副本可以用来恢复我们的数据。
    复制的另一个优点是我们可以使用其中一个服务器作为专用报告或备份服务器。
- en: In logical replication, we have our master/primary server performing operations; the
    slave/secondary server tails a queue of operations from the master and applies
    the same operations in the same order. Using of MongoDB as an example, the **operations
    log** (**oplog**) keeps track of operations that have happened on the primary
    server and applies them in the exact same order on the secondary server.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑复制中，我们的主/主服务器执行操作； 从/次要服务器从主服务器尾随操作队列，并按相同顺序应用相同的操作。 以MongoDB为例，**操作日志**（**oplog**）跟踪主服务器上发生的操作，并按相同顺序在次要服务器上应用它们。
- en: Logical replication is useful for a wide array of applications, such as information
    sharing, data analysis, and **Online Analytical Processing** (**OLAP**) reporting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑复制对各种应用非常有用，例如信息共享，数据分析和**在线分析处理**（**OLAP**）报告。
- en: In physical replication, data gets copied on the physical level, at a lower
    level than database operations. This means that we are not applying the operations,
    but copying the bytes that were affected by these operations. It also means that
    we can gain better efficiency, since we are using low-level structures to transfer
    data. We can also ensure that the state of the database is exactly the same, since
    they are identical, byte for byte.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理复制中，数据在物理级别上被复制，比数据库操作的更低级别。 这意味着我们不是应用操作，而是复制受这些操作影响的字节。 这也意味着我们可以获得更好的效率，因为我们使用低级结构来传输数据。
    我们还可以确保数据库的状态完全相同，因为它们是相同的，逐字节相同。
- en: What is typically missing from physical replication is knowledge about the database
    structure, which means that it is harder (if not impossible) to copy some collections
    from a database and ignore others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 物理复制通常缺少有关数据库结构的知识，这意味着更难（如果不是不可能）从数据库复制一些集合并忽略其他集合。
- en: Physical replication is typically suited for more rare circumstances, like disaster
    recovery, wherein a full and exact copy of everything (including data, indexes,
    the internal state of the database in a journal, and redoing/undoing logs) is
    of crucial importance to bringing the application back to the exact state it was
    in.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 物理复制通常适用于更罕见的情况，例如灾难恢复，在这种情况下，一切（包括数据，索引，数据库内部状态在日志中的重做/撤消日志）的完整和精确副本对于将应用程序恢复到确切状态至关重要。
- en: Different high availability types
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的高可用性类型
- en: 'In high availability, there are several configurations that we can use. Our
    primary server is called the **hot server**, as it can process each and every
    request coming in. Our secondary server can be in any of the following states:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在高可用性中，有几种配置可以使用。 我们的主服务器称为**热服务器**，因为它可以处理每一个请求。 我们的次要服务器可以处于以下任何状态：
- en: Cold
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冷
- en: Warm
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温暖
- en: Hot
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热
- en: A **secondary cold server** is a server that is there just in case the primary
    server goes offline, without any expectation of it holding the data and state
    that the primary server had.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**次要冷服务器**是一个服务器，仅在主服务器离线时存在，而不期望它保存主服务器的数据和状态。'
- en: A **secondary warm server** receives periodic updates of data from the primary
    server, but typically, it is not entirely up to date with the primary server.
    It can be used for some non-real-time analytics reporting to offload the main
    server, but typically, it will not be able to pick up the transactional load of
    the primary server if it goes down.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**次要温暖服务器**定期从主服务器接收数据更新，但通常不会完全与主服务器同步。 它可以用于一些非实时分析报告，以卸载主服务器，但通常情况下，如果主服务器宕机，它将无法承担事务负载。'
- en: A **secondary hot server** always keeps an up-to-date copy of the data and state
    from the primary server. It usually waits in a hot standby state, ready to take
    over when the primary server goes down.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**次要热服务器**始终保持与主服务器的数据和状态的最新副本。 它通常处于热备状态，准备在主服务器宕机时接管。'
- en: MongoDB has both the hot and warm server types of functionality, as we will
    explore in the following sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB具有热服务器和温服务器功能，我们将在接下来的部分中探讨。
- en: Most database systems employ a similar notion of primary/secondary servers,
    so conceptually, everything from MongoDB gets applied there, too.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据库系统都采用类似的主/次服务器概念，因此从概念上讲，MongoDB的所有内容也适用于那里。
- en: An architectural overview
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构概述
- en: 'MongoDB''s replication is provided in the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB的复制在以下图表中提供：
- en: '![](img/4a5ebcaa-a723-4afe-9e23-054f1d663936.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a5ebcaa-a723-4afe-9e23-054f1d663936.png)'
- en: The primary server is the only one that can take writes at any time. The secondary
    servers are in a hot standby state, ready to take over if the primary server fails.
    Once the primary server fails, an election takes place regarding which secondary
    server will become primary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主服务器是唯一可以随时进行写入的服务器。次要服务器处于热备状态，一旦主服务器故障，它们就可以接管。一旦主服务器故障，就会进行选举，确定哪个次要服务器将成为主服务器。
- en: We can also have **arbiter nodes**. Arbiter nodes do not hold any data, and
    their sole purpose is to participate in the election process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以有**仲裁节点**。仲裁节点不保存任何数据，它们唯一的目的是参与选举过程。
- en: We must always have an odd number of nodes (including arbiters). Three, five,
    and seven are all fine, so that in the event of the primary (or more servers)
    failing, we have a majority of votes in the election process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须始终有奇数个节点（包括仲裁者）。三、五和七都可以，这样在主服务器（或更多服务器）故障时，我们在选举过程中有多数选票。
- en: When the other members of a replica set don't hear from the primary for more
    than 10 seconds (configurable), an eligible secondary will start the election
    process to vote for a new primary. The first secondary to hold the election and
    win the majority will become the new primary. All remaining servers will now replicate
    from the new primary server, keeping their roles as secondaries but syncing up
    from the new primary.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当副本集的其他成员在10秒以上（可配置）没有收到来自主服务器的消息时，一个合格的次要成员将开始选举过程，投票选举出新的主服务器。首个进行选举并赢得多数的次要成员将成为新的主服务器。所有剩余的服务器现在将从新的主服务器复制，保持它们作为次要服务器的角色，但从新的主服务器同步。
- en: Starting with MongoDB 3.6, client drivers can retry write operations a **single
    time **if they detect that the primary is down. A replica set can have up to 50
    members, but only up to seven of them can vote in the election process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.6开始，客户端驱动程序可以在检测到主服务器宕机时**重试一次**写操作。副本集最多可以有50个成员，但其中只有最多七个可以参与选举过程。
- en: 'The setup for our replica set after the new election will be as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 新选举后我们副本集的设置如下：
- en: '![](img/66b01e77-7539-4d79-a218-813ea11b5d05.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66b01e77-7539-4d79-a218-813ea11b5d05.png)'
- en: In the next section, we will discuss how elections work.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论选举的工作原理。
- en: How do elections work?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选举是如何工作的？
- en: All of the servers in a replica set maintain regular communication with every
    other member via a heartbeat. The heartbeat is a small packet that's regularly
    sent to verify that all members are operating normally.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集中的所有服务器都通过心跳定期与每个其他成员保持通信。心跳是一个小数据包，定期发送以验证所有成员是否正常运行。
- en: Secondary members also communicate with the primary to get the latest updates
    from the oplog and apply them to their own data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 次要成员还与主服务器通信，从oplog获取最新更新并将其应用于自己的数据。
- en: The information here refers to the latest replication election protocol, version
    1, which was introduced in MongoDB v3.2.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的信息是指最新的复制选举协议，即版本1，它是在MongoDB v3.2中引入的。
- en: Schematically, we can see how this works.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中，我们可以看到它是如何工作的。
- en: When the primary member goes down, all of the secondaries will miss a heartbeat
    or more. They will be waiting up until the `settings.electionTimeoutMillis` time
    passes (the default is 10 seconds), and then the secondaries will start one or
    more rounds of elections to find the new primary.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当主成员下线时，所有次要成员都会错过一个或多个心跳。它们将等待直到`settings.electionTimeoutMillis`时间过去（默认为10秒），然后次要成员将开始一轮或多轮选举，以找到新的主服务器。
- en: 'For a server to be elected as primary from the secondaries, it must have two
    properties:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要从次要服务器中选举出主服务器，它必须具备两个属性：
- en: Belong in a group of voters that have *50% + 1* of the votes
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于拥有*50% + 1*选票的选民组
- en: Be the most up-to-date secondary in this group
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为这个组中最新的次要
- en: In a simple example of three servers with one vote each, once we lose the primary,
    the other two servers will each have one vote (so, in total, two-thirds), and
    as such, the one with the most up-to-date oplog will be elected as primary.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的例子中，有三个服务器，每个服务器一票，一旦我们失去主服务器，其他两个服务器将各自有一票（因此总共是三分之二），因此，拥有最新oplog的服务器将被选举为主服务器。
- en: 'Now, consider a more complex setup, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个更复杂的设置，如下：
- en: Seven servers (one primary, six secondaries)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 七个服务器（一个主服务器，六个次要服务器）
- en: One vote each
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点一票
- en: 'We lose the primary server, and the six remaining servers have network connectivity
    issues, resulting in a network partition:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们失去了主服务器，剩下的六个服务器出现了网络连接问题，导致网络分区：
- en: '![](img/d794932d-b88f-4c1f-8bd7-be491ae4be45.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d794932d-b88f-4c1f-8bd7-be491ae4be45.png)'
- en: 'These partitions can be described as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分区可以描述如下：
- en: 'Partition North: Three servers (one vote each)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 北区：三个服务器（每个一票）
- en: 'Partition South: Three servers (one vote each)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 南区：三个服务器（每个一票）
- en: Neither partition has any knowledge of what happened to the rest of the servers.
    Now, when they hold elections, no partition can establish a majority, as they
    have three out of seven votes. No primary will get elected from either partition.
    This problem can be overcome by having, for example, one server with three votes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一个分区都不知道其他服务器发生了什么。现在，当它们进行选举时，没有一个分区能够建立多数，因为它们有七票中的三票。没有主服务器会从任何一个分区中被选举出来。这个问题可以通过例如拥有一个拥有三票的服务器来解决。
- en: 'Now, our overall cluster setup looks as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的整体集群设置如下：
- en: '**Server #1**: one vote'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃1**：一票'
- en: '**Server #2**: one vote'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃2**：一票'
- en: '**Server #3**: one vote'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃3**：一票'
- en: '**Server #4**: one vote'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃4**：一票'
- en: '**Server #5**: one vote'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃5**：一票'
- en: '**Server #6**: one vote'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃6**：一票'
- en: '**Server #7**: three votes'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃7**：三票'
- en: 'After losing Server #1, our partitions now look as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在失去服务器＃1后，我们的分区现在如下：
- en: '![](img/7841ea05-e5c9-411c-9a9d-b66cbaf99b5f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7841ea05-e5c9-411c-9a9d-b66cbaf99b5f.png)'
- en: 'Partition North is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 北分区如下：
- en: '**Server #2**: One vote'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃2**：一票'
- en: '**Server #3**: One vote'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃3**：一票'
- en: '**Server #4**: One vote'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃4**：一票'
- en: 'Partition South is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 南分区如下：
- en: '**Server #5**: One vote'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃5**：一票'
- en: '**Server #6**: One vote'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃6**：一票'
- en: '**Server #7**: Three votes'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器＃7**：三票'
- en: 'Partition South has three servers, with a total of five out of nine votes.
    The secondary among servers #5, #6, and #7 that is most up to date (according
    to its oplog entries) will be elected as the primary.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 南分区有三个服务器，共有九票中的五票。服务器＃5、＃6和＃7中最新（根据其oplog条目）的辅助服务器将被选为主服务器。
- en: What is the use case for a replica set?
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集的用例是什么？
- en: 'MongoDB offers most of the advantages of using a replica set, some of which
    are listed as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB提供了使用副本集的大部分优势，其中一些列举如下：
- en: Protection from data loss
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止数据丢失
- en: High availability of data
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的高可用性
- en: Disaster recovery
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灾难恢复
- en: Avoidance of downtime for maintenance
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免维护停机时间
- en: Scaling reads, since we can read from multiple servers
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展读取，因为我们可以从多个服务器读取
- en: Helping to design for geographically dispersed services
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助设计地理分散服务
- en: Data privacy
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据隐私
- en: The most notable item that's missing from the list is scaling writes. This is
    because, in MongoDB, we can only have one primary, and only this primary can take
    writes from our application server.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表中缺少的最显着的项目是扩展写入。这是因为在MongoDB中，我们只能有一个主服务器，只有这个主服务器才能从我们的应用服务器接收写入。
- en: When we want to scale write performance, we typically design and implement sharding,
    which will be the topic of the next chapter. Two interesting properties of the
    way that MongoDB replication is implemented are geographically dispersed services
    and data privacy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要扩展写性能时，通常会设计和实现分片，这将是下一章的主题。MongoDB复制实现的两个有趣特性是地理分散服务和数据隐私。
- en: It is not uncommon for our application servers to be located in multiple data
    centers across the globe. Using replication, we can have a secondary server as
    close to the application server as possible. What this means is that our reads
    will be fast, as if they were local, and we will get a latency performance penalty
    just for our writes. This requires some planning at the application level, of
    course, so that we can maintain two different pools of connections to our database,
    which can be easily done by either using the official MongoDB drivers or using
    higher-level ODMs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用服务器通常位于全球多个数据中心。使用复制，我们可以尽可能将辅助服务器靠近应用服务器。这意味着我们的读取将很快，就像本地一样，并且我们只会为写入获得延迟性能惩罚。当然，这需要在应用程序级别进行一些规划，以便我们可以维护两个不同的数据库连接池，这可以通过使用官方的MongoDB驱动程序或使用更高级别的ODM轻松完成。
- en: The second interesting property of MongoDB's replication design is implementing
    data privacy. When we have servers geographically dispersed across different data
    centers, we can enable replication per database. By keeping a database out of
    the replication process, we can make sure that our data stays confined in the
    data center that we need. We can also set up different replication schemas per
    database in the same MongoDB server so that we have multiple replication strategies
    according to our data privacy needs, excluding some servers from our replica sets
    if they are not allowed by our data privacy regulations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB复制设计的第二个有趣特性是实现数据隐私。当我们在不同数据中心地理分散的服务器上，我们可以启用每个数据库的复制。通过将数据库排除在复制过程之外，我们可以确保我们的数据保持在我们需要的数据中心内。我们还可以在同一个MongoDB服务器上为每个数据库设置不同的复制模式，以满足我们的数据隐私需求，如果某些服务器不符合我们的数据隐私规定，可以将其排除在副本集之外。
- en: Setting up a replica set
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置副本集
- en: In this section, we will go over the most common deployment procedures to set
    up a replica set. These involve either converting a standalone server into a replica
    set or setting up a replica set from scratch.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍设置副本集的最常见部署程序。这些包括将独立服务器转换为副本集或从头开始设置副本集。
- en: Converting a standalone server into a replica set
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将独立服务器转换为副本集
- en: 'To convert a standalone server into a replica set, we first need to cleanly
    shut down the `mongo` server:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要将独立服务器转换为副本集，我们首先需要干净地关闭`mongo`服务器：
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we start the server with the `--replSet` configuration option via the
    command line (which we will do here), or by using a configuration file, as we
    will explain in the next section:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过命令行使用`--replSet`配置选项启动服务器（我们将在这里执行），或者使用配置文件，如我们将在下一节中解释的那样：
- en: 'First, we connect (via the mongo shell) to the new replica set enabled instance,
    as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过mongo shell连接到新的启用了副本集的实例，如下所示：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we have the first server of our replica set. We can add the other servers
    (which must have also been started with `--replSet`) by using the mongo shell,
    as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有了副本集的第一个服务器。我们可以使用mongo shell添加其他服务器（这些服务器也必须使用`--replSet`启动），如下所示：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Double-check the replica set configuration by using `rs.conf()`. Verify the
    replica set status by using `rs.status()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`rs.conf()`来双重检查副本集配置。通过使用`rs.status()`来验证副本集状态。
- en: Creating a replica set
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建副本集
- en: 'Starting a MongoDB server as a part of a replica set is as easy as setting
    it in the configuration via the command line:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作为副本集的一部分启动MongoDB服务器就像通过命令行在配置中设置它一样容易：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is fine for development purposes. For production environments, it''s recommended
    that we use a configuration file instead:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这对开发目的来说是可以的。对于生产环境，建议使用配置文件：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, the `<path-to-config>` can be as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<path-to-config>`可以如下：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This configuration file has to be in a YAML format.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置文件必须采用YAML格式。
- en: YAML does not support tabs. Convert tabs to spaces by using your editor of choice.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: YAML不支持制表符。请使用您选择的编辑器将制表符转换为空格。
- en: 'A simple configuration file sample is as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的配置文件示例如下：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Root-level options define the sections that leaf-level options apply to by nesting.
    Regarding replication, the mandatory options are `oplogSizeMB` (the oplog size
    for the member, in MB) and `replSetName` (the replica set name, such as `xmr_cluster`).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 根级选项通过嵌套定义叶级选项适用于的部分。关于复制，强制选项是`oplogSizeMB`（成员的oplog大小，以MB为单位）和`replSetName`（副本集名称，例如`xmr_cluster`）。
- en: 'We can also set the following on the same level as `replSetName`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在与`replSetName`相同级别上设置以下内容：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is only available for the MMAPv1 storage engine, and it refers to the indexes
    on secondaries that will get loaded into memory before applying operations from
    the oplog.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于MMAPv1存储引擎，并且指的是在应用操作之前将加载到内存中的次要服务器上的索引。
- en: 'It defaults to `all`, and the available options are `none` and `_id_only`,
    in order to load no indexes into memory and only load the default index that was created
    on `_id` fields:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它默认为`all`，可用选项为`none`和`_id_only`，以便不将索引加载到内存中，只加载在`_id`字段上创建的默认索引：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is the configuration setting for enabling the read preference of `majority`
    for this member.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是启用此成员的`majority`读取偏好的配置设置。
- en: After we have started all of the replica set processes on different nodes, we
    log in to one of the nodes using `mongo` from the command line with the appropriate
    `host:port`. Then, we need to initiate the cluster from one member.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同节点上启动了所有副本集进程后，我们可以使用适当的`host:port`从命令行使用`mongo`登录到其中一个节点。然后，我们需要从一个成员初始化集群。
- en: 'We can use configuration files, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下配置文件：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Or, we can pass in the configurations as a document parameter, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将配置作为文档参数传递，如下所示：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can verify that the cluster was initiated by using `rs.conf()` in the shell.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`rs.conf()`在shell中验证集群是否已初始化。
- en: 'Following that, we add each other member to our replica set by using the `host:port` that
    we defined in our networking setup:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用我们在网络设置中定义的`host:port`，将每个其他成员添加到我们的副本集中：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The minimum number of servers that we must use for an HA replica set is `3`.
    We could replace one of the servers with an arbiter, but this is not recommended.
    Once we have added all of the servers and have waited a bit, we can check the
    status of our cluster by using `rs.status()`. By default, the oplog will be 5%
    of the free disk space. If we want to define it when we create our replica set,
    we can do so by passing the command-line parameter `--oplogSizeMB` or `replication.oplogSizeMB`
    in our configuration file. An oplog size cannot be more than 50 GB.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为HA副本集使用的最小服务器数量是`3`。我们可以用仲裁者替换其中一个服务器，但这并不推荐。一旦我们添加了所有服务器并等待了一会儿，我们可以使用`rs.status()`来检查我们集群的状态。默认情况下，oplog将是空闲磁盘空间的5%。如果我们想在创建副本集时定义它，我们可以通过传递命令行参数`--oplogSizeMB`或在配置文件中使用`replication.oplogSizeMB`来这样做。oplog大小不能超过50GB。
- en: Read preference
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取偏好
- en: By default, all writes and reads go/come from the primary server. Secondary
    servers replicate data, but are not used for querying.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有写入和读取都来自主服务器。次要服务器复制数据，但不用于查询。
- en: In some cases, it may be beneficial to change this and start to take reads from
    secondaries.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，更改这一点并开始从次要服务器读取可能是有益的。
- en: 'The MongoDB official drivers support five levels of read preference:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB官方驱动程序支持五个级别的读取偏好：
- en: '| **Read Preference Mode** | **Description** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **读取偏好模式** | **描述** |'
- en: '| `primary` | This is the default mode, where reads come from the `primary` server
    of the replica set. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `primary` | 这是默认模式，其中读取来自副本集的`primary`服务器。 |'
- en: '| `primaryPreferred` | With this mode, applications will read from the `primary`,
    unless it is unavailable, in which case reads will come from `secondary` members.
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| `primaryPreferred` | 使用此模式，应用程序将从`primary`读取数据，除非它不可用，在这种情况下，读取将来自`secondary`成员。
    |'
- en: '| `secondary` | Reads come exclusively from `secondary` servers. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `secondary` | 读取仅来自`secondary`服务器。 |'
- en: '| `secondaryPreferred` | With this mode, applications will read from `secondary`
    members, unless they are unavailable, in which case reads will come from the `primary` member.
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| `secondaryPreferred` | 使用此模式，应用程序将从`secondary`成员读取数据，除非它们不可用，在这种情况下，读取将来自`primary`成员。
    |'
- en: '| `nearest` | Applications will read from the member of the replica set that
    is `nearest` in terms of network latency, not taking into account the member''s
    type. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| `nearest` | 应用程序将从副本集中在网络延迟方面最接近的成员读取数据，而不考虑成员的类型。 |'
- en: Using any read preference other than `primary` can be beneficial for asynchronous
    operations that are not extremely time-sensitive. For example, reporting servers
    can take reads from secondaries instead of the primary, as we may be fine with
    a small delay in our aggregation data, with the benefit of incurring more read
    load on our primary server.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`primary`之外的任何读取偏好对于非常时间敏感的异步操作可能是有益的。例如，报告服务器可以从次要服务器读取，而不是从主服务器读取，因为我们可能对聚合数据的小延迟可以接受，而又能在主服务器上产生更多的读取负载。
- en: Geographically distributed applications will also benefit from reading from
    secondaries, as these will have significantly lower latency. Although it's probably
    counter-intuitive, just changing the read preference from `primary` to `secondary`
    will not significantly increase the total read capacity of our cluster. This is
    because all of the members of our cluster are taking the same write load from
    clients' writes, and replication for the primary and secondaries, respectively.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 地理分布的应用程序也将受益于从次要服务器读取，因为这些服务器的延迟会显著较低。尽管这可能有违直觉，但仅将读取偏好从`primary`更改为`secondary`不会显著增加集群的总读取容量。这是因为我们集群的所有成员都在承受来自客户端写入的相同写入负载，并分别复制主服务器和次要服务器的数据。
- en: More importantly, however, reading from a secondary may return stale data, which
    has to be dealt with at the application level. Reading from different secondaries
    that may have variable replication lag (compared to our primary writes) may result
    in reading documents out of their insertion order (**non-monotonic reads**).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从辅助节点读取可能会返回过期数据，这必须在应用程序级别处理。从可能具有可变复制延迟的不同辅助节点读取（与我们的主要写入相比）可能导致读取文档的插入顺序不一致（**非单调读取**）。
- en: With all of the preceding caveats, it is still a good idea to test reading from
    secondaries if our application design supports it. An additional configuration
    option that can help us to avoid reading stale data is `maxStalenessSeconds`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在所有上述警告，如果我们的应用程序设计支持，从辅助节点读取仍然是一个好主意。可以帮助我们避免读取过期数据的另一个配置选项是`maxStalenessSeconds`。
- en: Based on a coarse estimation from each secondary as to how far behind the primary
    it is, we can set this to a value of 90 (seconds) or more to avoid reading stale
    data. Given that secondaries know how far behind they are from the primary (but
    don't accurately or aggressively estimate it), this should be treated as an approximation,
    rather than something we base our design o.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个辅助节点对于与主节点相比落后程度的粗略估计，我们可以将其设置为90（秒）或更高的值，以避免读取过期数据。鉴于辅助节点知道它们与主节点的落后程度（但并不准确或积极地估计），这应被视为一种近似，而不是我们设计的基础。
- en: Write concern
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写关注
- en: 'By default, the write operations in MongoDB replica sets will be acknowledged
    once the write has been acknowledged by the primary server. If we want to change
    this behavior, we can do so in two different ways:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB副本集中，默认情况下，写操作将在主服务器确认写入后得到确认。如果我们想要更改此行为，可以通过两种不同的方式进行：
- en: 'We can request a different write concern per operation, in cases where we want
    to make sure that a write has propagated to multiple members of our replica set
    before marking it as complete, as follows:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可以针对每个操作请求不同的写关注，以确保写入在标记为完成之前已传播到我们副本集的多个成员，如下所示：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding example, we are waiting for the write to be confirmed by two
    servers (the primary, plus any one of the secondaries). We are also setting a
    timeout of `5000` milliseconds to avoid our write from blocking in cases where
    the network is slow or we just don't have enough servers to acknowledge the request.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们正在等待两个服务器（主服务器加上任何一个辅助服务器）确认写入。我们还设置了`5000`毫秒的超时，以避免在网络速度慢或我们没有足够的服务器来确认请求的情况下阻塞我们的写入。
- en: 'We can also change the default write concern across the entire replica set,
    as follows:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以通过以下方式更改整个副本集的默认写关注：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we set the write concern to `majority` with a timeout of `5` seconds.
    The write concern `majority` makes sure that our writes will propagate to at least
    *n/2+1* servers, where *n* is the number of our replica set members.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将写关注设置为`majority`，超时为`5`秒。写关注`majority`确保我们的写入将传播到至少*n/2+1*个服务器，其中*n*是我们的副本集成员的数量。
- en: 'The write concern `majority` is useful if we have a read preference of `majority` as
    well, as it ensures that every write with `w: "majority"` will also be visible
    with the same read preference. If we set `w>1`, it''s useful to also set `wtimeout:
    <milliseconds>` with it. `wtimeout` will return from our write operation once
    the timeout has been reached, thus not blocking our client for an indefinite period
    of time. It''s recommended to set `j: true`, as well. `j: true` will wait for
    our write operation to be written to the journal before acknowledging it. `w>1`,
    along with `j: true`, will wait for the number of servers that we have specified
    to write to the journal before acknowledgement.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '写关注`majority`在我们的读取偏好为`majority`时非常有用，因为它确保每个带有`w: "majority"`的写入也将以相同的读取偏好可见。如果设置了`w>1`，还可以设置`wtimeout:
    <milliseconds>`。`wtimeout`将在达到超时后从我们的写操作返回，因此不会无限期地阻塞我们的客户端。建议还设置`j: true`。`j:
    true`将等待我们的写操作在确认之前写入日志。`w>1`与`j: true`一起将等待我们指定的服务器数量在确认之前写入日志。'
- en: Custom write concerns
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义写关注
- en: 'We can also identify our replica set members with different tags (that is,
    `reporting`, East Coast Servers, and HQ servers) and specify a custom write concern
    per operation, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用不同的标签（即`reporting`，东海岸服务器和总部服务器）标识我们的副本集成员，并针对每个操作指定自定义写关注，如下所示：
- en: 'Use the usual procedure for connecting to the primary via the mongo shell,
    as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用mongo shell连接到主服务器的常规过程如下：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now set a custom write concern, as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置自定义写关注，如下所示：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After applying this, we use the `reconfig` command:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用此设置后，我们使用`reconfig`命令：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now start by setting `writeConcern` in our writes, as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下方式在我们的写入中设置`writeConcern`：
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This means that our write will only be acknowledged if the `UKWrites` write
    concern is satisfied, which, in turn, will be satisfied by at least two servers,
    with the tag `location_uk` verifying it. Since we only have two servers located
    in the UK, we can make sure that with this custom write concern, we have written
    our data to all of our UK-based servers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的写入只有在满足`UKWrites`写关注时才会得到确认，而`UKWrites`写关注将由至少两个带有`location_uk`标签的服务器验证。由于我们只有两台位于英国的服务器，因此通过此自定义写关注，我们可以确保将数据写入到我们所有的英国服务器。
- en: Priority settings for replica set members
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集成员的优先级设置
- en: MongoDB allows us to set different priority levels for each member. This allows
    for some interesting applications and topologies to be implemented.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB允许我们为每个成员设置不同的优先级级别。这允许实现一些有趣的应用程序和拓扑结构。
- en: 'To change the priority after we have set up our cluster, we have to connect
    to our primary using the mongo shell and get the configuration object (in this
    case, `cfg`):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置完集群后更改优先级，我们必须使用mongo shell连接到我们的主服务器并获取配置对象（在本例中为`cfg`）：
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we can change the `members` sub-document `priority` attribute to the
    value of our choice:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将`members`子文档的`priority`属性更改为我们选择的值：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The default `priority` is `1` for every member. The priority can be set from
    `0` (never become a primary) to `1000`, in floating-point precision.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个成员的默认`priority`为`1`。`priority`可以从`0`（永远不成为主要）设置为`1000`，以浮点精度。
- en: Higher priority members will be the first to call an election when the primary
    steps down, and are also the most likely to win the election.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级较高的成员将是主服务器下台时首先发起选举的成员，并且最有可能赢得选举。
- en: Custom priorities should be configured with consideration of the different network
    partitions. Setting priorities the wrong way may lead to elections not being able
    to elect a primary, thus stopping all writes to our MongoDB replica set.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 应该考虑不同网络分区来配置自定义优先级。错误地设置优先级可能导致选举无法选举主服务器，从而停止所有对我们MongoDB副本集的写入。
- en: If we want to prevent a secondary from becoming a primary, we can set its priority
    to `0`, as we will explain in the following section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要阻止次要服务器成为主服务器，我们可以将其`priority`设置为`0`，如我们将在下一节中解释的那样。
- en: Zero priority replica set members
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零优先级副本集成员
- en: In some cases (for example, if we have multiple data centers), we will want
    some of the members to never be able to become a primary server.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下（例如，如果我们有多个数据中心），我们将希望一些成员永远无法成为主服务器。
- en: In a scenario with multiple data center replications, we may have our primary
    data center with one primary and one secondary based in the UK, and a secondary
    server located in Russia. In this case, we don't want our Russia-based server
    to become primary, as it would incur latency on our application servers based
    in the UK. In this case, we will set up our Russia-based server with `priority`
    as `0`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个数据中心复制的情况下，我们的主要数据中心可能有一个基于英国的主服务器和一个次要服务器，以及一个位于俄罗斯的次要服务器。在这种情况下，我们不希望我们基于俄罗斯的服务器成为主服务器，因为这将给我们位于英国的应用服务器带来延迟。在这种情况下，我们将设置我们基于俄罗斯的服务器的`priority`为`0`。
- en: 'Replica set members with `priority` as `0` also can''t trigger elections. In
    all other aspects, they are identical to every other member in the replica set.
    To change the `priority` of a replica set member, we must first get the current
    replica set configuration by connecting (via the mongo shell) to the primary server:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`priority`为`0`的副本集成员也不能触发选举。在所有其他方面，它们与副本集中的每个其他成员相同。要更改副本集成员的`priority`，我们必须首先通过连接（通过mongo
    shell）到主服务器获取当前的副本集配置：'
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will provide the config document that contains the configuration for every
    member in our replica set. In the `members` sub-document, we can find the `priority`
    attribute, which we have to set to `0`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供包含副本集中每个成员配置的配置文档。在`members`子文档中，我们可以找到`priority`属性，我们必须将其设置为`0`：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we need to reconfigure the replica set with the updated configuration:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要使用更新后的配置重新配置副本集：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Make sure that you have the same version of MongoDB running in every node, otherwise
    there may be unexpected behavior. Avoid reconfiguring the replica set cluster
    during high volume periods. Reconfiguring a replica set may force an election
    for a new primary, which will close all active connections, and may lead to a
    downtime of 10-30 seconds. Try to identify the lowest traffic time window to run
    maintenance operations like reconfiguration, and always have a recovery plan in
    case something goes wrong.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 确保每个节点中运行的MongoDB版本相同，否则可能会出现意外行为。避免在高流量时期重新配置副本集群。重新配置副本集可能会强制进行新主要选举，这将关闭所有活动连接，并可能导致10-30秒的停机时间。尝试识别最低流量时间窗口来运行维护操作，始终在发生故障时有恢复计划。
- en: Hidden replica set members
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏的副本集成员
- en: Hidden replica set members are used for special tasks. They are invisible to
    clients, will not show up in the `db.isMaster()` mongo shell command and similar
    administrative commands, and, for all purposes, will not be taken into account
    by clients (that is, read preference options).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的副本集成员用于特殊任务。它们对客户端不可见，在`db.isMaster()` mongo shell命令和类似的管理命令中不会显示，并且对客户端不会被考虑（即读取首选项选项）。
- en: They can vote for elections, but will never become a primary server. A hidden
    replica set member will only sync up to the primary server, and doesn't take reads
    from the clients. As such, it has the same write load as the primary server (for
    replication purposes), but no read load on its own.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以投票选举，但永远不会成为主服务器。隐藏的副本集成员只会同步到主服务器，并不会从客户端读取。因此，它具有与主服务器相同的写入负载（用于复制目的），但自身没有读取负载。
- en: Due to the previously mentioned characteristics, reporting is the most common
    application of a hidden member. We can connect directly to this member and use
    it as the data source of truth for OLAP.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面提到的特性，报告是隐藏成员最常见的应用。我们可以直接连接到此成员并将其用作OLAP的数据源。
- en: 'To set up a hidden replica set member, we follow a similar procedure to `priority`
    to `0`. After we have connected to our primary via the mongo shell, we get the
    configuration object, identify the member in the members sub-document that corresponds
    to the member we want to set as `hidden`, and subsequently set its `priority`
    to `0` and its `hidden` attribute to `true`. Finally, we have to apply the new
    configuration by calling `rs.reconfig(config_object)` with `config_object` that
    we used as a parameter:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置隐藏的副本集成员，我们遵循与`priority`为`0`类似的过程。在通过mongo shell连接到我们的主服务器后，我们获取配置对象，识别在成员子文档中对应于我们想要设置为`hidden`的成员的成员，并随后将其`priority`设置为`0`，将其`hidden`属性设置为`true`。最后，我们必须通过调用`rs.reconfig(config_object)`并将`config_object`作为参数使用来应用新配置：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A `hidden` replica set member can also be used for backup purposes. However,
    as you will see in the next section, we may want to use other options, either
    at the physical level or to replicate data at the logical level. In those cases,
    consider using a delayed replica set instead.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`hidden`副本集成员也可以用于备份目的。然而，正如您将在下一节中看到的，我们可能希望在物理级别或逻辑级别复制数据时使用其他选项。在这些情况下，考虑使用延迟副本集。'
- en: Delayed replica set members
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟副本集成员
- en: In many cases, we will want to have a node that holds a copy of our data at
    an earlier point in time. This helps to recover from a big subset of human errors,
    like accidentally dropping a collection, or an upgrade going horrendously wrong.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们希望有一个节点在较早的时间点保存我们的数据副本。这有助于从大量人为错误中恢复，比如意外删除集合或升级出现严重问题。
- en: A delayed replica set member has to be `priority = 0` and `hidden = true`. A
    delayed replica set member can vote for elections, but will never be visible to
    clients (`hidden = true`) and will never become a primary (`priority = 0`).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟的副本集成员必须是 `priority = 0` 和 `hidden = true`。延迟的副本集成员可以投票进行选举，但永远不会对客户端可见（`hidden
    = true`），也永远不会成为主服务器（`priority = 0`）。
- en: 'An example is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例如下：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will set the `members[0]` to a delay of 2 hours. Two important factors
    for deciding the delta time period between the primary and delayed secondary server
    are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把 `members[0]` 设置为延迟 2 小时。决定主服务器和延迟次要服务器之间时间间隔的两个重要因素如下：
- en: Enough oplog size in the primary
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要副本中足够的 oplog 大小
- en: Enough time for the maintenance to finish before the delayed member starts picking
    up data
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在延迟成员开始获取数据之前，足够的维护时间
- en: 'The following table shows the delay of the replica set in hours:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了副本集的延迟时间（以小时为单位）：
- en: '| **Maintenance window, in hours** | **Delay** | **Oplog size on primary, in
    hours** |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **维护窗口，以小时为单位** | **延迟** | **主要副本的 oplog 大小，以小时为单位** |'
- en: '| *0.5* | *[0.5,5)* | *5* |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| *0.5* | *[0.5,5)* | *5* |'
- en: Production considerations
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产考虑
- en: Deploy each `mongod` instance on a separate physical host. If you are using
    VMs, make sure that they map to different underlying physical hosts. Use the `bind_ip`
    option to make sure that your server maps to a specific network interface and
    port address.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独的物理主机上部署每个 `mongod` 实例。如果使用虚拟机，请确保它们映射到不同的基础物理主机。使用 `bind_ip` 选项确保服务器映射到特定的网络接口和端口地址。
- en: Use firewalls to block access to any other port and/or only allow access between
    application servers and MongoDB servers. Even better, set up a VPN so that your
    servers communicate with each other in a secure, encrypted fashion.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用防火墙阻止对任何其他端口的访问和/或仅允许应用程序服务器和 MongoDB 服务器之间的访问。更好的做法是设置 VPN，以便您的服务器以安全的加密方式相互通信。
- en: Connecting to a replica set
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到副本集
- en: 'Connecting to a replica set is not fundamentally different from connecting
    to a single server. In this section, we will show some examples that use the official
    `mongo-ruby-driver`. We will use the following steps for the replica set, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到副本集与连接到单个服务器本质上没有太大不同。在本节中，我们将展示一些使用官方 `mongo-ruby-driver` 的示例。我们将按以下步骤进行副本集的操作：
- en: 'First, we need to set our `host` and `options` objects:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要设置我们的 `host` 和 `options` 对象：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding example, we are getting ready to connect to `hostname:port`,
    in the database signals in `replica_set xmr_btc`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们准备连接到 `hostname:port`，在 `replica_set xmr_btc` 数据库中的信号。
- en: 'Calling the initializer on `Mongo::Client` will now return a `client` object
    that contains a connection to our replica set and database:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Mongo::Client` 上调用初始化器现在将返回一个包含连接到我们的副本集和数据库的 `client` 对象：
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `client` object has the same options it has when connecting to a single
    server.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`client` 对象在连接到单个服务器时具有相同的选项。'
- en: MongoDB uses auto-discovery after connecting to our `client_host` to identify
    the other members of our replica set, regardless of whether they are the primary
    or secondaries. The `client` object should be used as a singleton, created once
    and reused across our code base.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到副本集后，MongoDB 在连接到我们的 `client_host` 后使用自动发现来识别副本集的其他成员，无论它们是主服务器还是次要服务器。`client`
    对象应该作为单例使用，创建一次并在整个代码库中重复使用。
- en: Having a singleton `client` object is a rule that can be overridden in some
    cases. We should create different `client` objects if we have different classes
    of connections to our replica set.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某些情况下，可以覆盖使用单例 `client` 对象的规则。如果我们有不同类别的连接到副本集，应该创建不同的 `client` 对象。
- en: 'An example would be having a `client` object for most operations, and then
    another `client` object for operations that are fine with only reading from secondaries:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于大多数操作使用一个 `client` 对象，然后对于只从次要服务器读取的操作使用另一个 `client` 对象：
- en: '[PRE27]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This Ruby MongoDB `client` command will return a copy of the `MongoDB:Client`
    object with a read preference secondary that can be used, for example, for reporting
    purposes.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个 Ruby MongoDB `client` 命令将返回一个包含读取偏好为次要的 `MongoDB:Client` 对象的副本，例如，用于报告目的。
- en: 'Some of the most useful options that we can use in our `client_options` initialization
    object are as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `client_options` 初始化对象中可以使用的一些最有用的选项如下：
- en: '| **Option** | **Description** | **Type** | **Default** |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **选项** | **描述** | **类型** | **默认** |'
- en: '| `replica_set` | As used in our example: the replica set name. | String |
    None |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| `replica_set` | 在我们的示例中使用：副本集名称。 | 字符串 | 无 |'
- en: '| `write` | The `write` concern options as a `hash` object; the available options
    are `w`, `wtimeout`, `j`, and `fsync`.That is, to specify writes to two servers,
    with journaling, flushing to disk (`fsync`) `true`, and a timeout of `1` second:`{
    write: { w: 2, j: true, wtimeout: 1000, fsync: true } }` | Hash | `{ w: 1 }` |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| `write` | `write` 关注选项作为 `hash` 对象；可用选项为 `w`、`wtimeout`、`j` 和 `fsync`。也就是说，要指定写入到两个服务器，启用日志记录，刷新到磁盘（`fsync`）为
    `true`，并设置超时为 `1` 秒：`{ write: { w: 2, j: true, wtimeout: 1000, fsync: true } }`
    | 哈希 | `{ w: 1 }` |'
- en: '| `read` | The read preference mode as a hash. Available options are `mode`
    and `tag_sets`.That is, to limit reads from secondary servers that have tag `UKWrites`:`{
    read:` ` { mode: :secondary,`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '| `read` | 读取偏好模式作为哈希。可用选项为 `mode` 和 `tag_sets`。也就是说，限制从具有标签 `UKWrites` 的次要服务器读取：`{
    read:` ` { mode: :secondary,`'
- en: '`   tag_sets: [ "UKWrites" ]`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`   tag_sets: [ "UKWrites" ]`'
- en: '` }`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '` }`'
- en: '`}` | Hash | `{ mode: primary }` |'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`}` | 哈希 | `{ mode: primary }` |'
- en: '| `user` | The name of the user to authenticate with. | String | None |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `user` | 要进行身份验证的用户的名称。 | 字符串 | 无 |'
- en: '| `password` | The password of the user to authenticate with. | String | None
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `password` | 要进行身份验证的用户的密码。 | 字符串 | 无 |'
- en: '| `connect` | Using `:direct`, we can force treat a replica set member as a
    standalone server, bypassing auto-discovery.Other options include: `:direct`,
    `:replica_set`, and `:sharded`. | Symbol | None |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `connect` | 使用`:direct`，我们可以强制将副本集成员视为独立服务器，绕过自动发现。其他选项包括：`:direct`，`:replica_set`和`:sharded`。
    | 符号 | 无 |'
- en: '| `heartbeat_frequency` | How often replica set members will communicate to
    check whether they are all alive. | Float | `10` |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `heartbeat_frequency` | 副本集成员定期通信以检查它们是否都存活的频率。 | 浮点数 | `10` |'
- en: '| `database` | Database connection. | String | `admin` |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `database` | 数据库连接。 | 字符串 | `admin` |'
- en: Similar to connecting to a standalone server, there are also options for SSL
    and authentication that are used in the same way.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与连接到独立服务器类似，SSL和身份验证也有相同的选项。
- en: 'We can also configure the connection pool by setting the following code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过设置以下代码来配置连接池：
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The MongoDB driver will try to reuse existing connections, if available, or
    it will open a new connection. Once the pool limit has been reached, the driver
    will block, waiting for a connection to be released to use it.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可用，MongoDB驱动程序将尝试重用现有连接，否则将打开新连接。一旦达到池限制，驱动程序将阻塞，等待连接被释放以使用它。
- en: Replica set administration
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集管理
- en: The administration of a replica set can be significantly more complex than what
    is needed for single-server deployments. In this section, instead of trying to
    exhaustively cover all of the different cases, we will focus on some of the most
    common administrative tasks that we will have to perform, and how to do them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集的管理可能比单服务器部署所需的要复杂得多。在本节中，我们将重点放在一些最常见的管理任务上，而不是试图详尽地涵盖所有不同的情况，以及如何执行这些任务。
- en: How to perform maintenance on replica sets
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何对副本集执行维护
- en: 'If we have some maintenance tasks that we have to perform in every member in
    a replica set, we always start with the secondaries. We perform maintenance by
    performing the following steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一些在副本集的每个成员中都必须执行的维护任务，我们总是从辅助节点开始。我们通过执行以下步骤来执行维护：
- en: 'First, we connect to one of the secondaries via the mongo shell. Then, we stop
    that secondary:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过mongo shell连接到其中一个辅助节点。然后，我们停止该辅助节点：
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, using the same user that was connected to the mongo shell in the previous
    step, we restart the mongo server as a standalone server in a different port:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用在上一步中连接到mongo shell的相同用户，我们在不同的端口上将mongo服务器重新启动为独立服务器：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next step is to connect to this `mongod` server (which is using `dbpath`):'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是连接到使用`dbpath`的`mongod`服务器：
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: At this point, we can safely perform all of the administrative tasks on our
    standalone server without affecting our replica set operations. When we are done,
    we shut down the standalone server in the same way that we did in the first step.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以安全地执行所有独立服务器上的管理任务，而不会影响我们的副本集操作。完成后，我们以与第一步相同的方式关闭独立服务器。
- en: 'We can then restart our server in the replica set by using the command line
    or the configuration script that we normally use. The final step is to verify
    that everything works fine by connecting to the replica set server and getting
    its replica set `status`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过使用命令行或我们通常使用的配置脚本来重新启动副本集中的服务器。最后一步是通过连接到副本集服务器并获取其副本集`status`来验证一切是否正常：
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The server should initially be in `state: RECOVERING`, and, once it has caught
    up with the secondary, it should be back in `state: SECONDARY`, like it was before
    starting the maintenance.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '服务器最初应处于`state: RECOVERING`状态，一旦它赶上了辅助服务器，它应该回到`state: SECONDARY`状态，就像在开始维护之前一样。'
- en: 'We will repeat the same process for every secondary server. In the end, we
    have to perform maintenance on the primary. The only difference in the process
    for the primary is that we will start by stepping down our primary server into
    a secondary server before every other step:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个辅助服务器重复相同的过程。最后，我们必须对主服务器进行维护。主服务器的过程唯一的不同之处在于，在每一步之前，我们将首先将主服务器降级为辅助服务器：
- en: '[PRE33]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: By using the above argument, we prevent our secondary from being elected as
    a master for 10 minutes. This should be enough time to shut down the server and
    continue with our maintenance, like we did with the secondaries.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用上述参数，我们可以防止我们的辅助节点在10分钟内被选为主节点。这应该足够的时间来关闭服务器并继续进行维护，就像我们对辅助节点所做的那样。
- en: Re-syncing a member of a replica set
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新同步副本集的成员
- en: Secondaries sync up with the primary by replaying the contents of the oplog.
    If our oplog is not large enough, or if we encounter network issues (partitioning,
    an underperforming network, or just an outage of the secondary server) for a period
    of time larger than the oplog, then MongoDB cannot use the oplog to catch up to
    the primary anymore.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助节点通过重放oplog的内容与主节点同步。如果我们的oplog不够大，或者如果我们遇到网络问题（分区、网络性能不佳，或者辅助服务器的故障）的时间超过oplog，那么MongoDB将无法使用oplog来赶上主节点。
- en: 'At this point, we have two options:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有两个选择：
- en: The more straightforward option is to delete our `dbpath` directory and restart
    the `mongod` process. In this case, MongoDB will start an initial sync from scratch.
    This option has the downside of putting a strain on our replica set, and our network,
    as well.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更直接的选择是删除我们的`dbpath`目录并重新启动`mongod`进程。在这种情况下，MongoDB将从头开始进行初始同步。这种选择的缺点是对我们的副本集和网络造成压力。
- en: The more complicated (from an operational standpoint) option is to copy data
    files from another well-behaving member of the replica set. This goes back to
    the contents of Chapter 8, *Monitoring, Backup, and Security*. The important thing
    to keep in mind is that a simple file copy will probably not suffice, as data
    files will have changed from the time that we started copying to the time that
    the copying ended.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更复杂（从操作角度）的选项是从副本集的另一个表现良好的成员复制数据文件。这回到了第8章的内容，*监控、备份和安全性*。要记住的重要事情是，简单的文件复制可能不够，因为数据文件在我们开始复制到复制结束的时间内已经发生了变化。
- en: Thus, we need to be able to take a snapshot copy of the filesystem under our
    `data` directory.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要能够在我们的`data`目录下拍摄文件系统的快照副本。
- en: Another point of consideration is that by the time we start our secondary server
    with the newly copied files, our MongoDB secondary server will try to sync up
    to the primary using the oplog again. So, if our oplog has fallen so far behind
    the primary that it can't find the entry on our primary server, this method will
    fail, too.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的问题是，当我们使用新复制的文件启动次要服务器时，我们的MongoDB次要服务器将尝试再次使用oplog与主服务器同步。因此，如果我们的oplog已经落后于主服务器，以至于它无法在主服务器上找到条目，这种方法也会失败。
- en: Keep a sufficiently sized oplog. Don't let data grow out of hand in any replica
    set member. Design, test, and deploy sharding early on.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 保持足够大小的oplog。不要让任何副本集成员的数据失控。尽早设计、测试和部署分片。
- en: Changing the oplog's size
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改oplog的大小
- en: 'Hand in hand with the previous operational tip, we may need to rethink and
    resize our oplog as our data grows. Operations become more complicated and time-consuming
    as our data grows, and we need to adjust our oplog size to accommodate for it.
    The steps for changing the oplog''s size are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的操作提示相辅相成，随着数据的增长，我们可能需要重新考虑和调整oplog的大小。随着数据的增长，操作变得更加复杂和耗时，我们需要调整oplog的大小来适应。更改oplog大小的步骤如下：
- en: The first step is to restart our MongoDB secondary server as a standalone server,
    an operation that was described in the *How to perform maintenance on replica
    sets* section.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是将我们的MongoDB次要服务器重新启动为独立服务器，这是在*如何对副本集执行维护*部分中描述的操作。
- en: 'We then make a backup of our existing oplog:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们备份我们现有的oplog：
- en: '[PRE34]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We keep a copy of this data, just in case. We then connect to our standalone
    database:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们保留这些数据的副本，以防万一。然后我们连接到我们的独立数据库：
- en: '[PRE35]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Up until now, we have connected to the `local` database and deleted the `temp`
    collection, just in case it had any leftover documents.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已连接到`local`数据库并删除了`temp`集合，以防它有任何剩余文档。
- en: 'The next step is to get the last entry of our current oplog and save it in
    the `temp` collection:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是获取我们当前oplog的最后一个条目，并将其保存在`temp`集合中：
- en: '[PRE36]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This entry will be used when we restart our secondary server, in order to track
    where it has reached in the oplog replication:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们重新启动次要服务器时，将使用此条目，以跟踪它在oplog复制中的进度：
- en: '[PRE37]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we delete our existing oplog, and in the next step, we will create a new
    oplog of `4` GB in `size`:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们删除我们现有的oplog，在下一步中，我们将创建一个大小为`4`GB的新oplog：
- en: '[PRE38]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The next step is to copy the one entry from our `temp` collection back to our
    oplog:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将我们的`temp`集合中的一个条目复制回我们的oplog：
- en: '[PRE39]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Finally, we cleanly shut down our server from the `admin` database, using the
    `db.shutdownServer()` command, and we restart our secondary as a member of the
    replica set.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们从`admin`数据库中干净地关闭服务器，使用`db.shutdownServer()`命令，然后将我们的次要服务器重新启动为副本集的成员。
- en: 'We repeat this process for all secondary servers, and as a last step, we repeat
    the procedure for our primary member, which is done after we step the primary
    down by using the following command:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对所有次要服务器重复此过程，最后一步是对我们的主要成员重复该过程，这是在使用以下命令将主服务器降级之后完成的：
- en: '[PRE40]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Reconfiguring a replica set when we have lost the majority of our servers
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在我们失去大多数服务器时重新配置副本集
- en: This is only intended as an interim solution and a last resort when we are faced
    with downtime and disrupted cluster operations. When we lose the majority of our
    servers and we still have enough servers to start a replica set (maybe including
    some quickly spawned arbiters), we can force a reconfiguration with only the surviving
    members.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个临时解决方案，也是在面临停机和集群操作中断时的最后手段。当我们失去大多数服务器，但仍有足够的服务器可以启动一个副本集（可能包括一些快速生成的仲裁者）时，我们可以强制只使用幸存成员进行重新配置。
- en: 'First, we get the replica set configuration document:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们获取副本集配置文档：
- en: '[PRE41]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using `printjson(cfg)`, we identify the members that are still operational.
    Let''s say that these are `1`, `2`, and `3`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`printjson(cfg)`，我们确定仍在运行的成员。假设这些成员是`1`、`2`和`3`：
- en: '[PRE42]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'By using `force : true`, we are forcing this reconfiguration to happen. Of
    course, we need to have at least three surviving members in our replica set for
    this to work.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`force：true`，我们强制进行此重新配置。当然，我们需要至少有三个幸存成员在我们的副本集中才能使其工作。
- en: It's important to remove the failing servers as soon as possible, by killing
    the processes and/or taking them out of the network, avoid unintended consequences;
    these servers may believe that they are still a part of a cluster that doesn't
    acknowledge them anymore.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽快删除故障服务器非常重要，方法是终止进程和/或将它们从网络中移除，以避免意外后果；这些服务器可能认为它们仍然是集群的一部分，而集群已不再承认它们。
- en: Chained replication
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链式复制
- en: Replication in MongoDB usually happens from the primary to the secondaries.
    In some cases, we may want to replicate from another secondary, instead of the
    primary. Chained replication helps to alleviate the primary from read load, but
    at the same time, it increases the average replication lag for the secondary that
    chooses to replicate from a secondary. This makes sense, as replication has to
    go from the primary to the secondary (1), and then from this server to another
    secondary (2).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中，复制通常发生在主服务器和次要服务器之间。在某些情况下，我们可能希望从另一个次要服务器复制，而不是从主服务器复制。链式复制有助于减轻主服务器的读取负载，但与此同时，它会增加选择从次要服务器复制的次要服务器的平均复制延迟。这是有道理的，因为复制必须从主服务器到次要服务器（1），然后从这台服务器到另一个次要服务器（2）。
- en: 'Chained replication can be enabled (and disabled, respectively) with the following
    `cfg` command:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下`cfg`命令启用（或分别禁用）链式复制：
- en: '[PRE43]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In cases where `printjson(cfg)` doesn''t reveal a settings sub-document, we
    need to create an empty one first:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在`printjson(cfg)`不显示设置子文档的情况下，我们需要首先创建一个空文档：
- en: '[PRE44]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If there is already a `settings` document, the preceding command will result
    in deleting its settings, leading to potential data loss.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经存在一个`settings`文档，上述命令将导致删除其设置，可能导致数据丢失。
- en: Cloud options for a replica set
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集的云选项
- en: We can set up and operate a replica set from our own servers, but we can reduce
    our operational overhead by using a **Database as a Service** (**DBaaS**) provider
    to do so. The two most widely used MongoDB cloud providers are mLab (formerly
    MongoLab) and MongoDB Atlas, the native offering from MongoDB, Inc.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从我们自己的服务器上设置和操作副本集，但是我们可以通过使用**数据库即服务**（**DBaaS**）提供商来减少我们的运营开销。最广泛使用的两个MongoDB云提供商是mLab（以前是MongoLab）和MongoDB
    Atlas，后者是MongoDB, Inc.的原生产品。
- en: In this section, we will go over these options and how they fare in comparison
    to using our own hardware and data centers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论这些选项以及它们与使用我们自己的硬件和数据中心相比的优劣。
- en: mLab
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mLab
- en: mLab is one of the most popular cloud DBaaS providers for MongoDB. It has been
    offered since 2011 and it is considered a stable and mature provider.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: mLab是MongoDB最受欢迎的云DBaaS提供商之一。自2011年以来一直提供，并被认为是一个稳定和成熟的提供商。
- en: After signing up, we can easily deploy a replica set cluster in a set of cloud
    servers without any operational overhead. Configuration options include AWS, Microsoft
    Azure, or Google Cloud as the underlying server provider.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注册后，我们可以在一组云服务器上轻松部署副本集群，而无需任何运营开销。配置选项包括AWS、Microsoft Azure或Google Cloud作为基础服务器提供商。
- en: There are multiple sizing options for the latest MongoDB version. There was
    no support at the time of writing this book for the MMAPv1 storage engine. There
    are multiple regions for each provider (US, Europe, and Asia). Most notably, the
    regions that are missing are the AWS China, AWS US Gov, and AWS Germany regions.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的MongoDB版本有多个大小选项。在撰写本书时，MMAPv1存储引擎没有支持。每个提供商都有多个地区（美国、欧洲和亚洲）。值得注意的是，缺少的地区是AWS中国、AWS美国政府和AWS德国地区。
- en: MongoDB Atlas
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MongoDB Atlas
- en: MongoDB Atlas is a newer offering from MongoDB, Inc., and was launched in summer
    2016\. Similar to mLab, it offers the deployment of single-server, replica set,
    or sharded clusters, through a web interface.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Atlas是MongoDB, Inc.的一个较新的产品，于2016年夏季推出。与mLab类似，它通过Web界面提供单服务器、副本集或分片集群的部署。
- en: It offers the latest MongoDB version. The only storage option is WiredTiger.
    There are multiple regions for each provider (US, Europe, and Asia).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了最新的MongoDB版本。唯一的存储选项是WiredTiger。每个提供商都有多个地区（美国、欧洲和亚洲）。
- en: Most notably, the regions that are missing are the AWS China and AWS US Gov
    regions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，缺少的地区是AWS中国和AWS美国政府地区。
- en: In both (and most of the other) providers, we can't have a cross-region replica
    set. This is prohibitive if we want to deploy a truly global service, serving
    users from multiple data centers around the globe, with our MongoDB servers being
    as close to the application servers as possible.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个（以及大多数其他）提供商中，我们无法拥有跨区域的副本集。如果我们想要部署一个真正全球的服务，为来自全球多个数据中心的用户提供服务，并且希望我们的MongoDB服务器尽可能靠近应用服务器，这是不利的。
- en: The running costs for cloud-hosted services can be significantly higher than
    setting them up in our own servers. What we gain in convenience and time to market
    may have to be paid in operational costs.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 云托管服务的运行成本可能会比在我们自己的服务器上设置要高得多。我们在便利性和上市时间上所获得的可能需要以运营成本来支付。
- en: Replica set limitations
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集的限制
- en: 'A replica set is great when we understand why we need it and what it cannot
    do. The different limitations for a replica set are as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们了解为什么需要副本集以及它不能做什么时，副本集就非常好。副本集的不同限制如下：
- en: It will not scale horizontally; we need sharding for it
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不会进行水平扩展；我们需要分片来实现。
- en: We will introduce replication issues if our network is flaky
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的网络不稳定，我们将引入复制问题。
- en: We will make debugging issues more complex if we use secondaries for reads,
    and these have fallen behind our primary server
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用辅助服务器进行读取，那么调试问题将变得更加复杂，而且这些辅助服务器已经落后于我们的主服务器。
- en: On the flip side, as we explained in previous sections in this chapter, a replica
    set can be a great choice for replication, data redundancy, conforming with data
    privacy, backups, and even recovery from errors caused by humans, or otherwise.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，正如我们在本章的前几节中所解释的，副本集对于复制、数据冗余、符合数据隐私、备份甚至从人为错误或其他原因引起的错误中恢复来说都是一个很好的选择。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed replica sets and how to administer them. Starting
    with an architectural overview of replica sets and replica set internals involving
    elections, we dove into setting up and configuring a replica set.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了副本集以及如何对其进行管理。从副本集的架构概述和涉及选举的副本集内部开始，我们深入到了设置和配置副本集。
- en: You learned how to perform various administrative tasks with replica sets, and
    you learned about the main options for outsourcing operations to a cloud DBaaS
    provider. Finally, we identified some of the limitations that replica sets in
    MongoDB currently have.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 您学会了如何使用副本集执行各种管理任务，并了解了将操作外包给云DBaaS提供商的主要选项。最后，我们确定了MongoDB目前副本集存在的一些限制。
- en: 'In the next chapter, we will move on to one of the most interesting concepts
    in MongoDB (which helps it to achieve horizontal scaling): sharding.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论MongoDB中最有趣的概念之一（帮助其实现水平扩展的概念）：分片。
