- en: Sharding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片
- en: Sharding is the ability to horizontally scale out our database by partitioning
    our datasets across different servers—the shards. It has been a feature of MongoDB
    since version 1.6 was released in August, 2010\. Foursquare and Bitly are two
    of MongoDB's most famous early customers, and have used the sharding feature from
    its inception all the way to its general release.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分片是通过将数据集分区到不同服务器（分片）上来横向扩展我们的数据库的能力。这是MongoDB自2010年8月发布1.6版本以来的一个特性。Foursquare和Bitly是MongoDB最著名的早期客户之一，从其推出一直到其正式发布都使用了分片功能。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: How to design a sharding cluster and how to make the single most important decision
    concerning its use—choosing the shard key
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设计分片集群以及如何做出关于其使用的最重要决定——选择分片键
- en: Different sharding techniques and how to monitor and administrate sharded clusters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的分片技术以及如何监视和管理分片集群
- en: The `mongos` router and how it is used to route our queries across different
    shards
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mongos`路由器及其用于在不同分片之间路由我们的查询的方式'
- en: How we can recover from errors in our shard
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何从分片中恢复错误
- en: Why do we use sharding?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用分片？
- en: In database systems and computing systems in general, we have two ways to improve
    performance. The first one is to simply replace our servers with more powerful
    ones, keeping the same network topology and systems architecture. This is called
    **vertical scaling**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库系统和计算系统中，我们有两种方法来提高性能。第一种方法是简单地用更强大的服务器替换我们的服务器，保持相同的网络拓扑和系统架构。这被称为**垂直扩展**。
- en: An advantage of vertical scaling is that it is simple, from an operational standpoint,
    especially with cloud providers such as Amazon making it a matter of a few clicks
    to replace an **m2.medium** with an **m2.extralarge** server instance. Another
    advantage is that we don't need to make any code changes, and so there is little
    to no risk of something going catastrophically wrong.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展的优点是从操作的角度来看很简单，特别是像亚马逊这样的云服务提供商只需点击几下就可以用**m2.extralarge**服务器实例替换**m2.medium**。另一个优点是我们不需要进行任何代码更改，因此几乎没有什么东西会出现灾难性的错误。
- en: The main disadvantage of vertical scaling is that there is a limit to it; we
    can only get servers that are as powerful as those that our cloud provider can
    give to us.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展的主要缺点是存在限制；我们只能获得与云服务提供商提供给我们的服务器一样强大的服务器。
- en: A related disadvantage is that getting more powerful servers generally comes
    with an increase in cost that is not linear but exponential. So, even if our cloud
    provider offers more powerful instances, we will hit the cost effectiveness barrier
    before we hit the limit of our department's credit card.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的缺点是获得更强大的服务器通常会带来成本的增加，这种增加不是线性的而是指数级的。因此，即使我们的云服务提供商提供更强大的实例，我们在部门信用卡的成本效益限制之前就会遇到成本效益的障碍。
- en: The second way to improve performance is by using the same servers with the
    same capacity and increase their number. This is called **horizontal scaling**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的第二种方法是使用相同容量的相同服务器并增加它们的数量。这被称为**水平扩展**。
- en: 'Horizontal scaling offers the advantage of theoretically being able to scale
    exponentially while remaining practical enough for real-world applications. The
    main disadvantage is that it can be operationally more complex and requires code
    changes and the careful design of the system upfront. Horizontal scaling is also
    more complex when it comes to the system because it requires communication between
    the different servers over network links that are not as reliable as inter-process
    communication on a single server. The following diagram shows the difference between
    horizontal and vertical scaling:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 水平扩展的优点在于理论上能够呈指数级扩展，同时对于现实世界的应用来说仍然足够实用。主要缺点是在操作上可能更加复杂，需要进行代码更改并在系统设计上进行仔细设计。水平扩展在系统方面也更加复杂，因为它需要在不太可靠的网络链接上的不同服务器之间进行通信，而不是在单个服务器上进行进程间通信。以下图表显示了水平和垂直扩展之间的区别：
- en: '![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)'
- en: 'To understand scaling, it''s important to understand the limitations of single-server
    systems. A server is typically bound by one or more of the following characteristics:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解扩展，重要的是要了解单服务器系统的限制。服务器通常受以下一个或多个特征的限制：
- en: '**CPU**: A CPU-bound system is one that is limited by our CPU''s speed. A task
    such as the multiplication of matrices that can fit in RAM will be CPU bound because
    there is a specific number of steps that have to be performed in the CPU without
    any disk or memory access needed for the task to complete. In this case, CPU usage
    is the metric that we need to keep track of.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU**：CPU受限系统是受CPU速度限制的系统。例如，可以放入RAM的矩阵相乘任务将受到CPU限制，因为CPU必须执行特定数量的步骤，而不需要进行任何磁盘或内存访问即可完成任务。在这种情况下，CPU使用率是我们需要跟踪的指标。'
- en: '**I/O**: Input-output bound systems are similarly limited by the speed of our
    storage system (HDD or SSD). A task such as reading large files from a disk to
    load into memory will be I/O bound as there is little to do in terms of CPU processing;
    the great majority of the time is spent reading the files from the disk. The important
    metrics to keep track of are all the metrics related to disk access, the reads
    per second, and the writes per second, compared to the practical limit of our
    storage system.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**I/O**：输入输出受限系统同样受到存储系统（HDD或SSD）速度的限制。例如，从磁盘读取大文件加载到内存中的任务将受到I/O限制，因为在CPU处理方面几乎没有什么要做的；大部分时间都花在从磁盘读取文件上。需要跟踪的重要指标是与磁盘访问相关的所有指标，每秒读取次数和每秒写入次数，与我们存储系统的实际限制相比。'
- en: '**Memory and cache**: Memory-bound and cache-bound systems are restricted by
    the amount of available RAM memory and/or the cache size that we have assigned
    to them. A task that multiplies matrices larger than our RAM size will be memory
    bound, as it will need to page in/out data from the disk to perform the multiplication.
    The important metric to keep track of is the memory used. This may be misleading
    in MongoDB MMAPv1, as the storage engine will allocate as much memory as possible
    through the filesystem cache.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存和缓存**：受内存限制和缓存限制的系统受到可用RAM内存和/或我们分配给它们的缓存大小的限制。一个将矩阵乘以大于我们RAM大小的任务将受到内存限制，因为它将需要从磁盘中分页数据来执行乘法。要跟踪的重要指标是已使用的内存。在MongoDB
    MMAPv1中，这可能会产生误导，因为存储引擎将通过文件系统缓存分配尽可能多的内存。'
- en: In the WiredTiger storage engine, on the other hand, if we don't allocate enough
    memory for the core MongoDB process, out-of-memory errors may kill it, and this
    is something that we want to avoid at all costs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在WiredTiger存储引擎中，如果我们没有为核心MongoDB进程分配足够的内存，内存不足的错误可能会导致其崩溃，这是我们要尽一切努力避免的。
- en: Monitoring memory usage has to be done both directly through the operating system
    and indirectly by keeping a track of page in/out data. An increasing memory paging
    number is often an indication that we are running short of memory and the operating
    system is using virtual address space to keep up.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监控内存使用量必须通过操作系统直接进行，并间接地通过跟踪分页数据来进行。增加的内存分页数通常表明我们的内存不足，操作系统正在使用虚拟地址空间来跟上。
- en: MongoDB, being a database system, is generally memory and I/O bound. Investing
    in SSD and more memory for our nodes is almost always a good investment. Most
    systems are a combination of one or more of the preceding limitations. Once we
    add more memory, our system may become CPU bound, as complex operations are almost
    always a combination of CPU, I/O, and memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据库系统的MongoDB通常受到内存和I/O的限制。为我们的节点投资SSD和更多内存几乎总是一个不错的投资。大多数系统是前述限制的一个或多个组合。一旦我们增加了更多内存，我们的系统可能会变得CPU受限，因为复杂的操作几乎总是CPU、I/O和内存使用的组合。
- en: MongoDB's sharding is simple enough to set up and operate, and this has contributed
    to its huge success over the years as it provides the advantages of horizontal
    scaling without requiring a large commitment of engineering and operations resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB的分片设置和操作非常简单，这也是它多年来取得巨大成功的原因，因为它提供了横向扩展的优势，而不需要大量的工程和运营资源。
- en: That being said, it's really important to get sharding right from the beginning,
    as it is extremely difficult from an operational standpoint to change the configuration
    once it has been set up. Sharding should not be an afterthought, but rather a
    key architectural design decision from an early point in the design process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，从一开始就正确地进行分片非常重要，因为一旦设置好了，从操作的角度来看，要更改配置是非常困难的。分片不应该是一个事后想法，而应该是设计过程早期的一个关键架构设计决策。
- en: Architectural overview
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构概述
- en: 'A sharded cluster is comprised of the following elements:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分片集群由以下元素组成：
- en: Two or more shards. Each shard must be a replica set.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个或更多分片。每个分片必须是一个副本集。
- en: One or more query routers (`mongos`). A `mongos` provides an interface between
    our application and the database.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个查询路由器（`mongos`）。`mongos`提供了我们的应用程序和数据库之间的接口。
- en: A replica set of config servers. Config servers store metadata and configuration
    settings for the entire cluster.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个副本集的配置服务器。配置服务器存储整个集群的元数据和配置设置。
- en: 'The relationships between these elements is shown in the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素之间的关系如下图所示：
- en: '![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)'
- en: As of MongoDB 3.6, shards must be implemented as replica sets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.6开始，分片必须实现为副本集。
- en: Development, continuous deployment, and staging environments
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发、持续部署和暂存环境
- en: In preproduction environments, it may be overkill to use the full set of servers.
    For efficiency reasons, we may opt to use a more simplified architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在预生产环境中，使用完整服务器集可能是过度的。出于效率原因，我们可能选择使用更简化的架构。
- en: 'The simplest possible configuration that we can deploy for sharding is the
    following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为分片部署的最简单配置如下：
- en: One `mongos` router
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`mongos`路由器
- en: One sharded replica set with one MongoDB server and two arbiters
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分片副本集，有一个MongoDB服务器和两个仲裁者
- en: One replica set of config servers with one MongoDB server and two arbiters
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个配置服务器的副本集，有一个MongoDB服务器和两个仲裁者
- en: This should be strictly used for development and testing as this architecture
    defies most of the advantages that a replica set provides, such as high availability,
    scalability, and replication of data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这应严格用于开发和测试，因为这种架构违背了副本集提供的大多数优势，如高可用性、可扩展性和数据复制。
- en: Staging is strongly recommended to mirror our production environment in terms
    of its servers, configuration, and (if possible) dataset requirements too, in
    order to avoid surprises at deployment time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议在暂存环境中镜像我们的生产环境，包括服务器、配置和（如果可能）数据集要求，以避免在部署时出现意外。
- en: Planning ahead with sharding
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提前计划分片
- en: As we will see in the next sections, sharding is complicated and expensive,
    operation wise. It is important to plan ahead and make sure that we start the
    sharding process long before we hit our system's limits.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的部分中看到的，分片在操作上是复杂且昂贵的。重要的是要提前计划，并确保我们在达到系统极限之前很久就开始分片过程。
- en: 'Some rough guidelines on when you need to start sharding are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于何时需要开始分片的大致指导原则如下：
- en: When you have a CPU utilization of less than 70% on average
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当平均CPU利用率低于70%时
- en: When I/O (and especially write) capacity is less than 80%
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当I/O（尤其是写入）容量低于80%时
- en: When memory utilization is less than 70% on average
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当平均内存利用率低于70%时
- en: As sharding helps with write performance, it's important to keep an eye on our
    I/O write capacity and the requirements of our application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分片有助于写入性能，重要的是要关注我们的I/O写入容量和应用程序的要求。
- en: Don't wait until the last minute to start sharding in an already busy-up-to-the-neck
    MongoDB system, as it can have unintended consequences.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 不要等到最后一刻才开始在已经忙碌到极致的MongoDB系统中进行分片，因为这可能会产生意想不到的后果。
- en: Sharding setup
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片设置
- en: Sharding is performed at the collection level. We can have collections that
    we don't want or need to shard for several reasons. We can leave these collections
    unsharded.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分片是在集合级别执行的。我们可以有一些我们不想或不需要分片的集合，有几个原因。我们可以将这些集合保持为未分片状态。
- en: These collections will be stored in the primary shard. The primary shard is
    different for each database in MongoDB. The primary shard is automatically selected
    by MongoDB when we create a new database in a sharded environment. MongoDB will
    pick the shard that has the least data stored at the moment of creation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集合将存储在主分片中。在MongoDB中，每个数据库的主分片都不同。在分片环境中创建新数据库时，MongoDB会自动选择主分片。MongoDB将选择在创建时存储数据最少的分片。
- en: 'If we want to change the primary shard at any other point, we can issue the
    following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在任何其他时间更改主分片，我们可以发出以下命令：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With this, we move the database named `mongo_books` to the shard named `UK_based`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们将名为`mongo_books`的数据库移动到名为`UK_based`的分片中。
- en: Choosing the shard key
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择分片键
- en: 'Choosing our shard key is the most important decision we need to make: once
    we shard our data and deploy our cluster, it becomes very difficult to change
    the shard key. First, we will go through the process of changing the shard key.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们的分片键是我们需要做出的最重要的决定：一旦我们分片我们的数据并部署我们的集群，更改分片键就变得非常困难。首先，我们将经历更改分片键的过程。
- en: Changing the shard key
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改分片键
- en: There is no command or simple procedure to change the shard key in MongoDB.
    The only way to change the shard key involves backing up and restoring all of
    our data, something that may range from being extremely difficult to impossible
    in high-load production environments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中，没有命令或简单的程序可以更改分片键。更改分片键的唯一方法涉及备份和恢复所有数据，这在高负载生产环境中可能从极其困难到不可能。
- en: 'The following are the steps that we need to go through in order to change the
    shard key:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要经历的步骤，以更改分片键：
- en: Export all data from MongoDB
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MongoDB导出所有数据
- en: Drop the original sharded collection
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除原始的分片集合
- en: Configure sharding with the new key
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新键配置分片
- en: Presplit the new shard key range
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预先拆分新的分片键范围
- en: Restore our data back into MongoDB
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的数据恢复到MongoDB中
- en: Of these steps, step 4 is the one that needs further explanation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤中，步骤4是需要进一步解释的步骤。
- en: MongoDB uses chunks to split data in a sharded collection. If we bootstrap a
    MongoDB sharded cluster from scratch, chunks will be calculated automatically
    by MongoDB. MongoDB will then distribute the chunks across different shards to
    ensure that there are an equal number of chunks in each shard.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB使用块来分割分片集合中的数据。如果我们从头开始引导MongoDB分片集群，MongoDB将自动计算块。然后，MongoDB将这些块分布到不同的分片上，以确保每个分片中有相等数量的块。
- en: The only time when we cannot really do this is when we want to load data into
    a newly sharded collection.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一不能真正做到这一点的时候是当我们想要将数据加载到新的分片集合中。
- en: 'The reasons for this are threefold:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因有三个：
- en: MongoDB creates splits only after an `insert` operation.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB仅在`insert`操作后创建拆分。
- en: Chunk migration will copy all of the data in that chunk from one shard to another.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块迁移将从一个分片复制该块中的所有数据到另一个分片。
- en: The `floor(n/2)` chunk migrations can happen at any given time, where `n` is
    the number of shards we have. Even with three shards, this is only a `floor(1.5)=1`
    chunk migration at a time.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`floor(n/2)`块迁移可以在任何时间发生，其中`n`是我们拥有的分片数量。即使有三个分片，这也只是一次`floor(1.5)=1`块迁移。'
- en: These three limitations mean that letting MongoDB figure it out on its own will
    definitely take much longer, and may result in an eventual failure. This is why
    we want to presplit our data and give MongoDB some guidance on where our chunks should go.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个限制意味着让MongoDB自行解决这个问题肯定会花费更长时间，并且可能最终导致失败。这就是为什么我们希望预先拆分我们的数据，并为MongoDB提供一些关于我们的块应该放在哪里的指导。
- en: 'In our example of the `mongo_books` database and the `books` collection, this
    would be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`mongo_books`数据库和`books`集合如下：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `middle` command parameter will split our key space in documents that have `id` less
    than or equal to `50` and documents that have `id` greater than `50`. There is
    no need for a document to exist in our collection with `id` that is equal to `50` as
    this will only serve as the guidance value for our partitions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`middle`命令参数将在我们的键空间中拆分文档，这些文档的`id`小于或等于`50`，以及`id`大于`50`的文档。我们的集合中没有必要存在`id`等于`50`的文档，因为这只会作为我们分区的指导值。'
- en: In this example, we chose `50`, as we assume that our keys follow a uniform
    distribution (that is, there is the same count of keys for each value) in the
    range of values from `0` to `100`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们选择了`50`，因为我们假设我们的键在值范围从`0`到`100`中遵循均匀分布（即，每个值的键数量相同）。
- en: We should aim to create at least 20-30 chunks to grant MongoDB flexibility in
    potential migrations. We can also use `bounds` and `find` instead of `middle`
    if we want to manually define the partition key, but both parameters need data
    to exist in our collection before applying them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该努力创建至少20-30个块，以赋予MongoDB在潜在迁移中的灵活性。如果我们想手动定义分区键，我们也可以使用`bounds`和`find`而不是`middle`，但是这两个参数在应用它们之前需要数据存在于我们的集合中。
- en: Choosing the correct shard key
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的分片键
- en: After the previous section, it's now self-evident that we need to take the choice
    of our shard key into consideration as it is a decision that we have to stick
    with.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分之后，现在很明显我们需要考虑我们的分片键的选择，因为这是一个我们必须坚持的决定。
- en: 'A great shard key has three characteristics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的分片键具有三个特点：
- en: High cardinality
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高基数
- en: Low frequency
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低频率
- en: Nonmonotonic changes in value
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值的非单调变化
- en: 'We will go over the definitions of these three properties first to understand
    what they mean:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍这三个属性的定义，以了解它们的含义：
- en: '**High cardinality**: It means that the shard key must have as many distinct
    values as possible. A Boolean can take only the values of `true`/`false`, and
    so it is a bad shard key choice. A 64-bit long value field that can take any value
    from *−(2^63)* to *2^63−1* is a good shard key choice, in terms of cardinality.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高基数**：这意味着分片键必须具有尽可能多的不同值。布尔值只能取`true`/`false`，因此不是一个好的分片键选择。一个可以取从*−(2^63)*到*2^63−1*的任何值的64位长值字段在基数方面是一个好的选择。'
- en: '**Low frequency**: It directly relates to the argument about high cardinality.
    A low-frequency shard key will have a distribution of values as close to a perfectly
    random/uniform distribution. Using the example of our 64-bit long value, it is
    of little use to us if we have a field that can take values ranging from *−(2^63)*
    to *2^63−1* if we end up observing the values of zero and one all the time. In
    fact, it is as bad as using a Boolean field, which can also take only two values. If
    we have a shard key with high-frequency values, we will end up with chunks that
    are indivisible. These chunks cannot be further divided and will grow in size, negatively affecting
    the performance of the shard that contains them.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低频率**：它直接关系到高基数的论点。低频率的分片键将具有接近完全随机/均匀分布的值分布。以我们64位长值的例子，如果我们一直观察到零和一这样的值，那么它对我们几乎没有用处。事实上，这和使用布尔字段一样糟糕，因为布尔字段也只能取两个值。如果我们有一个高频率值的分片键，我们最终会得到不可分割的块。这些块无法进一步分割，并且会增长，负面影响包含它们的分片的性能。'
- en: '**Nonmonotonically changing values**:It mean that our shard key should not
    be, for example, an integer that always increases with every new insert. If we
    choose a monotonically increasing value as our shard key, this will result in
    all writes ending up in the last of all of our shards, limiting our write performance.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非单调变化的值**：这意味着我们的分片键不应该是一个每次新插入都增加的整数，例如。如果我们选择一个单调递增的值作为我们的分片键，这将导致所有写入最终都进入我们所有分片中的最后一个，从而限制我们的写入性能。'
- en: If we want to use a monotonically changing value as the shard key, we should
    consider using hash-based sharding.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用单调变化的值作为分片键，我们应该考虑使用基于哈希的分片。
- en: In the next section, we will describe different sharding strategies, including
    their advantages and disadvantages.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述不同的分片策略，包括它们的优点和缺点。
- en: Range-based sharding
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于范围的分片
- en: The default and most widely used sharding strategy is range-based sharding.
    This strategy will split our collection's data into chunks, grouping documents
    with nearby values in the same shard.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认和最广泛使用的分片策略是基于范围的分片。这种策略将把我们集合的数据分成块，将具有相邻值的文档分组到同一个分片中。
- en: 'For our example database and collection, `mongo_books` and `books` respectively,
    we have the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据库和集合，分别是`mongo_books`和`books`，我们有以下内容：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This creates a range-based shard key on `id` with an ascending direction. The
    direction of our shard key will determine which documents will end up in the first
    shard and which ones in the subsequent ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`id`上创建一个基于范围的分片键，并且是升序的。我们的分片键的方向将决定哪些文档将最终出现在第一个分片中，哪些文档出现在随后的分片中。
- en: This is a good strategy if we plan to have range-based queries, as these will
    be directed to the shard that holds the result set instead of having to query
    all shards.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计划进行基于范围的查询，这是一个很好的策略，因为这些查询将被定向到保存结果集的分片，而不必查询所有分片。
- en: Hash-based sharding
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于哈希的分片
- en: If we don't have a shard key (or can't create one) that achieves the three goals
    mentioned previously, we can use the alternative strategy of using hash-based
    sharding. In this case, we are trading data distribution with query isolation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有一个达到前面提到的三个目标的分片键（或者无法创建一个），我们可以使用替代策略，即使用基于哈希的分片。在这种情况下，我们正在用数据分布来交换查询隔离。
- en: Hash-based sharding will take the values of our shard key and hash them in a
    way that guarantees close to uniform distribution. This way, we can be sure that
    our data will be evenly distributed across the shards. The downside is that only
    exact match queries will get routed to the exact shard that holds the value. Any
    range query will have to go out and fetch data from all the shards.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于哈希的分片将获取我们的分片键的值，并以一种接近均匀分布的方式进行哈希。这样，我们可以确保我们的数据将均匀分布在分片中。缺点是只有精确匹配查询将被路由到持有该值的确切分片。任何范围查询都必须从所有分片中获取数据。
- en: 'For our example database and collection (`mongo_books` and `books` respectively),
    we have the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据库和集合（分别是`mongo_books`和`books`），我们有以下内容：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Similar to the preceding example, we are now using the `id` field as our hashed
    shard key.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例类似，我们现在将`id`字段作为我们的哈希分片键。
- en: Suppose we use fields with float values for hash-based sharding. Then we will
    end up with collisions if the precision of our floats is more than *2^53*. These
    fields should be avoided where possible.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用浮点值字段进行基于哈希的分片。如果我们的浮点数的精度超过*2^53*，那么我们将会遇到碰撞。在可能的情况下，应该避免使用这些字段。
- en: Coming up with our own key
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提出我们自己的键
- en: Range-based sharding does not need to be confined to a single key. In fact,
    in most cases, we would like to combine multiple keys to achieve high cardinality
    and low frequency.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于范围的分片不需要局限于单个键。事实上，在大多数情况下，我们希望结合多个键来实现高基数和低频率。
- en: A common pattern is to combine a low-cardinality first part (but still with
    a number of distinct values more than two times the number of shards that we have)
    with a high-cardinality key as its second field. This achieves both read and write
    distribution from the first part of the sharding key and then cardinality and
    read-locality from the second part.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的模式是将低基数的第一部分（但仍具有两倍于我们拥有的分片数量的不同值的数量）与高基数键作为其第二字段组合。这既从分片键的第一部分实现了读取和写入分布，又从第二部分实现了基数和读取局部性。
- en: On the other hand, if we don't have range queries, then we can get away with
    using hash-based sharding on a primary key, as this will exactly target the shard
    and document that we are going after.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们没有范围查询，那么我们可以在主键上使用基于哈希的分片，因为这将精确地定位我们要查找的分片和文档。
- en: To make things more complicated, these considerations may change depending on
    our workload. A workload that consists almost exclusively (say 99.5%) of reads
    won't care about write distribution. We can use the built-in `_id` field as our
    shard key, and this will only add 0.5% load to the last shard. Our reads will
    still be distributed across shards. Unfortunately, in most cases, this is not
    simple.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使事情变得更加复杂的是，这些考虑因我们的工作负载而改变。几乎完全由读取（比如99.5%）组成的工作负载不会关心写入分布。我们可以使用内置的`_id`字段作为我们的分片键，这只会给最后一个分片增加0.5%的负载。我们的读取仍然会分布在各个分片上。不幸的是，在大多数情况下，情况并不简单。
- en: Location-based data
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于位置的数据
- en: Because of government regulations and the desire to have our data as close to
    our users as possible, there is often a constraint and need to limit data in a
    specific data center. By placing different shards at different data centers, we
    can satisfy this requirement.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于政府法规和希望将数据尽可能靠近用户，通常存在对特定数据中心中的数据进行限制和需要的约束。通过将不同的分片放置在不同的数据中心，我们可以满足这一要求。
- en: Every shard is essentially a replica set. We can connect to it as we would connect
    to a replica set for administrative and maintenance operations. We can query one
    shard's data directly, but the results will only be a subset of the full sharded
    result set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分片本质上都是一个副本集。我们可以像连接副本集一样连接到它进行管理和维护操作。我们可以直接查询一个分片的数据，但结果只会是完整分片结果集的子集。
- en: Sharding administration and monitoring
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片管理和监控
- en: Sharded MongoDB environments have some unique challenges and limitations compared
    to single-server or replica set deployments. In this section, we will explore
    how MongoDB balances our data across shards using chunks and how we can tweak
    them if we need to. Together, we will explore some of sharding's design limitations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与单服务器或副本集部署相比，分片的MongoDB环境具有一些独特的挑战和限制。在本节中，我们将探讨MongoDB如何使用chunks平衡我们的数据跨分片，并在需要时如何调整它们。我们将一起探讨一些分片设计的限制。
- en: Balancing data – how to track and keep our data balanced
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡数据-如何跟踪和保持我们的数据平衡
- en: One of the advantages of sharding in MongoDB is that it is mostly transparent
    to the application and requires minimal administration and operational effort.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中分片的一个优点是，它对应用程序基本上是透明的，并且需要最少的管理和运营工作。
- en: One of the core tasks that MongoDB needs to perform continuously is balancing
    data between shards. No matter whether we implement range-based or hash-based
    sharding, MongoDB will need to calculate bounds for the hashed field to be able
    to figure out which shard to direct every new document insert or update toward.
    As our data grows, these bounds may need to get readjusted to avoid having a hot
    shard that ends up with the majority of our data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB需要不断执行的核心任务之一是在分片之间平衡数据。无论我们实现基于范围还是基于哈希的分片，MongoDB都需要计算哈希字段的边界，以便确定将每个新文档插入或更新到哪个分片。随着数据的增长，这些边界可能需要重新调整，以避免出现大部分数据都集中在一个热分片上的情况。
- en: 'For the sake of this example, let''s assume that there is a data type named
    `extra_tiny_int` with integer values from [`-12`, `12`). If we enable sharding
    on this `extra_tiny_int` field, then the initial bounds of our data will be the
    whole range of values denoted by `$minKey: -12` and `$maxKey: 11`.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '为了举例说明，假设有一种名为`extra_tiny_int`的数据类型，其整数值范围为[`-12`, `12`)。如果我们在这个`extra_tiny_int`字段上启用分片，那么我们数据的初始边界将是由`$minKey:
    -12`和`$maxKey: 11`表示的整个值范围。 '
- en: After we insert some initial data, MongoDB will generate chunks and recalculate
    the bounds of each chunk to try and balance our data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们插入一些初始数据后，MongoDB将生成chunks并重新计算每个chunk的边界，以尝试平衡我们的数据。
- en: By default, the initial number of chunks created by MongoDB is *2 × number of
    shards*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，MongoDB创建的初始chunk数量是*2 × 分片数量*。
- en: 'In our case of two shards and four initial chunks, the initial bounds will
    be calculated as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，有两个分片和四个初始chunk，初始边界将如下计算：
- en: '*Chunk1: [-12..-6)*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chunk1: [-12..-6)*'
- en: '*Chunk2:  [-6..0)*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chunk2:  [-6..0)*'
- en: '*Chunk3:  [0..6)*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chunk3:  [0..6)*'
- en: '*Chunk4:  [6,12)* where *''[''* is inclusive and *'')''* is not inclusive'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chunk4:  [6,12)*其中*''[''*是包含的，*'')''*是不包含的'
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了前面的解释：
- en: '![](img/75712448-e2de-4265-a8ce-458548db4503.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75712448-e2de-4265-a8ce-458548db4503.png)'
- en: 'After we insert some data, our chunks will look as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们插入一些数据后，我们的chunks将如下所示：
- en: '*ShardA*:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ShardA*：'
- en: '*Chunk1*:* -12,-8,-7*'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chunk1*:* -12,-8,-7*'
- en: '*Chunk2*:*  -6*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chunk2*:*  -6*'
- en: '*ShardB*:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ShardB*:'
- en: '*Chunk3*:* 0, 2      *'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chunk3*:* 0, 2      *'
- en: '*Chunk4*: *7,8,9,10,11,11,11,11*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chunk4*: *7,8,9,10,11,11,11,11*'
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了前面的解释：
- en: '![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)'
- en: In this case, we observe that c*hunk4* has more items than any other chunk.
    MongoDB will first split *chunk4* into two new chunks, attempting to keep the
    size of each chunk under a certain threshold (64 MB, by default).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们观察到c*hunk4*的项比任何其他chunk都多。MongoDB将首先将*chunk4*分成两个新的chunk，试图保持每个chunk的大小在一定的阈值以下（默认为64
    MB）。
- en: Now, instead of c*hunk4*, we have *chunk4A*:*7,8,9,10* and *chunk4B*: *11,11,11,11*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有*chunk4A*：*7,8,9,10*和*chunk4B*：*11,11,11,11*，而不是c*hunk4*。
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了先前的解释：
- en: '![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)'
- en: 'The new bounds of this are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其新边界如下：
- en: '*chunk4A*: *[6,11)*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk4A*: *[6,11)*'
- en: '*chunk4B*: *[11,12)*'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk4B*: *[11,12)*'
- en: Note that *chunk4B* can only hold one value. This is now an indivisible chunk—a
    chunk that cannot be broken down into smaller ones anymore—and will grow in size
    unbounded, causing potential performance issues down the line.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*chunk4B*只能容纳一个值。这现在是一个不可分割的分片，无法再分割成更小的分片，并且将无限增长，可能会导致性能问题。
- en: This clarifies why we need to use a high-cardinality field as our shard key
    and why something like a Boolean, which only has `true`/`false` values, is a bad
    choice for a shard key.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么我们需要使用高基数字段作为我们的分片键，以及为什么像布尔值这样只有`true`/`false`值的字段是分片键的不良选择。
- en: 'In our case, we now have two chunks in *ShardA* and three chunks in *ShardB*.
    Let''s look at the following table:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，*ShardA*现在有两个分片，*ShardB*有三个分片。让我们看看下表：
- en: '| **Number of chunks** | **Migration threshold** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **分片数量** | **迁移阈值** |'
- en: '| *≤19* | *2* |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| *≤19* | *2* |'
- en: '| *20-79* | *4* |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| *20-79* | *4* |'
- en: '| *≥80* | *8* |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *≥80* | *8* |'
- en: We have not reached our migration threshold yet, since *3-2 = 1*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有达到迁移阈值，因为*3-2 = 1*。
- en: 'The migration threshold is calculated as the number of chunks in the shard
    with the highest count of chunks and the number of chunks in the shard with the
    lowest count of chunks, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移阈值是根据拥有最多分片的分片和拥有最少分片的分片数量计算得出的，如下所示：
- en: '*Shard1 -> 85 chunks*'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard1 -> 85 chunks*'
- en: '*Shard2 -> 86 chunks*'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard2 -> 86 chunks*'
- en: '*Shard3 -> 92 chunks*'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard3 -> 92 chunks*'
- en: In the preceding example, balancing will not occur until *Shard3* (or *Shard2*)
    reaches *93* chunks because the migration threshold is *8* for *≥80* chunks and
    the difference between *Shard1* and *Shard3* is still *7* chunks (*92-85*).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，直到*Shard3*（或*Shard2*）达到*93*个分片之前，平衡都不会发生，因为迁移阈值对于*≥80*个分片是*8*，而*Shard1*和*Shard3*之间的差距仍然是*7*个分片（*92-85*）。
- en: If we continue adding data in *chunk4A*, it will eventually be split into *chunk4A1*
    and *chunk4A2*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续在*chunk4A*中添加数据，它最终将被分割成*chunk4A1*和*chunk4A2*。
- en: Now we have four chunks in *ShardB* (*chunk3*, *chunk4A1*, *chunk4A2*, and *chunk4B*)
    and two chunks in *ShardA* (*chunk1* and *chunk2*).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在*ShardB*有四个分片（*chunk3*，*chunk4A1*，*chunk4A2*和*chunk4B*），*ShardA*有两个分片（*chunk1*和*chunk2*）。
- en: 'The following diagram illustrates the relationships of the chunks to the shards:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了分片与分片之间的关系：
- en: '![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)'
- en: 'The MongoDB balancer will now migrate one chunk from *ShardB* to *ShardA* as
    *4-2 = 2*, reaching the migration threshold for fewer than *20* chunks. The balancer
    will adjust the boundaries between the two shards in order to be able to query
    more effectively (targeted queries). The following diagram illustrates the preceding
    explanation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB平衡器现在将从*ShardB*迁移一个分片到*ShardA*，因为*4-2 = 2*，达到了少于*20*个分片的迁移阈值。平衡器将调整两个分片之间的边界，以便能够更有效地查询（有针对性的查询）。以下图表说明了先前的解释：
- en: '![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)'
- en: As you can see from the preceding diagram, MongoDB will try to split *>64* MB
    chunks in half in terms of size. The bounds between the two resulting chunks may
    be completely uneven if our data distribution is uneven to begin with. MongoDB
    can split chunks into smaller ones, but cannot merge them automatically. We need
    to manually merge chunks, a delicate and operationally expensive procedure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图表中可以看出，MongoDB将尝试将*>64* MB的分片一分为二。如果我们的数据分布不均匀，那么两个结果分片之间的边界可能会完全不均匀。MongoDB可以将分片分割成更小的分片，但不能自动合并它们。我们需要手动合并分片，这是一个复杂且操作成本高昂的过程。
- en: Chunk administration
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片管理
- en: Most of the time, we should leave chunk administration to MongoDB. We should
    manually manage chunks at the start, upon receiving the initial load of data,
    when we change our configuration from a replica set to sharding.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们应该让MongoDB来管理分片。我们应该在开始时手动管理分片，在接收到初始数据负载时，当我们将配置从副本集更改为分片时。
- en: Moving chunks
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移动分片
- en: 'To move a chunk manually, we need to issue the following command after connecting
    to `mongos` and the `admin` database:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动移动一个分片，我们需要在连接到`mongos`和`admin`数据库后发出以下命令：
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using the preceding command, we move the chunk containing the document with
    `id: 50` (this has to be the shard key) from the `books` collection of the `mongo_books` database to
    the new shard named `shard1.packtdb.com`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '使用上述命令，我们将包含`id: 50`的文档（这必须是分片键）从`mongo_books`数据库的`books`集合移动到名为`shard1.packtdb.com`的新分片。'
- en: 'We can also more explicitly define the bounds of the chunk that we want to
    move. Now the syntax is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更明确地定义我们要移动的分片的边界。现在的语法如下：
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `minValue` and `maxValue` are the values that we get from `db.printShardingStatus()`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`minValue`和`maxValue`是我们从`db.printShardingStatus()`中获取的值。
- en: In the example used previously, for *chunk2*, `minValue` would be `-6` and `maxValue`
    would be `0`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的示例中，对于*chunk2*，`minValue`将是`-6`，`maxValue`将是`0`。
- en: Do not use `find` in hash-based sharding. Use `bounds` instead.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于哈希的分片中不要使用`find`。使用`bounds`代替。
- en: Changing the default chunk size
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改默认的分片大小
- en: To change the default chunk size, we need to connect to a `mongos` router and,
    consequently, to the `config` database.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改默认的分片大小，我们需要连接到`mongos`路由器，因此连接到`config`数据库。
- en: 'Then we issue the following command to change our global `chunksize` to `16`
    MB:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们发出以下命令将我们的全局`chunksize`更改为`16` MB：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The main reasoning behind changing `chunksize` comes from cases where the default
    `chunksize` of 64 MB can cause more I/O than our hardware can handle. In this
    case, defining a smaller `chunksize` will result in more frequent but less data-intensive
    migrations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更改`chunksize`的主要原因来自于默认的64 MB `chunksize`可能会导致比我们的硬件处理能力更多的I/O。在这种情况下，定义较小的`chunksize`将导致更频繁但数据密度较小的迁移。
- en: 'Changing the default chunk size has the following drawbacks:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 更改默认块大小有以下缺点：
- en: Creating more splits by defining a smaller chunk size cannot be undone automatically.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过定义较小的块大小创建更多的拆分无法自动撤消。
- en: Increasing the chunk size will not force any chunk migration; instead, chunks
    will grow through inserts and updates until they reach the new size.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加块大小不会强制进行任何块迁移；相反，块将通过插入和更新而增长，直到达到新的大小。
- en: Lowering the chunk size may take quite some time to complete.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低块大小可能需要相当长的时间才能完成。
- en: Automatic splitting to comply with the new chunk size if it is lower will only
    happen upon an insert or update. We may have chunks that don't get any write operations,
    and thus will not be changed in size.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果较低的块大小需要遵守新的块大小，那么只有在插入或更新时才会自动拆分。我们可能有一些块不会进行任何写操作，因此大小不会改变。
- en: The chunk size can be 1 to 1024 MB.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小可以为1到1024 MB。
- en: Jumbo chunks
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 巨型块
- en: In rare cases, we may end up with jumbo chunks, chunks that are larger than
    the chunk size and cannot be split by MongoDB. We may also run into the same situation
    if the number of documents in our chunk exceeds the maximum document limit.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在罕见情况下，我们可能会遇到巨型块，即大于块大小且无法由MongoDB拆分的块。如果我们的块中的文档数量超过最大文档限制，也可能遇到相同的情况。
- en: These chunks will have the `jumbo` flag enabled. Ideally, MongoDB will keep
    track of whether it can split the chunk, and, as soon as it can, it will get split;
    however, we may decide that we want to manually trigger the split before MongoDB
    does.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块将启用`jumbo`标志。理想情况下，MongoDB将跟踪它是否可以拆分块，并且一旦可以，它将被拆分；但是，我们可能决定在MongoDB之前手动触发拆分。
- en: 'The way to do this is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的方法如下：
- en: 'Connect via shell to your `mongos` router and run the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过shell连接到您的`mongos`路由器并运行以下命令：
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Identify the chunk that has `jumbo` in its description using the following
    code:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码标识具有`jumbo`的块：
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Invoke `splitAt()` or `splitFind()` manually to split the chunk on the `books` collection of
    the `mongo_books` database at the `id` that is equal to `8` using the following
    code:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`splitAt()`或`splitFind()`手动在`mongo_books`数据库的`books`集合上拆分`id`等于`8`的块，使用以下代码：
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `splitAt()` function will split based on the split point we define. The
    two new splits may or may not be balanced.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`splitAt()`函数将根据我们定义的拆分点进行拆分。两个新的拆分可能平衡也可能不平衡。'
- en: 'Alternatively, if we want to leave it to MongoDB to find where to split our
    chunk, we can use `splitFind`, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们想让MongoDB找到拆分块的位置，我们可以使用`splitFind`，如下所示：
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `splitFind` phrase will try to find the chunk that the `id:7` query belongs
    to and automatically define the new bounds for the split chunks so that they are
    roughly balanced.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`splitFind`短语将尝试找到`id:7`查询所属的块，并自动定义拆分块的新边界，使它们大致平衡。'
- en: In both cases, MongoDB will try to split the chunk, and if successful, it will
    remove the `jumbo` flag from it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，MongoDB将尝试拆分块，如果成功，它将从中删除`jumbo`标志。
- en: 'If the preceding operation is unsuccessful, then, and only then, should we try
    stopping the balancer first, while also verifying the output and waiting for any
    pending migrations to finish first, as shown in the following code:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果前面的操作不成功，那么只有在这种情况下，我们应该首先尝试停止平衡器，同时验证输出并等待任何待处理的迁移完成，如下所示：
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This should return `false `
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回`false`
- en: Wait for any `waiting…` messages to stop printing, and then find the `jumbo`
    flagged chunk in the same way as before.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待任何`waiting…`消息停止打印，然后以与之前相同的方式找到带有`jumbo`标志的块。
- en: 'Then update the `chunks` collection in your `config` database of the `mongos`
    router, like this:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后在`mongos`路由器的`config`数据库中更新`chunks`集合，如下所示：
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding command is a regular `update()` command, with the first argument
    being the `find()` part to find out which document to update and the second argument
    being the operation to apply to it (`$unset: jumbo flag`).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的命令是一个常规的`update()`命令，第一个参数是`find()`部分，用于查找要更新的文档，第二个参数是要应用于它的操作（`$unset:
    jumbo flag`）。'
- en: 'After all this is done, we re-enable the balancer, as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成所有这些操作后，我们重新启用平衡器，如下所示：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we connect to the `admin` database to flush the new configuration to
    all nodes, as follows:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们连接到`admin`数据库，将新配置刷新到所有节点，如下所示：
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Always back up the `config` database before modifying any state manually.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动修改任何状态之前，始终备份`config`数据库。
- en: Merging chunks
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并块
- en: As we have seen previously, usually MongoDB will adjust the bounds for each
    chunk in our shard to make sure that our data is equally distributed. This may
    not work in some cases—especially when we define the chunks manually—if our data
    distribution is surprisingly unbalanced, or if we have many `delete` operations
    in our shard.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，通常情况下，MongoDB将调整每个分片的块边界，以确保我们的数据均匀分布。在某些情况下，这可能不起作用，特别是当我们手动定义块时，如果我们的数据分布出奇地不平衡，或者我们的分片中有许多`delete`操作。
- en: Having empty chunks will invoke unnecessary chunk migrations and give MongoDB
    a false impression of which chunk needs to be migrated. As we have explained before,
    the threshold for chunk migration is dependent on the number of chunks that each
    shard holds. Having empty chunks may or may not trigger the balancer when it's
    needed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有空块将引发不必要的块迁移，并使MongoDB对需要迁移的块产生错误印象。正如我们之前解释的那样，块迁移的阈值取决于每个分片持有的块的数量。拥有空块可能会触发平衡器，也可能不会在需要时触发平衡器。
- en: Chunk merging can only happen when at least one of the chunks is empty, and
    only between adjacent chunks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当至少有一个块为空时，块合并才会发生，并且只会发生在相邻块之间。
- en: 'To find empty chunks, we need to connect to the database that we want to inspect
    (in our case, `mongo_books`) and use `runCommand` with `dataSize` set as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到空块，我们需要连接到要检查的数据库（在我们的情况下是`mongo_books`），并使用`runCommand`，设置`dataSize`如下：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `dataSize` phrase follows the `database_name.collection_name` pattern, whereas
    `keyPattern` is the shard key that we have defined for this collection.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataSize`短语遵循`database_name.collection_name`模式，而`keyPattern`是我们为这个集合定义的分片键。'
- en: The `min` and `max` values should be calculated by the chunks that we have in
    this collection. In our case, we have entered *chunkB's* details from the example
    earlier in this chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`min`和`max`值应该由我们在这个集合中拥有的数据块计算得出。在我们的情况下，我们已经在本章前面的示例中输入了*chunkB*的详细信息。'
- en: 'If the bounds of our query (which, in our case, are the bounds of *chunkB*)
    return no documents, the result will resemble the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的查询边界（在我们的情况下是*chunkB*的边界）没有返回任何文档，结果将类似于以下内容：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we know that *chunkB* has no data, we can merge it with another chunk
    (in our case, this could only be *chunkA*) like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道*chunkB*没有数据，我们可以像这样将它与另一个数据块（在我们的情况下，只能是*chunkA*）合并：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On a success, this will return MongoDB''s default `ok` status message, as shown
    in the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 成功后，这将返回MongoDB的默认`ok`状态消息，如下所示：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can then verify that we only have one chunk on *ShardA* by invoking `sh.status()` again.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过再次调用`sh.status()`来验证*ShardA*上只有一个数据块。
- en: Adding and removing shards
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加和移除分片
- en: 'Adding a new shard to our cluster is as easy as connecting to `mongos`, connecting
    to the `admin` database, and invoking `runCommand` with the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 向我们的集群添加一个新的分片就像连接到`mongos`，连接到`admin`数据库，并使用以下命令调用`runCommand`一样简单：
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This adds a new shard from the replica set named `mongo_books_replica_set` from
    the `rs01.packtdb.com` host running on port `27017`. We also define the `maxSize`
    of data for this shard as `18000` MB (or we can set it to `0` to give it no limit)
    and the name of the new shard as `packt_mongo_shard_UK`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从`rs01.packtdb.com`主机的端口`27017`上运行的`mongo_books_replica_set`复制集中添加一个新的分片。我们还将为这个分片定义数据的`maxSize`为`18000`
    MB（或者我们可以将其设置为`0`以不设限），新分片的名称为`packt_mongo_shard_UK`。
- en: This operation will take quite some time to complete as chunks will have to
    be rebalanced and migrated to the new shard.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作将需要相当长的时间来完成，因为数据块将需要重新平衡和迁移到新的分片。
- en: 'Removing a shard, on the other hand, requires more involvement since we have
    to make sure that we won''t lose any data on the way. We do this as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，移除一个分片需要更多的参与，因为我们必须确保在这个过程中不会丢失任何数据。我们按照以下步骤进行：
- en: 'First, we need to make sure that the balancer is enabled using `sh.getBalancerState()`.
    Then, after identifying the shard we want to remove using any one of the `sh.status()`,
    `db.printShardingStatus()`, or `listShards admin` commands, we connect to the
    `admin` database and invoke `removeShard` as follows:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确保负载均衡器已启用，使用`sh.getBalancerState()`。然后，在使用`sh.status()`、`db.printShardingStatus()`或`listShards
    admin`命令中任何一个来识别我们想要移除的分片后，我们连接到`admin`数据库并按以下方式调用`removeShard`：
- en: '[PRE20]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output should contain the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该包含以下内容：
- en: '[PRE21]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, if we invoke the same command again, we get the following:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，如果我们再次调用相同的命令，我们会得到以下结果：
- en: '[PRE22]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The remaining document in the result contains the number of `chunks` and `dbs`
    that are still being transferred. In our case, it's `2` and `3` respectively.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中剩下的文档包含仍在传输的`chunks`和`dbs`的数量。在我们的情况下，分别是`2`和`3`。
- en: All the commands need to be executed in the `admin` database.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 所有命令都需要在`admin`数据库中执行。
- en: An extra complication in removing a shard can arise if the shard we want to
    remove serves as the primary shard for one or more of the databases that it contains.
    The primary shard is allocated by MongoDB when we initiate sharding, so when we
    remove the shard, we need to manually move these databases to a new shard.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 移除分片时可能会出现额外的复杂情况，如果我们要移除的分片作为它包含的一个或多个数据库的主分片。主分片是在我们启用分片时由MongoDB分配的，因此当我们移除分片时，我们需要手动将这些数据库移动到一个新的分片。
- en: 'We will know whether we need to perform this operation by looking at the following
    section of the result from `removeShard()`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过查看`removeShard()`结果中的以下部分来确定是否需要执行此操作：
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We need to drop or `movePrimary` our `mongo_books` database. The way to do this
    is to first make sure that we are connected to the `admin` database.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要删除或`movePrimary`我们的`mongo_books`数据库。首先要确保我们连接到`admin`数据库。
- en: We need to wait for all of the chunks to finish migrating before running this
    command.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此命令之前，我们需要等待所有数据块完成迁移。
- en: 'Make sure that the result contains the following before proceeding:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，请确保结果包含以下内容：
- en: '[PRE24]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Only after we have made sure that the chunks to be moved are down to zero can
    we safely run the following command:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有在我们确保要移动的数据块已经减少到零后，我们才能安全地运行以下命令：
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This command will invoke a blocking operation, and, when it returns, it should
    have the following result:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个命令将调用一个阻塞操作，当它返回时，应该有以下结果：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Invoking the same `removeShard()` command after we are all done should return
    the following result:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们完成所有操作后再次调用相同的`removeShard()`命令应该返回以下结果：
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once we get to `state` as `completed` and `ok` as `1`, it is safe to remove
    our `packt_mongo_shard_UK` shard.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`state`为`completed`，`ok`为`1`，就可以安全地移除我们的`packt_mongo_shard_UK`分片。
- en: Removing a shard is naturally more complicated than adding one. We need to allow
    some time, hope for the best, and plan for the worst when performing potentially
    destructive operations on our live cluster.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 移除分片自然比添加分片更复杂。在对我们的实时集群执行潜在破坏性操作时，我们需要留出一些时间，希望一切顺利，并为最坏的情况做好准备。
- en: Sharding limitations
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片限制
- en: Sharding comes with great flexibility. Unfortunately, there are a few limitations
    in the way that we perform some of the operations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 分片提供了很大的灵活性。不幸的是，在执行一些操作的方式上存在一些限制。
- en: 'We will highlight the most important ones in the following list:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下列表中突出显示最重要的部分：
- en: The `group()` database command does not work. The `group()` command should not
    be used anyway; use `aggregate()` and the aggregation framework instead, or `mapreduce()`.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group()`数据库命令不起作用。无论如何都不应该使用`group()`命令；而是使用`aggregate()`和聚合框架，或者`mapreduce()`。'
- en: The `db.eval()` command does not work and should be disabled in most cases for
    security reasons.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`db.eval()`命令不起作用，出于安全原因，在大多数情况下应禁用。'
- en: The `$isolated` option for updates does not work. This is a functionality that
    is missing in sharded environments. The `$isolated` option for `update()` provides
    the guarantee that, if we update multiple documents at once, other readers and
    writers will not see some of the documents updated with the new value, and the
    others will still have the old value. The way this is implemented in unsharded
    environments is by holding a global write lock and/or serializing operations to
    a single thread to make sure that every request for the documents affected by
    `update()` will not be accessed by other threads/operations. This implementation
    means that it is not performant and does not support any concurrency, which makes
    it prohibitive to allow the `$isolated` operator in a sharded environment.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新的`$isolated`选项不起作用。这是分片环境中缺少的功能。`update()`的`$isolated`选项提供了保证，即如果我们一次更新多个文档，其他读者和写入者将不会看到一些文档被更新为新值，而其他文档仍将保留旧值。在非分片环境中实现这一点的方式是通过保持全局写锁和/或将操作序列化到单个线程，以确保`update()`受影响的文档的每个请求不会被其他线程/操作访问。这种实现意味着它不具备性能，并且不支持任何并发，这使得在分片环境中允许`$isolated`运算符成本过高。
- en: The `$snapshot` operator for queries is not supported. The `$snapshot` operator in
    the `find()` cursor prevents documents from appearing more than once in the results,
    as a result of being moved to a different location on the disk after an update.
    The `$snapshot` operator is operationally expensive and often not a hard requirement.
    The way to substitute it is by using an index for our queries on a field whose
    keys will not change for the duration of the query.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持查询的`$snapshot`运算符。`find()`游标中的`$snapshot`运算符防止文档在更新后由于移动到磁盘上的不同位置而出现多次在结果中。`$snapshot`运算符在操作上是昂贵的，通常不是硬性要求。替代它的方法是在查询中使用一个字段的索引，这个字段的键在查询期间不会改变。
- en: The indexes cannot cover our queries if our queries do not contain the shard
    key. Results in sharded environments will come from the disk and not exclusively
    from the index. The only exception is if we query only on the built-in `_id` field
    and return only the `_id` field, in which case, MongoDB can still cover the query
    using built-in indexes.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的查询不包含分片键，索引将无法覆盖我们的查询。在分片环境中，结果将来自磁盘，而不仅仅来自索引。唯一的例外是如果我们仅在内置的`_id`字段上进行查询，并且仅返回`_id`字段，那么MongoDB仍然可以使用内置索引来覆盖查询。
- en: The `update()` and `remove()` operations work differently. All `update()` and
    `remove()` operations in a sharded environment must include either the `_id` of
    the documents that are to be affected or the shard key; otherwise, the `mongos`
    router will have to do a full table scan across all collections, databases, and
    shards, which would be operationally very expensive.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update()`和`remove()`操作的工作方式不同。在分片环境中，所有`update()`和`remove()`操作必须包括要受影响的文档的`_id`或分片键；否则，`mongos`路由器将不得不在所有集合、数据库和分片上进行全表扫描，这在操作上将非常昂贵。'
- en: Unique indexes across shards need to contain the shard key as a prefix of the
    index. In other words, to achieve uniqueness of documents across shards, we need
    to follow the data distribution that MongoDB follows for the shards.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨分片的唯一索引需要包含分片键作为索引的前缀。换句话说，为了实现跨分片的文档唯一性，我们需要遵循MongoDB为分片所遵循的数据分布。
- en: The shard key has to be up to 512 bytes in size. The shard key index has to
    be in ascending order on the key field that gets sharded and optionally other
    fields as well, or a hashed index on it.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片键的大小必须达到512字节。分片键索引必须按照分片的关键字段以及可选的其他字段的升序排列，或者对其进行哈希索引。
- en: The shard key value in a document is also immutable. If our shard key for our
    `User` collection is `email`, then we cannot update the `email` value for any
    user after we set it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的分片键值也是不可变的。如果我们的`User`集合的分片键是`email`，那么在设置后我们就不能更新任何用户的`email`值。
- en: Querying sharded data
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询分片数据
- en: Querying our data using a MongoDB shard is different than a single-server deployment
    or a replica set. Instead of connecting to the single server or the primary of
    the replica set, we connect to the `mongos` router that decides which shard to
    ask for our data. In this section, we will explore how the query router operates
    and use Ruby to illustrate how similar to a replica set this is for the developer
    .
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MongoDB分片查询我们的数据与单服务器部署或副本集不同。我们不是连接到单个服务器或副本集的主服务器，而是连接到决定要请求我们的数据的分片的`mongos`路由器。在本节中，我们将探讨查询路由器的操作方式，并使用Ruby来说明对开发人员来说这与副本集有多相似。
- en: The query router
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询路由器
- en: The query router, also known as the `mongos` process, acts as the interface
    and entry point to our MongoDB cluster. Applications connect to it instead of
    connecting to the underlying shards and replica sets; `mongos` executes queries,
    gathers results and passes them to our application.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 查询路由器，也称为`mongos`进程，充当我们MongoDB集群的接口和入口。应用程序连接到它，而不是连接到底层的分片和副本集；`mongos`执行查询，收集结果并将其传递给我们的应用程序。
- en: The `mongos` process doesn't hold any persistent state and is typically low
    on system resources.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongos`进程不持有任何持久状态，通常对系统资源要求较低。'
- en: The `mongos` process is typically hosted in the same instance as the application
    server.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongos`进程通常托管在与应用程序服务器相同的实例中。'
- en: It acts as a proxy for requests. When a query comes in, `mongos` will examine
    and decide which shards need to execute the query and establish a cursor in each
    one of them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 它充当请求的代理。当查询进来时，`mongos`将检查并决定哪些分片需要执行查询，并在每个分片中建立一个游标。
- en: Find
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找
- en: If our query includes the shard key or a prefix of the shard key, `mongos` will
    perform a targeted operation, only querying the shards that hold the keys that
    we are looking for.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的查询包括分片键或分片键的前缀，`mongos`将执行有针对性的操作，只查询持有我们寻找的键的分片。
- en: 'For example, with a composite shard key of `{_id, email, address}` on our `User `collection,
    we can have a targeted operation with any of the following queries:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的`User`集合上使用`{ _id，email，address }`的复合分片键，我们可以使用以下任何查询进行有针对性的操作：
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These queries consist of either a prefix (as is the case with the first two)
    or the complete shard key.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些查询由前缀（与前两个情况相同）或完整的分片键组成。
- en: On the other hand, a query on `{email, address}` or `{address}` will not be
    able to target the right shards, resulting in a broadcast operation. A broadcast
    operation is any operation that doesn't include the shard key or a prefix of the
    shard key, and they result in `mongos` querying every shard and gathering results
    from them. They are also known as **scatter-and-gather operations** or **fan out
    queries**.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对`{email，address}`或`{address}`的查询将无法定位正确的分片，导致广播操作。广播操作是不包括分片键或分片键前缀的任何操作，并且它们导致`mongos`查询每个分片并从中收集结果。它们也被称为**分散和聚集操作**或**扇出查询**。
- en: This behavior is a direct result of the way indexes are organized, and is similar
    to the behavior that we identified in the chapter about indexing.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为是索引组织方式的直接结果，并且类似于我们在索引章节中确定的行为。
- en: Sort/limit/skip
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排序/限制/跳过
- en: 'If we want to sort our results, we have the following two options:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对结果进行排序，我们有以下两个选项：
- en: If we are using the shard key in our sort criteria, then `mongos` can determine
    the order in which it has to query the shard or shards. This results in an efficient
    and, again, targeted operation.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在排序标准中使用分片键，那么`mongos`可以确定查询分片的顺序。这将导致高效且再次是有针对性的操作。
- en: If we are not using the shard key in our sort criteria, then, as is the case
    with a query without any sort criteria, it's going to be a fan out query. To sort
    the results when we are not using the shard key, the primary shard executes a
    distributed merge sort locally before passing on the sorted result set to `mongos`.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在排序标准中不使用分片键，那么与没有任何排序标准的查询一样，它将成为一个扇出查询。在不使用分片键时对结果进行排序时，主分片在将排序结果集传递给`mongos`之前在本地执行分布式合并排序。
- en: A limit on the queries is enforced on each individual shard and then again at
    the `mongos` level, as there may be results from multiple shards. A `skip` operator,
    on the other hand, cannot be passed on to individual shards, and will be applied
    by `mongos` after retrieving all the results locally.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个单独的分片强制执行查询的限制，然后再次在`mongos`级别进行，因为可能来自多个分片的结果。另一方面，`skip`操作符无法传递给各个分片，并且在本地检索所有结果后由`mongos`应用。
- en: If we combine the `skip` and `limit` operators, `mongos` will optimize the query
    by passing both values to individual shards. This is particularly useful in cases
    such as pagination. If we query without `sort` and the results are coming from
    more than one shard, `mongos` will round robin across shards for the results.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们结合`skip`和`limit`操作符，`mongos`将通过将两个值传递给各个分片来优化查询。这在分页等情况下特别有用。如果我们在没有`sort`的情况下查询，而结果来自多个分片，`mongos`将在分片之间进行轮询以获取结果。
- en: Update/remove
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新/删除
- en: In document modifier operations, such as `update` and `remove`, we have a similar
    situation to the one we saw with `find`. If we have the shard key in the `find`
    section of the modifier, then `mongos` can direct the query to the relevant shard.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档修改操作中，例如`update`和`remove`，我们与`find`中看到的情况类似。如果我们在修改器的`find`部分中有分片键，那么`mongos`可以将查询定向到相关的分片。
- en: If we don't have the shard key in the `find` section, then it will again be
    a fanout operation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在`find`部分没有分片键，那么它将再次成为一个扇出操作。
- en: The `UpdateOne`, `replaceOne`, and `removeOne` operations must have the shard
    key or the `_id` value.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`UpdateOne`，`replaceOne`和`removeOne`操作必须具有分片键或`_id`值。'
- en: 'The following table sums up the operations that we can use with sharding:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表总结了我们可以在分片中使用的操作：
- en: '| **Type of operation** | **Query topology** |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|**操作类型**|**查询拓扑**|'
- en: '| Insert | Must have the shard key |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|插入|必须具有分片键|'
- en: '| Update | Can have the shard key |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|更新|可以具有分片键|'
- en: '| Query with shard key | Targeted operation |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|具有分片键的查询|目标操作|'
- en: '| Query without shard key | Scatter-and-gather operation/fan out query |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|没有分片键的查询|分散和聚集操作/扇出查询|'
- en: '| Indexed, sorted query with shard key | Targeted operation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|具有分片键的索引、排序查询|目标操作|'
- en: '| Indexed, sorted query without shard key | Distributed sort merge |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|具有分片键的索引、排序查询而没有分片键|分布式排序合并|'
- en: Querying using Ruby
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ruby进行查询
- en: 'Connecting to a sharded cluster using Ruby is no different than connecting
    to a replica set. Using the official Ruby driver, we have to configure the `client`
    object to define the set of `mongos` servers, as shown in the following code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ruby连接到分片集群与连接到副本集没有区别。使用官方的Ruby驱动程序，我们必须配置`client`对象以定义一组`mongos`服务器，如下面的代码所示：
- en: '[PRE29]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `mongo-ruby-driver` will then return a `client` object, which is no different
    than connecting to a replica set from the Mongo Ruby client. We can then use the
    `client` object as we did in previous chapters, with all the caveats around how
    sharding behaves differently than a standalone server or a replica set with regards
    to querying and performance.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后`mongo-ruby-driver`将返回一个`client`对象，这与从Mongo Ruby客户端连接到副本集没有区别。然后我们可以像在之前的章节中一样使用`client`对象，包括关于分片行为与独立服务器或具有查询和性能方面的副本集不同的所有注意事项。
- en: Performance comparison with replica sets
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与副本集的性能比较
- en: Developers and architects are always looking out for ways to compare performance
    between replica sets and sharded configurations.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员和架构师总是在寻找比较副本集和分片配置性能的方法。
- en: The way MongoDB implements sharding is based on top of replica sets. Every shard
    in production should be a replica set. The main difference in performance comes
    from fan out queries. When we are querying without the shard key, MongoDB's execution
    time is limited by the worst performing replica set. In addition, when using sorting
    without the shard key, the primary server has to implement the distributed merge
    sort on the entire dataset. This means that it has to collect all data from different
    shards, merge sort them, and pass them as sorted to `mongos`. In both cases, network
    latency and limitations in bandwidth can slow down operations, which they wouldn't
    do with a replica set.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB实现分片的方式是基于副本集。生产中的每个分片都应该是一个副本集。性能上的主要区别来自于扇出查询。当我们在没有分片键的情况下进行查询时，MongoDB的执行时间受到最差表现的副本集的限制。此外，当使用没有分片键的排序时，主服务器必须在整个数据集上实现分布式归并排序。这意味着它必须收集来自不同分片的所有数据，对它们进行归并排序，并将它们作为排序后的数据传递给`mongos`。在这两种情况下，网络延迟和带宽限制可能会减慢操作的速度，而在副本集中则不会出现这种情况。
- en: On the flip side, by having three shards, we can distribute our working set
    requirements across different nodes, thereby serving results from RAM instead
    of reaching out to the underlying storage, HDD or SSD.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过拥有三个分片，我们可以将工作集需求分布在不同的节点上，从而从RAM中提供结果，而不是访问底层存储，HDD或SSD。
- en: On the other hand, writes can be sped up significantly since we are no longer
    bound by a single node's I/O capacity, and we can have writes in as many nodes
    as there are shards. Summing up, in most cases, and especially for the cases where
    we are using the shard key, both queries and modification operations will be significantly
    sped up by sharding.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，写操作可以显著加快，因为我们不再受限于单个节点的I/O容量，而且我们可以在所有分片中进行写操作。总而言之，在大多数情况下，特别是在使用分片键的情况下，查询和修改操作都将因分片而显著加快。
- en: The shard key is the single most important decision in sharding, and should
    reflect and apply to our most common application use cases.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 分片键是分片中最重要的决定，并且应反映和应用于我们最常见的应用程序用例。
- en: Sharding recovery
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片恢复
- en: In this section, we will explore different failure types and how we can recover
    in a sharded environment. Failure in a distributed system can take multiple forms
    and shapes. In this section we will cover all the possible cases, from the simplest
    case of a stateless component like `mongos` failing to an entire shard going down.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨不同的故障类型以及在分片环境中如何进行恢复。在分布式系统中，故障可能以多种形式出现。在本节中，我们将涵盖所有可能的情况，从像`mongos`这样的无状态组件失败到整个分片宕机的最简单情况。
- en: mongos
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mongos
- en: The `mongos` process is a relatively lightweight process that holds no state.
    In the case that the process fails, we can just restart it or spin up a new process
    in a different server. It's recommended that `mongos` processes are located in
    the same server as our application, and so it makes sense to connect from our
    application using the set of `mongos` servers that we have colocated in our application
    servers to ensure a high availability of `mongos` processes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongos`进程是一个相对轻量级的进程，不保存状态。如果该进程失败，我们只需重新启动它或在不同的服务器上启动一个新进程。建议将`mongos`进程放置在与我们的应用程序相同的服务器上，因此从我们的应用程序使用我们在应用程序服务器中共同放置的一组`mongos`服务器连接是有意义的，以确保`mongos`进程的高可用性。'
- en: mongod
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mongod
- en: A `mongod` process failing in a sharded environment is no different than it
    failing in a replica set. If it is a secondary, the primary and the other secondary
    (assuming three-node replica sets) will continue as usual.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在分片环境中，`mongod`进程失败与在副本集中失败没有区别。如果是一个secondary，primary和其他secondary（假设是三节点副本集）将继续正常运行。
- en: If it is a `mongod` process acting as a primary, then an election round will
    start to elect a new primary in this shard (which is really a replica set).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是一个`mongod`进程充当primary，那么选举将开始选举该分片（实际上是一个副本集）中的新primary。
- en: In both cases, we should actively monitor and try to repair the node as soon
    as possible, as our availability can be impacted.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们应该积极监视并尽快修复节点，因为我们的可用性可能会受到影响。
- en: Config server
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置服务器
- en: Starting from MongoDB 3.4, config servers are also configured as a replica set.
    A config server failing is no different than a regular `mongod` process failing.
    We should monitor, log, and repair the process.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.4开始，配置服务器也被配置为副本集。配置服务器的故障与常规的`mongod`进程故障没有区别。我们应该监视、记录和修复该进程。
- en: A shard goes down
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个分片宕机
- en: Losing an entire shard is pretty rare, and in many cases can be attributed to
    network partitioning rather than failing processes. When a shard goes down, all
    operations that would go to this shard will fail. We can (and should) implement
    fault tolerance in our application level, allowing our application to resume for
    the operations that can be completed.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 失去整个分片是非常罕见的，在许多情况下可以归因于网络分区而不是失败的进程。当一个分片宕机时，所有会发送到该分片的操作都将失败。我们可以（而且应该）在应用程序级别实现容错，使我们的应用程序能够恢复完成的操作。
- en: Choosing a shard key that can easily map on our operational side can also help;
    for example, if our shard key is based on location, we may lose the EU shard,
    but will still be able to write and read data regarding US-based customers through
    our US shard.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个可以轻松映射到我们操作方面的分片键也可以帮助；例如，如果我们的分片键是基于位置的，我们可能会失去EU分片，但仍然能够通过我们的US分片写入和读取关于美国客户的数据。
- en: The entire cluster goes down
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整个集群宕机
- en: If we lose the entire cluster, we can't do anything other than get it back up
    and running as soon as possible. It's important to have monitoring, and to put
    a proper process in place to understand what needs to be done, when, and by whom,
    should this ever happen.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们失去整个集群，除了尽快恢复运行之外，我们无法做任何其他事情。重要的是要进行监视，并制定适当的流程，以了解如果发生这种情况，需要在何时以及由谁来完成。
- en: Recovering when the entire cluster goes down essentially involves restoring
    from backups and setting up new shards, which is complicated and will take time.
    Dry testing this in a staging environment is also advisable, as  is investing
    in regular backups via MongoDB Ops Manager or any other backup solution.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当整个集群崩溃时，恢复基本上涉及从备份中恢复并设置新的分片，这很复杂并且需要时间。在测试环境中进行干测试也是明智的选择，另外，通过MongoDB Ops
    Manager或任何其他备份解决方案进行定期备份也是明智的选择。
- en: A member of each shard's replica set could be in a different location for disaster-recovery
    purposes.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了灾难恢复目的，每个分片的副本集成员可能位于不同的位置。
- en: Further reading
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following sources are recommended for you to study sharding in depth:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 以下资源建议您深入学习分片：
- en: '*Scaling MongoDB* by Kristina Chodorow'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《扩展MongoDB》由Kristina Chodorow
- en: '*MongoDB: The Definitive Guide* by Kristina Chodorow and Michael Dirolf'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《MongoDB：权威指南》由Kristina Chodorow和Michael Dirolf编写
- en: '[https://docs.mongodb.com/manual/sharding/](https://docs.mongodb.com/manual/sharding/)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.mongodb.com/manual/sharding/](https://docs.mongodb.com/manual/sharding/)'
- en: '[https://www.mongodb.com/blog/post/mongodb-16-released](https://www.mongodb.com/blog/post/mongodb-16-released)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/blog/post/mongodb-16-released](https://www.mongodb.com/blog/post/mongodb-16-released)'
- en: '[https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)'
- en: '[https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)'
- en: '[http://plusnconsulting.com/post/mongodb-sharding-and-chunks/](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://plusnconsulting.com/post/mongodb-sharding-and-chunks/](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)'
- en: '[https://github.com/mongodb/mongo/wiki/Sharding-Internals](https://github.com/mongodb/mongo/wiki/Sharding-Internals)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/mongodb/mongo/wiki/Sharding-Internals](https://github.com/mongodb/mongo/wiki/Sharding-Internals)'
- en: '[http://learnmongodbthehardway.com/schema/sharding](http://learnmongodbthehardway.com/schema/sharding)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://learnmongodbthehardway.com/schema/sharding](http://learnmongodbthehardway.com/schema/sharding)'
- en: '[http://learnmongodbthehardway.com/schema/sharding/](http://learnmongodbthehardway.com/schema/sharding/)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://learnmongodbthehardway.com/schema/sharding/](http://learnmongodbthehardway.com/schema/sharding/)'
- en: '[https://www.infoq.com/news/2010/08/MongoDB-1.6](https://www.infoq.com/news/2010/08/MongoDB-1.6)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.infoq.com/news/2010/08/MongoDB-1.6](https://www.infoq.com/news/2010/08/MongoDB-1.6)'
- en: '[http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png](http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png](http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)'
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored sharding, one of the most interesting features
    of MongoDB. We started with an architectural overview of sharding and moved on
    to how we can design a shard and choose the right shard key.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了MongoDB最有趣的功能之一，即分片。我们从分片的架构概述开始，然后讨论了如何设计分片并选择正确的分片键。
- en: We learned about monitoring, administration, and the limitations that come with
    sharding. We also learned about `mongos`, the MongoDB sharding router that directs
    our queries to the correct shard. Finally, we discussed recovery from common failure
    types in a MongoDB sharded environment.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了监控、管理和分片带来的限制。我们还学习了`mongos`，MongoDB的分片路由器，它将我们的查询定向到正确的分片。最后，我们讨论了在MongoDB分片环境中从常见故障类型中恢复。
- en: The next chapter on fault tolerance and high availability will offer some useful
    tips and tricks that have not been covered in the 11 other chapters.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章关于容错和高可用性将提供一些有用的技巧和窍门，这些内容在其他11章中没有涉及。
