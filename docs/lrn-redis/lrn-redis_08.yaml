- en: Chapter 8. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 集群
- en: If you are reading this, it would mean that you have fair amount of understanding
    of what Redis is and how it can be used in applications for web and business.
    Apart from that, it would be fair to assume that you also have fair amount of
    understanding of the data structures it can hold and how to use them in your application.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在阅读本文，这意味着您对Redis及其在Web和业务应用中的使用有相当多的了解。除此之外，可以合理地假设您对它可以容纳的数据结构以及如何在应用程序中使用它们也有相当多的了解。
- en: In this chapter, we will continue ahead and discuss the steps that need to be
    taken for a Redis application to get deployed in a production environment. Well,
    deployment in a production environment is always tricky and calls for a greater
    in-depth understanding of the architecture and the business requirement. Since
    we cannot envisage the business requirements that the applications have to fulfil
    but we can always abstract out the nonfunctional requirements, most of applications
    have and create patterns which can be used by the readers as they see fit.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续讨论在生产环境中部署Redis应用所需采取的步骤。在生产环境中部署总是棘手的，并需要对架构和业务需求有更深入的了解。由于我们无法设想应用程序必须满足的业务需求，但我们总是可以抽象出大多数应用程序具有的非功能需求，并创建可以供读者根据需要使用的模式。
- en: 'Some of the most common nonfunctional requirements that come to mind when we
    think or talk about production environments are listed as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑或谈论生产环境时，脑海中浮现的一些最常见的非功能需求列举如下：
- en: Performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能
- en: Availability
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性
- en: Scalability
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Manageability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可管理性
- en: Security
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性
- en: All the mentioned nonfunctional requirements are always addressed in the way
    we create the blueprint for our deployment architecture. Going forward, we will
    take these nonfunctional requirements and map them to the cluster patterns that
    we will discuss.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有提到的非功能需求都总是在我们创建部署架构的蓝图时得到解决。接下来，我们将把这些非功能需求与我们将讨论的集群模式进行映射。
- en: Clusters
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: A computer cluster consists of a set of loosely or tightly connected computers
    that work together so that, in many respects, they can be viewed as a single system.
    The source of this information is [http://en.wikipedia.org/wiki/Computer_cluster](http://en.wikipedia.org/wiki/Computer_cluster).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机集群由一组松散或紧密连接的计算机组成，它们共同工作，因此在许多方面，它们可以被视为一个单一系统。此信息来源于[http://en.wikipedia.org/wiki/Computer_cluster](http://en.wikipedia.org/wiki/Computer_cluster)。
- en: There are multiple reasons why we do clustering of systems. Enterprises have
    requirements to grow which have to be matched with cost effectiveness and future
    roadmap of solutions; therefore, it always makes sense to go for clustered solution.
    One big machine to handle all the traffic is always desirable but the problem
    with vertical scalability is the ceiling in compute capability of the chip. Moreover,
    bigger machines always cost more as compared to a group of smaller machines with
    aggregate same compute power. Along with cost effectiveness, the other nonfunctional
    requirements that a cluster can take care of are performance, availability, and
    scalability. However, having a cluster also increases efforts of manageability,
    maintainability, and security.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对系统进行集群有多种原因。企业需要与成本效益和解决方案未来路线图相匹配的增长需求；因此，选择集群解决方案总是有意义的。一个大型机器来处理所有流量总是理想的，但纵向扩展的问题在于芯片的计算能力上限。此外，与一组具有相同计算能力的较小机器相比，更大的机器成本总是更高。除了成本效益之外，集群还可以满足的其他非功能需求包括性能、可用性和可扩展性。然而，拥有集群也增加了可管理性、可维护性和安全性的工作量。
- en: 'With the traffic that the modern websites are generating, clustering is not
    just a low cost option but the only option left. With that perspective in mind,
    let''s look into various patterns of clustering and see how they fit with Redis.
    The two patterns that can be developed for Redis-based clusters are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代网站产生的流量，集群不仅是一种低成本选择，而且是唯一的选择。从这个角度来看，让我们来看看各种集群模式，并看看它们如何与Redis配合。可以为基于Redis的集群开发的两种模式是：
- en: Master-master
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主-主
- en: Master-slave
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主-从
- en: Cluster pattern – master-master
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群模式-主-主
- en: This pattern of cluster is created for applications where read and writes are
    very frequent and the state across the nodes needs to be the same at any given
    point in time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集群模式是为了应用程序而创建的，其中读取和写入非常频繁，并且节点之间的状态在任何给定时间点都需要保持一致。
- en: 'From a nonfunctional requirement perspective, following behaviors can be seen
    in a master-master setup:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从非功能需求的角度来看，在主-主设置中可以看到以下行为：
- en: '![Cluster pattern – master-master](img/1794_08_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![集群模式-主-主](img/1794_08_01.jpg)'
- en: Getters and setters in master–master cluster pattern
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 主-主集群模式中的获取器和设置器
- en: Performance
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: The performance for reads and writes are very high in this kind of setup. Since
    the requests are load balanced across the master nodes, the individual load on
    a master reduces, thus resulting in better performance. As Redis inherently does
    not have this capability, this has to be provided outside the box. A write replicator
    and read load balancer kept in front of the master-master cluster will do the
    trick. What we are doing here is that, if there is a write request, the data will
    be written to all the masters, and all the read requests can be split among any
    of the master nodes, since the data in all the master nodes is in a consistent
    state.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的设置中，读取和写入的性能非常高。由于请求在主节点之间进行负载平衡，主节点的个体负载减少，从而导致更好的性能。由于Redis本身并没有这种能力，因此必须在外部提供。在主-主集群的前面放置写复制器和读负载均衡器将起到作用。我们在这里所做的是，如果有写入请求，数据将被写入所有主节点，并且所有读取请求可以在任何主节点之间分配，因为所有主节点中的数据处于一致的状态。
- en: Another dimension that we have to keep in mind is that when data quantity is
    very high. In case the data quantity is very high, then we have to create **shards**
    (**nodes**) inside the master node setup. These shards in individual master nodes
    can distribute the data based on the key. Later in the chapter, we will discuss
    the sharding capability in Redis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要考虑的另一个方面是数据量非常大的情况。如果数据量非常大，那么我们必须在主节点设置内部创建**分片**（**节点**）。各个主节点内的这些分片可以根据密钥分发数据。在本章后面，我们将讨论Redis中的分片能力。
- en: '![Performance](img/1794_08_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![性能](img/1794_08_02.jpg)'
- en: Read and write operations in a sharded environment
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在分片环境中的读写操作'
- en: Availability
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '- 可用性'
- en: The availability of data is high or rather depends upon the number of master
    nodes maintained for replication. The state of the data is same across all the
    master nodes, so even if one of the master nodes goes down, the rest of the master
    nodes can cater to the request. During this condition, the performance of the
    application will dip since the requests have to be shared among the remaining
    master nodes. In case of data being distributed across shards inside the master
    node, if one of the shards were to go down, the request for that shard can be
    catered to by the replica shard in the other master nodes. This will still keep
    the affected master node working (but not to the full extent).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的可用性很高，或者更依赖于用于复制的主节点数量。所有主节点上的数据状态是相同的，因此即使其中一个主节点宕机，其余的主节点也可以处理请求。在这种情况下，应用程序的性能会下降，因为请求必须在剩余的主节点之间共享。如果数据分布在主节点内的分片中，如果其中一个分片宕机，那么其他主节点中的副本分片可以处理该分片的请求。这将使受影响的主节点仍然能够工作（但不是完全）。
- en: Scalability
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '- 可扩展性'
- en: 'The issue of scalability is a bit tricky in this case. While provisioning a
    new master node, following care has to be taken:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可扩展性的问题有点棘手。在提供新的主节点时，必须注意以下事项：
- en: The new master node has to be in the same state as the other master nodes.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的主节点必须处于与其他主节点相同的状态。
- en: The information for the new master node has to be added in the client API, as
    the client can then take this new master node while governing the data writes
    and data reads.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的主节点的信息必须添加到客户端API中，因为客户端可以在管理数据写入和数据读取时使用这个新的主节点。
- en: For these tasks, a period of downtime is required which will impact availability.
    Moreover, data volumes have to be factored in before sizing the master nodes or
    shard nodes in a master node to avoid these scenarios.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些任务需要一段停机时间，这将影响可用性。此外，在调整主节点或主节点中的分片节点的大小之前，必须考虑数据量，以避免出现这些情况。
- en: Manageability
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可管理性
- en: 'Manageability of this type of cluster pattern requires efforts at both the
    node level and client level. This is because Redis does not provide an in-built
    mechanism for this pattern. Since the responsibility of doing data replication
    and data load is of the client adaptor, so following observations need to be addressed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的集群模式的可管理性需要在节点级别和客户端级别进行努力。这是因为Redis没有为这种模式提供内置机制。由于进行数据复制和数据加载的责任在于客户端适配器，因此需要解决以下观察结果：
- en: The client adaptor has to factor serviceability in case the node (master or
    shard) goes down.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '- 客户端适配器必须考虑可服务性，以防节点（主节点或分片）宕机。'
- en: The client adaptor has to factor in serviceability in case a new master node
    is added.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '- 客户端适配器必须考虑可服务性，以防添加新的主节点。'
- en: Adding a new shard in to an already configured shard ecosystem should be avoided,
    as the sharding technique is based on the unique key generated by the client adaptor.
    This is decided on the basis of shard nodes configured at the beginning of the
    application, and adding a new shard will disturb the already set shards and the
    data inside it. This will render the entire application in an inconsistent state.
    This new shard will have some data replicated from the other shards in the master
    node's ecosystem. So the opted way for doing this would be to introduce consistent
    hashing to generate unique keys for assigning the master nodes.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在已经配置了分片生态系统的情况下，应该避免添加新的分片，因为分片技术是基于客户端适配器生成的唯一密钥。这是根据应用程序开始配置的分片节点来决定的，添加新的分片将会干扰已经设置的分片和其中的数据。这将使整个应用程序处于不一致的状态。新的分片将从主节点的生态系统中复制一些数据。因此，进行此操作的选择方式将是引入一致性哈希来生成分配主节点的唯一密钥。
- en: Security
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: Redis, being a very light weight data store, has very little to offer from a
    security perspective. The expectation here is that the Redis nodes will be provisioned
    in a secured environment where the responsibility is outside the box. Nevertheless,
    Redis does provide some form of security in terms of username/password authentication
    to connect to node. This mechanism has its limitations since the password is stored
    in the `Config` file in clear text. Another form of security can be obfuscating
    the commands so that it cannot be called accidently. In the cluster pattern we
    are discussing it has limited use and is more from a program perspective.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Redis作为一个非常轻量级的数据存储，从安全性的角度来看提供的内容很少。这里的期望是Redis节点将在一个安全的环境中进行配置，责任在于外部。尽管如此，Redis确实提供了一定形式的安全性，即用户名/密码身份验证连接到节点。这种机制有其局限性，因为密码以明文形式存储在`Config`文件中。另一种安全性形式可以是混淆命令，以防止意外调用。在我们讨论的集群模式中，这种安全性的用途有限，更多的是从程序的角度来看。
- en: Drawbacks of this pattern
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '- 此模式的缺点'
- en: This pattern has a few grey areas which we need to look out for before deciding
    to adopt it. A planned downtime is required for this pattern to work in a production
    environment. This downtime is essential if a node goes down and a new node is
    added to the cluster. This new node has to have the same state as the other replica
    master node. Another thing to look out for is data capacity planning, if underestimated,
    scaling horizontally would be a problem if done in a sharded environment. In the
    next section, we will run an example where we add another node and see a different
    distribution of data which can give us a hint of the problems. Data purging is
    another feature which is not addressed by Redis server as it's meant to hold all
    the data in the memory.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定采用这种模式之前，我们需要注意一些灰色地带。这种模式需要计划的停机时间才能在生产环境中工作。如果一个节点宕机并且向集群添加了一个新节点，则需要这种停机时间。这个新节点必须具有与其他副本主节点相同的状态。另一个需要注意的是数据容量规划，如果低估了，那么在分片环境中进行水平扩展将是一个问题。在下一节中，我们将运行一个示例，添加另一个节点，并查看不同的数据分布，这可以给我们一些问题的提示。数据清理是另一个特性，Redis服务器没有解决，因为它的目的是将所有数据保存在内存中。
- en: Sharding
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分片
- en: Sharding is a mechanism of horizontally splitting data and placing it in different
    nodes (machines). Here, each partition of data residing in a different node or
    machine forms a shard. Sharding technique is useful in scaling a data store to
    multiple nodes or machines. Sharding, if done correctly, can improve the performance
    of the system at large.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 分片是一种水平拆分数据并将其放置在不同的节点（机器）中的机制。在这里，驻留在不同节点或机器中的数据的每个分区形成一个分片。如果正确执行分片技术，可以将数据存储扩展到多个节点或机器。如果正确执行分片，可以提高系统的性能。
- en: It can also overcome the need to go for a bigger machine and can get the job
    done with smaller machines. Sharding can provide partial fault tolerance since
    if a node goes down then the request coming to that particular node cannot be
    served unless all the other nodes can cater to the incoming request.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 它还可以克服需要使用更大的机器，并且可以使用较小的机器完成工作。分片可以提供部分容错能力，因为如果一个节点宕机，那么来到该特定节点的请求将无法得到服务，除非所有其他节点都能满足传入的请求。
- en: Redis does not provide a direct mechanism to support sharding of data internally,
    so to achieve partitioning of data, techniques have to be applied from the client
    API when splitting the data. Since Redis is a key value data store, a unique ID
    can be generated based on an algorithm which can be mapped to nodes. So if a request
    to read, write, update, or delete comes, the algorithm can generate the same unique
    key or can direct it to the mapped node where the action can take place.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Redis没有提供直接机制来支持数据的内部分片，因此为了实现数据的分区，必须在客户端API中应用技术来拆分数据。由于Redis是一个键值数据存储，可以基于算法生成唯一ID，然后将其映射到节点。因此，如果有读取、写入、更新或删除的请求，算法可以生成相同的唯一键，或者将其定向到映射的节点，从而进行操作。
- en: Jedis, the client API we are using in this book, does provide a mechanism to
    shard data based on keys. Let's try out a sample and see the distribution of data
    across the nodes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中使用的客户端API Jedis提供了基于键的数据分片机制。让我们尝试一个示例，并查看数据在节点之间的分布。
- en: 'Start with at least two nodes. The procedure has been discussed in the previous
    chapters. In the current sample, we will be starting one node on port 6379 and
    other on 6380\. The first shard node should look similar to the following screenshot:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从至少两个节点开始。该过程已在前几章中讨论过。在当前示例中，我们将在端口6379上启动一个节点，另一个节点在6380上启动。第一个分片节点应该类似于以下屏幕截图：
- en: '![Sharding](img/1794_08_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![分片](img/1794_08_03.jpg)'
- en: Screenshot for first shard node
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个分片节点的屏幕截图
- en: 'The second shard node should look similar to the following screenshot:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个分片节点应该类似于以下屏幕截图：
- en: '![Sharding](img/1794_08_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![分片](img/1794_08_04.jpg)'
- en: Screenshot for second shard node
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个分片节点的屏幕截图
- en: 'Let''s open our editor and type in the following program:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开编辑器，输入以下程序：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The response in the console output should be as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台输出的响应应该如下所示：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Observations
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察
- en: 'The following observations can be made about the sample:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关于样本可以得出以下观察：
- en: The data distribution is random which basically is dependent upon the hashing
    algorithm used to distribute the program or shards
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分布是随机的，基本上取决于用于分发程序或分片的哈希算法
- en: Multiple execution of the same program with result in same result. This signifies
    that hashing algorithm is consistent with the hash it creates for a key.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多次执行相同的程序将得到相同的结果。这表明哈希算法对于为键创建哈希是一致的。
- en: If the key changes then the distribution of the data will be different, since
    a new hash code will be generated for the same given key; hence, a new target
    shard.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果键发生变化，那么数据的分布将不同，因为对于相同的给定键将生成新的哈希码；因此，会有一个新的目标分片。
- en: 'Add a new shard without cleaning the other shards:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在不清理其他分片的情况下添加一个新的分片：
- en: Start a new master at 6381:![Observations](img/1794_08_05.jpg)
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在6381上启动一个新的主节点：![观察](img/1794_08_05.jpg)
- en: 'Let''s type a new program wherein the client''s new shard information is added:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们输入一个新的程序，其中添加了客户端的新分片信息：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result will be as follows as we can see data from `SHARD_1` and `SHARD_2`
    getting replicated in `SHARD_3`. This *replicated data* is nothing but older data
    in the `SHARD_1` and `SHARD_2` because of the previous executions. This, in production
    environment, can be dangerous as it increases the dead data which cannot be accounted
    for:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将如下所示，因为我们可以看到来自`SHARD_1`和`SHARD_2`的数据在`SHARD_3`中被复制。这*复制的数据*实际上就是由于先前的执行而存在于`SHARD_1`和`SHARD_2`中的旧数据。在生产环境中，这可能是危险的，因为它增加了无法核算的死数据：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add a new master node for the same set of data and clean all the previous data
    in the `SHARD_1` and `SHARD_2` nodes, and the result will be as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为相同的数据集添加一个新的主节点，并清理`SHARD_1`和`SHARD_2`节点中的所有先前数据，结果将如下：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see data getting distributed cleanly amongst all the shards with no repetitions
    such as older data is cleaned.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据在所有分片之间得到了清洁的分布，没有重复，例如旧数据已被清理。
- en: Cluster pattern – master-slave
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群模式 - 主从
- en: This pattern of cluster is created for applications where reads are very frequent
    and writes are less frequent. Another condition necessary for this pattern to
    work is to have a limited size of data, or in other words, data capacity that
    can fit into the hardware provisioned for the master (the same hardware configuration
    is needed for the slaves too). Since the requirement is to cater to frequent reads,
    this pattern also has the capability to scale horizontally. Another point we have
    to keep in mind is that replication in slaves can have a time delay which can
    result in stale data getting served. The business requirement should be okay with
    that scenario.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集群模式适用于读取非常频繁而写入不太频繁的应用程序。这种模式能够工作的另一个条件是具有有限的数据大小，或者换句话说，数据容量可以适应为主节点配置的硬件（从节点也需要相同的硬件配置）。由于要满足频繁读取的需求，这种模式还具有水平扩展的能力。我们还必须记住的一点是，从节点中的复制可能会有时间延迟，这可能导致提供陈旧数据。业务需求应该能够接受这种情况。
- en: The solution for this pattern would be to have all the writes done to the master,
    and have the slave cater to all the reads. The reads to slaves need to be load
    balanced so that performance can be met.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的解决方案是将所有写操作都放在主节点上，并让从节点处理所有读操作。需要对从节点的读操作进行负载平衡，以满足性能要求。
- en: Redis provides an in-built capability to have master-slave configuration wherein
    the writes can be done to the master and the reads can be done to the slaves.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Redis提供了内置的主从配置功能，其中写入可以在主节点上进行，而读取可以在从节点上进行。
- en: '![Cluster pattern – master-slave](img/1794_08_06.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![集群模式-主从](img/1794_08_06.jpg)'
- en: Getters and setters in master–slave pattern
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 主-从模式中的获取器和设置器
- en: From a nonfunctional requirement perspective, the behaviors that can be seen
    in a master-slave setup are discussed in the following sections.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从非功能需求的角度来看，主-从设置中可以看到的行为在以下部分进行了讨论。
- en: Performance
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: The performance writes are very high in this kind of setup. This is because
    all the writes are happening to a single master node and the frequency of the
    writes is less as mentioned in the assumption. Since the read requests are load
    balanced across the slave nodes, the individual load on a slave reduces, thus
    resulting in better performance. As Redis inherently provides reads from slaves,
    nothing except for load balancing has to be provided outside the box. A read load
    balancer kept in front of the slave nodes will do the trick. What we are doing
    here is that if there is a write request the data will be written to the master
    and all the read requests will be split among all of the slave nodes since the
    data in all the slave nodes is in an eventual consistent state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，性能写入非常高。这是因为所有写入都发生在单个主节点上，并且写入的频率较低，如假设中所述。由于读取请求在从节点之间进行负载平衡，从节点上的单个负载减少，从而提高了性能。由于Redis本身提供了从从节点读取的功能，除了负载平衡外，无需在外部提供任何内容。放置在从节点前面的读取负载均衡器将起作用。我们在这里所做的是，如果有写入请求，数据将被写入主节点，所有读取请求将分配给所有从节点，因为所有从节点中的数据处于最终一致状态。
- en: The thing to be noted in this case is that due to the time difference between
    master pushing the new updated data and the slave updating it, a scenario can
    be reached where the slave continues to serve stale data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下需要注意的是，由于主节点推送新更新数据和从节点更新数据之间的时间差，可能会出现从节点继续提供陈旧数据的情况。
- en: Availability
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用性
- en: 'Availability in master-slave cluster pattern requires different approaches,
    one for the master nodes and the other for the slave nodes. The easiest part is
    slave availability. When it comes to availability of the slaves, it''s pretty
    easy to handle since there are more slaves as compared to a master, and even if
    one of the slaves goes down, there are other slaves to cater to requests. In case
    of the master, since there is only one master, if that node goes down then we
    have a problem. While the reads will continue to happen unaffected, the writes
    will stop. In order to do away with data losses, there are two things that can
    be done:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 主-从集群模式中的可用性需要不同的方法，一个用于主节点，另一个用于从节点。最容易处理的是从节点的可用性。当涉及到从节点的可用性时，很容易处理，因为从节点比主节点更多，即使其中一个从节点出现问题，也有其他从节点来处理请求。在主节点的情况下，由于只有一个主节点，如果该节点出现问题，我们就有麻烦了。虽然读取将继续进行，但写入将停止。为了消除数据丢失，可以采取以下两种措施：
- en: Have a message queue in front of the master node so that even in case the master
    goes down, the write message persists, which can be later written down.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主节点前面设置一个消息队列，以便即使主节点出现问题，写入消息仍然存在，可以稍后写入。
- en: Redis provides a mechanism or an observer called Sentinel which can be used.
    Discussion on Sentinel has been done in some of the upcoming sections.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis提供了一种称为Sentinel的机制或观察者。Sentinel的讨论已在即将到来的某些部分中进行。
- en: Scalability
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: 'The issue of scalability is a bit tricky in this case since we have two types
    of nodes here and both of them solve different kind of purpose. The scalability
    here will not be in terms of distributing the data across but more in terms of
    scaling out for performance. Following are some features:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可扩展性的问题有点棘手，因为这里有两种类型的节点，它们都解决不同类型的目的。在这里，可扩展性不在于数据的分布，而更多地在于为了性能而进行的扩展。以下是一些特点：
- en: Master node has to be sized according to the data capacity that needs to be
    kept in RAM for performance
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点的大小必须根据需要在RAM中保留的数据容量来确定性能
- en: The slave nodes can be attached to the cluster at run time but they eventually
    would come to the same state as master node, and the hardware capability of the
    slave should be on a par with the master
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从节点可以在运行时附加到集群，但最终会达到与主节点相同的状态，并且从节点的硬件能力应与主节点相当
- en: The new slave node should be registered into the load balancer for the load
    balancer to distribute the data
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的从节点应该注册到负载均衡器中，以便负载均衡器分发数据
- en: Manageability
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可管理性
- en: Manageability of this type of cluster pattern requires little effort at the
    master and slave node level and at the client level. This is because Redis does
    provide an in-built mechanism to support this pattern. Since the responsibility
    of doing data replication and data load is of the slave nodes, so what is left
    is managing the master and the client adaptors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的集群模式的可管理性在主节点和从节点级别以及客户端级别需要付出很少的努力。这是因为Redis确实提供了支持这种模式的内置机制。由于数据复制和数据加载的责任属于从节点，所以剩下的就是管理主节点和客户端适配器。
- en: 'The following observations need to be addressed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 需要解决以下观察结果：
- en: The client adaptor has to factor serviceability in case the slave node goes
    down. The adaptor has to be intelligent enough to avoid the slave node that is
    down.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端适配器必须考虑服务性，以防从节点宕机。适配器必须足够智能，以避免宕机的从节点。
- en: The client adaptor has to factor in serviceability in case a new slave node
    is added.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端适配器必须考虑服务性，以防新的从节点被添加。
- en: The client adaptor has to have a temporary persistence mechanism in case the
    master goes down.![Manageability](img/1794_08_07.jpg)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端适配器必须具有临时持久性机制，以防主节点宕机。![可管理性](img/1794_08_07.jpg)
- en: Fault tolerance in master node
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点的容错能力
- en: Security
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: Redis, being a very light weight data store, has very little to offer from a
    security perspective. The expectation here is that the Redis nodes will be provisioned
    in a secured environment where the responsibility is outside the box. Nevertheless,
    Redis does provide some form of security in terms of username/password authentication
    to connect to node. This mechanism has its limitations since the password is stored
    in the `Config` file in clear text. Another form of security can be obfuscating
    the commands so that it cannot be called accidently.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Redis作为一个非常轻量级的数据存储，在安全方面提供的内容非常有限。这里的期望是Redis节点将在一个受保护的环境中进行配置，责任在于环境之外。尽管如此，Redis确实提供了一定形式的安全性，例如用户名/密码认证连接到节点。这种机制有其局限性，因为密码是以明文存储在`Config`文件中的。另一种安全性形式可以是混淆命令，以防止意外调用。
- en: 'In the cluster pattern we are discussing it has limited use and is more from
    a program perspective. Another good practice is to have separate APIs so that
    the program doesn''t accidentally write to the slave nodes (though this will result
    in an error). Following are some APIs discussed:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论的集群模式中，它的使用有限，更多的是从程序的角度来看。另一个很好的做法是有单独的API，这样程序就不会意外地写入从节点（尽管这将导致错误）。以下是一些讨论过的API：
- en: '**WRITE API**: This component should be with the program interacting with the
    master node since the master can do writes in master-slave'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**写API**：这个组件应该与与主节点进行交互的程序一起使用，因为主节点可以在主从中进行写入'
- en: '**READ API**: This component should be with the program interacting with the
    slaves which have to fetch the records'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**读API**：这个组件应该与与必须获取记录的从节点进行交互的程序一起使用'
- en: Drawbacks of this pattern
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这种模式的缺点
- en: This pattern has few grey areas which we need to look out for before deciding
    to adopt it. One of the biggest problems is data size. The capacity sizing should
    be done in accordance to the vertical scaling capability of the master. The same
    hardware capability has to be done for the slaves. Another problem is the latency
    which can happen when the master copies the data to the slaves. This can sometimes
    result in stale data getting served in some conditions. Another area to look for
    is the time taken by the Sentinel to elect a new master if there is a failover
    in the master.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定采用这种模式之前，这种模式还有一些需要注意的地方。其中最大的问题之一是数据大小。容量大小应该根据主节点的垂直扩展能力来确定。从节点也必须具有相同的硬件能力。另一个问题是主节点将数据复制到从节点时可能出现的延迟。这有时会导致在某些情况下提供过时的数据。另一个需要注意的地方是如果主节点发生故障，Sentinel选举新的主节点所需的时间。
- en: This pattern is best suited for scenarios where Redis is used as caching engine.
    In case it's used as a caching engine, then it's a good practice to evict the
    data after it's reached a certain size. In the section that follows, there are
    the eviction policies which we can use in Redis to manage the data size.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式最适合用于Redis作为缓存引擎的情况。如果它被用作缓存引擎，那么在达到一定大小后清除数据是一个很好的做法。在接下来的部分中，有我们可以在Redis中使用的清除策略来管理数据大小。
- en: Configuring Redis Sentinel
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Redis Sentinel
- en: Datastores provide capability in handling faulty scenarios. These capabilities
    are in-built and do not expose themselves in the manner in which they handle fault
    tolerance. Redis, which started with simple key value datastore, has evolved into
    a system which provides an independent node to take care of the fault management.
    This system is called **Sentinel**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储提供了处理故障情况的能力。这些能力是内置的，并且不会以处理容错的方式暴露自己。Redis最初是一个简单的键值数据存储，已经发展成为一个提供独立节点来处理故障管理的系统。这个系统被称为**Sentinel**。
- en: 'The idea behind Sentinel is that it''s an independent node which keeps a track
    on the master node and the other slave nodes. When a master node goes down, it
    promotes the slave node to become the master. As discussed, in a master-slave
    scenario, master is meant to write and slaves are meant to read, so when a slave
    is promoted to master, it has the capability to read and write. All the other
    slaves will become slave to this new slave turned master. The following figure
    shows how Sentinel works:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Sentinel背后的理念是它是一个独立的节点，它跟踪主节点和其他从节点。当主节点宕机时，它会将从节点提升为主节点。正如讨论的那样，在主从场景中，主节点用于写入，从节点用于读取，所以当从节点被提升为主节点时，它具有读写的能力。所有其他从节点将成为这个新的从节点变成的主节点的从节点。下图显示了Sentinel的工作原理：
- en: '![Configuring Redis Sentinel](img/1794_08_08.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![配置Redis Sentinel](img/1794_08_08.jpg)'
- en: The working of Sentinel
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Sentinel的工作
- en: 'Let''s have an example now to demonstrate how Sentinel works as of Redis 2.6
    Version. Sentinel has problems running in Windows machines, so this example is
    best executed out of *NIX machine. The steps are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们举个例子，演示Sentinel在Redis 2.6版本中的工作原理。Sentinel在Windows机器上运行时会出现问题，因此最好在*NIX机器上执行此示例。步骤如下：
- en: Start a master node as shown:![Configuring Redis Sentinel](img/1794_08_09.jpg)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照所示启动主节点:![配置Redis Sentinel](img/1794_08_09.jpg)
- en: Start a slave as shown. Let's call it slave:![Configuring Redis Sentinel](img/1794_08_10.jpg)
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照所示启动从节点。让我们称其为从节点:![配置Redis Sentinel](img/1794_08_10.jpg)
- en: Let's start Sentinel as shown next:![Configuring Redis Sentinel](img/1794_08_11.jpg)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照所示启动Sentinel:![配置Redis Sentinel](img/1794_08_11.jpg)
- en: 'Let''s write a program in which we will do the following:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一个程序，我们将在其中执行以下操作：
- en: Write to the master
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入主节点
- en: Read from the master
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主节点读取
- en: Write to a slave
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入从节点
- en: Stop the master
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止主节点
- en: Read from the master after shutting the master
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭主节点后从主节点读取
- en: Read from the slave after shutting the master
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭主节点后从从节点读取
- en: Write to the slave
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入从节点
- en: Sentinel configuration
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sentinel配置
- en: 'Let''s type the program:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们输入程序：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should be able to see the following result for the program you have written:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该能够看到您编写的程序的以下结果：
- en: Write to the master:![Configuring Redis Sentinel](img/1794_08_12.jpg)
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入主节点:![配置Redis Sentinel](img/1794_08_12.jpg)
- en: Read from the master:![Configuring Redis Sentinel](img/1794_08_13.jpg)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主节点读取:![配置Redis Sentinel](img/1794_08_13.jpg)
- en: Write to a slave:![Configuring Redis Sentinel](img/1794_08_14.jpg)
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入从节点:![配置Redis Sentinel](img/1794_08_14.jpg)
- en: Stop the master:![Configuring Redis Sentinel](img/1794_08_15.jpg)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止主节点:![配置Redis Sentinel](img/1794_08_15.jpg)
- en: Read from the master after shutting the master:![Configuring Redis Sentinel](img/1794_08_16.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭主节点后从主节点读取:![配置Redis Sentinel](img/1794_08_16.jpg)
- en: Read from the slave after shutting the master:![Configuring Redis Sentinel](img/1794_08_17.jpg)
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭主节点后从从节点读取:![配置Redis Sentinel](img/1794_08_17.jpg)
- en: Write to the slave:![Configuring Redis Sentinel](img/1794_08_18.jpg)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入从节点:![配置Redis Sentinel](img/1794_08_18.jpg)
- en: 'Add the following text to the default Sentinel configuration:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下文本添加到默认的Sentinel配置中：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's understand the meaning of the five lines that we have added in the preceding
    code. The default Sentinel will have the information of the master running in
    the default port. If you have started the master in some other host or port, accordingly
    those changes have to be made in the Sentinel file.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解我们在前面的代码中添加的五行的含义。默认的Sentinel将包含运行在默认端口的主节点的信息。如果您在其他主机或端口上启动了主节点，则必须相应地在Sentinel文件中进行更改。
- en: '**Sentinel monitor slave2master 127.0.0.1 63682 1**: This gives us the information
    of the host and port of the slave node. Apart from that, `1` indicates the quorum
    agreement between the Sentinels for them to agree upon if the master fails. In
    our case, since we are running only one Sentinel, hence `1` is the value mentioned.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentinel monitor slave2master 127.0.0.1 63682 1**：这给出了从节点的主机和端口的信息。除此之外，`1`表示Sentinel之间对于主节点故障达成一致的法定人数。在我们的情况下，由于我们只运行一个Sentinel，因此提到了`1`的值。'
- en: '**Sentinel down-after-milliseconds slave2master 10000**: This is the time for
    which the master should not be reachable. The idea is for the Sentinel to keep
    on pinging the master and if the master does not respond or responds with an error,
    than the Sentinel kicks in and starts its activity. If the Sentinel detects the
    master as down then it will mark the node as `SDOWN`. But this alone cannot decide
    if the master is down, there has to be an agreement between all the Sentinels
    to initiate the failover activity. When the agreement has been reached by the
    Sentinels that the master is down, then it''s called `ODOWN` state. Think of this
    as democracy among the Sentinel before a master which is down is ousted and a
    new master is chosen.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentinel down-after-milliseconds slave2master 10000**：这是主节点不可达的时间。Sentinel会不断ping主节点，如果主节点不响应或响应错误，那么Sentinel会开始其活动。如果Sentinel检测到主节点已经宕机，那么它将标记节点为`SDOWN`。但这本身不能决定主节点是否宕机，所有Sentinel之间必须达成一致才能启动故障转移活动。当Sentinel达成一致认为主节点已宕机时，就会处于`ODOWN`状态。可以将其视为Sentinel在选举新主节点之前达成一致的民主制度。'
- en: '**Sentinel failover-timeout slave2master 900000**: This is time specified in
    milliseconds which takes care of the entire time span of the entire failover process.
    When a failover is detected, the Sentinel requests the configuration of the new
    master is written in all the slaves configured.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentinel failover-timeout slave2master 900000**：这是以毫秒为单位指定的时间，负责整个故障转移过程的时间跨度。当检测到故障转移时，Sentinel会请求将新主节点的配置写入所有配置的从节点。'
- en: '**Sentinel parallel-syncs slave2master 1**: This configuration indicates the
    number of slaves that are reconfigured simultaneously after a failover event.
    If we serve read queries from the read-only slaves, we will want to keep this
    value low. This is because all the slaves will be unreachable at the time when
    the synchronization is happening.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentinel parallel-syncs slave2master 1**：此配置指示故障转移事件发生后同时重新配置的从节点数量。如果我们从只读从节点提供读取查询，我们希望将此值保持较低。这是因为在同步发生时，所有从节点将无法访问。'
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learnt how to use clustering techniques to maximize performance
    and handle growing datasets. Apart from that, we also had a glimpse of handing
    of data for availability and how well we can do fault handling. Though there are
    techniques which Redis provides but we also saw how we can use other techniques
    if we do not require Sentinel. In the next chapter we will focus on how to maintain
    Redis.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用集群技术来最大化性能并处理不断增长的数据集。除此之外，我们还对可用性数据处理和故障处理进行了简要介绍。虽然Redis提供了一些技术，但我们也看到了如果不需要Sentinel，我们如何使用其他技术。在下一章中，我们将专注于如何维护Redis。
