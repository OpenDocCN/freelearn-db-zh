["```go\n    > db.atomicOperationsTest.drop()\n    > db.atomicOperationsTest.insert({i:1})\n\n    ```", "```go\n    > db.atomicOperationsTest.findAndModify({\n     query: {i: 1},\n     update: {$set : {text : 'Test String'}},\n     new: false\n     }\n    )\n\n    ```", "```go\n    > db.atomicOperationsTest.findAndModify({\n     query: {i: 1},\n     update: {$set : {text : 'Updated String'}}, fields: {i: 1, text :1, _id:0},\n     new: true\n     }\n    )\n\n    ```", "```go\n    >db.atomicOperationsTest.findAndModify({\n     query: {i: 2},\n     update: {$set : {text : 'Test String'}},\n     fields: {i: 1, text :1, _id:0},\n     upsert: true,\n     new: true\n     }\n    )\n\n    ```", "```go\n    > db.atomicOperationsTest.find().pretty()\n\n    ```", "```go\n    >db.atomicOperationsTest.findAndModify({\n     query: {i: 2},\n     remove: true,\n     fields: {i: 1, text :1, _id:0},\n     new: false\n     }\n    )\n\n    ```", "```go\n    > function getNextSequence(counterId) {\n     return db.counters.findAndModify(\n     {\n     query: {_id : counterId},\n     update: {$inc : {count : 1}},\n     upsert: true,\n     fields:{count:1, _id:0},\n     new: true\n     }\n     ).count\n    }\n\n    ```", "```go\n    > getNextSequence('Posts Counter')\n    > getNextSequence('Posts Counter')\n    > getNextSequence('Profile Counter')\n\n    ```", "```go\n>db.counters.find()\n{ \"_id\" : \"Posts Counter\", \"count\" : 2 }\n{ \"_id\" : \"Profile Counter\", \"count\" : 1 }\n\n```", "```go\n    > use test\n    > db.system.js.save({ _id : 'add', value : function(num1, num2) {return num1 + num2}})\n\n    ```", "```go\n    > db.loadServerScripts()\n\n    ```", "```go\n    > add(1, 2)\n\n    ```", "```go\n    > use test\n    > db.eval('return add(1, 2)')\n\n    ```", "```go\n    > use test1\n    > db.eval('return add(1, 2)')\n\n    ```", "```go\n> db.runCommand({eval: function (num1, num2) {return num1 + num2}, args:[1, 2],nolock:true})\n\n```", "```go\n    > db.testCapped.drop()\n\n    ```", "```go\n    > db.createCollection('testCapped', {capped : true, size:100})\n\n    ```", "```go\n    > for(i = 1; i < 100; i++) {\n    db.testCapped.insert({'i':i, val:'Test capped'})\n     }\n\n    ```", "```go\n    > db.testCapped.find()\n\n    ```", "```go\n    > db.testCapped.remove()\n\n    ```", "```go\n    > for(i = 101 ; i < 500 ; i++) {\n     sleep(1000)\n     db.testCapped.insert({'i': i, val :'Test Capped'})\n    }\n\n    ```", "```go\n    > var cursor = db.testCapped.find().addOption(DBQuery.Option.tailable).addOption(DBQuery.Option.awaitData)\n    while(cursor.hasNext()) {\n     var next = cursor.next()\n     print('i: ' + next.i + ', value: ' + next.val)\n    }\n\n    ```", "```go\n    > use test\n\n    ```", "```go\n    for(i = 1 ; i <= 100 ; i++) {\n     db.normalCollection.insert({'i': i, val :'Some Text Content'})\n    }\n\n    ```", "```go\n    > db.normalCollection.find()\n\n    ```", "```go\n    > db.system.namespaces.find({name : 'test.normalCollection'})\n\n    ```", "```go\n    > db.runCommand({convertToCapped : 'normalCollection', size : 100})\n\n    ```", "```go\n    > db.normalCollection.find()\n\n    ```", "```go\n    > db.system.namespaces.find({name : 'test.normalCollection'})\n\n    ```", "```go\n    $ mvn exec:java -Dexec.mainClass=com.packtpub.mongo.cookbook.BinaryDataTest\n\n    ```", "```go\n    > db.binaryDataTest.findOne()\n\n    ```", "```go\nDBObject doc = new BasicDBObject(\"_id\", 1);\ndoc.put(\"fileName\", resourceName);\ndoc.put(\"size\", imageBytes.length);\ndoc.put(\"data\", imageBytes);\n```", "```go\n    $ mongofiles put -l glimpse_of_universe-wide.jpg universe.jpg\n\n    ```", "```go\n    > db.fs.files.findOne({filename:'universe.jpg'})\n    > db.fs.chunks.find({}, {data:0})\n\n    ```", "```go\n    $ mongofiles get -l UploadedImage.jpg universe.jpg\n\n    ```", "```go\n    $ mongofiles delete universe.jpg\n\n    ```", "```go\n    > db.fs.files.findOne({filename:'universe.jpg'})\n    > db.fs.chunks.find({}, {data:0})\n\n    ```", "```go\nconnected to: 127.0.0.1\nadded file: { _id: ObjectId('5310d531d1e91f93635588fe'), filename: \"universe.jpg\n\", chunkSize: 262144, uploadDate: new Date(1393612082137), md5: \nd894ec31b8c5add\nd0c02060971ea05ca\", length: 2711259 }\ndone!\n\n```", "```go\n> db.fs.files.findOne({filename:'universe.jpg'})\n\n```", "```go\n> db.fs.chunks.find({}, {data:0})\n\n```", "```go\n{\n_id: <Unique identifier of type ObjectId representing this chunk>,\nfile_id: <ObjectId of the document in fs.files for the file whose chunk this document represent>,\nn:<The chunk identifier starts with 0, this is useful for knowing the order of the chunks>,\ndata: <BSON binary content  for the data uploaded for the file>\n}\n```", "```go\n    > use test\n    > db.fs.chunks.drop()\n    > db.fs.files.drop()\n\n    ```", "```go\n    $ mvn exec:java -Dexec.mainClass=com.packtpub.mongo.cookbook.GridFSTests -Dexec.args=\"put ~/glimpse_of_universe-wide.jpg universe.jpg\"\n\n    ```", "```go\n    Successfully written to universe.jpg, details are:\n    Upload Identifier: 5314c05e1c52e2f520201698\n    Length: 2711259\n    MD5 hash: d894ec31b8c5addd0c02060971ea05ca\n    Chunk Side in bytes: 262144\n    Total Number Of Chunks: 11\n\n    ```", "```go\n    > db.fs.files.findOne({filename:'universe.jpg'})\n    > db.fs.chunks.find({}, {data:0})\n\n    ```", "```go\n    $ mvn exec:java -Dexec.mainClass=com.packtpub.mongo.cookbook.GridFSTests -Dexec.args=\"get '~/universe.jpg' universe.jpg\"\n\n    ```", "```go\n    Connected successfully..\n    Successfully written 2711259 bytes to ~/universe.jpg\n\n    ```", "```go\n    $ mvn exec:java -Dexec.mainClass=com.packtpub.mongo.cookbook.GridFSTests -Dexec.args=\"delete universe.jpg\"\n\n    ```", "```go\n    Connected successfully..\n    Removed file with name 'universe.jpg' from GridFS\n\n    ```", "```go\nGridFS gfs = new GridFS(client.getDB(\"test\"));\n```", "```go\n    $ python\n\n    ```", "```go\n    >>>import pymongo\n    >>>import gridfs\n\n    ```", "```go\n    >>>client = pymongo.MongoClient('mongodb://localhost:27017')\n    >>>db = client.test\n\n    ```", "```go\n    >>> db.fs.files.drop()\n    >>> db.fs.chunks.drop()\n\n    ```", "```go\n    >>>fs = gridfs.GridFS(db)\n\n    ```", "```go\n    >>>file = open('glimpse_of_universe-wide.jpg', 'rb')\n\n    ```", "```go\n    >>>fs.put(file, filename='universe.jpg')\n\n    ```", "```go\n    >>> db.fs.files.find_one()\n\n    ```", "```go\n    >>> gout = fs.get_last_version('universe.jpg')\n\n    ```", "```go\n    >>> fout = open('universe.jpg', 'wb')\n\n    ```", "```go\n    >>>fout.write(gout.read())\n    >>>fout.close()\n    >>>gout.close()\n\n    ```", "```go\n    $ mvn exec:java -Dexec.mainClass=com.packtpub.mongo.cookbook.OplogTrigger -Dexec.args=\"test.oplogTriggerTest\"\n\n    ```", "```go\n    $ mongo --port 27000 TriggerOperations.js --shell\n\n    ```", "```go\n    test:PRIMARY> triggerOperations()\n\n    ```", "```go\n[INFO] <<< exec-maven-plugin:1.2.1:java (default-cli) @ mongo-cookbook-oplogtriger <<<\n[INFO]\n[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ mongo-cookbook-oplogtriger ---\nConnected successfully..\nStarting tailing oplog...\nOperation is Insert ObjectId is 5321c4c2357845b165d42a5f\nOperation is Insert ObjectId is 5321c4c2357845b165d42a60\nOperation is Insert ObjectId is 5321c4c2357845b165d42a61\nOperation is Insert ObjectId is 5321c4c2357845b165d42a62\nOperation is Insert ObjectId is 5321c4c2357845b165d42a63\nOperation is Insert ObjectId is 5321c4c2357845b165d42a64\nOperation is Update ObjectId is 5321c4c2357845b165d42a60\nOperation is Delete ObjectId is 5321c4c2357845b165d42a61\nOperation is Insert ObjectId is 5321c4c2357845b165d42a65\nOperation is Insert ObjectId is 5321c4c2357845b165d42a66\nOperation is Insert ObjectId is 5321c4c2357845b165d42a67\nOperation is Insert ObjectId is 5321c4c2357845b165d42a68\nOperation is Delete ObjectId is 5321c4c2357845b165d42a5f\nOperation is Delete ObjectId is 5321c4c2357845b165d42a62\nOperation is Delete ObjectId is 5321c4c2357845b165d42a63\nOperation is Delete ObjectId is 5321c4c2357845b165d42a64\nOperation is Delete ObjectId is 5321c4c2357845b165d42a60\nOperation is Delete ObjectId is 5321c4c2357845b165d42a65\nOperation is Delete ObjectId is 5321c4c2357845b165d42a66\nOperation is Delete ObjectId is 5321c4c2357845b165d42a67\nOperation is Delete ObjectId is 5321c4c2357845b165d42a68\n\n```", "```go\nDBCursor cursor = collection.find().sort(new BasicDBObject(\"$natural\", -1)).limit(1);\nint current = (int) (System.currentTimeMillis() / 1000);\nreturn cursor.hasNext() ? (BSONTimestamp)cursor.next().get(\"ts\") : new BSONTimestamp(current, 1);\n```", "```go\nDBCursor cursor = collection.find(QueryBuilder.start(\"ts\")\n          .greaterThan(lastreadTimestamp).get())\n          .addOption(Bytes.QUERYOPTION_TAILABLE)\n          .addOption(Bytes.QUERYOPTION_AWAITDATA);\n```", "```go\n    $ mongoimport -c areaMap -d test --drop 2dMapLegacyData.json\n\n    ```", "```go\n    connected to: 127.0.0.1\n    Mon Mar 17 23:58:27.880 dropping: test.areaMap\n    Mon Mar 17 23:58:27.932 check 9 26\n    Mon Mar 17 23:58:27.934 imported 26 objects\n\n    ```", "```go\n    > db.areaMap.find()\n\n    ```", "```go\n    $ db.areaMap.ensureIndex({co:'2d'})\n\n    ```", "```go\n    $ db.areaMap.find({co:{$near:[12, 8]}, type:'R'}).limit(3)\n\n    ```", "```go\n    $ db.areaMap.find({co:{$near:[12, 8], $maxDistance:4}, type:'R'})\n\n    ```", "```go\n> db.areaMap.find({co:{$near:[12, 8]}, type:'R'}).limit(3)\n\n```", "```go\n> db.areaMap.find({co:{$near:[12, 8], $maxDistance:4}, type:'R'})\n\n```", "```go\n    {\"_id\":1, \"name\":\"White Street\", \"type\":\"B\", co:[4, 23]}\n\n    ```", "```go\n    {\"_id\":1, \"name\":\"White Street\", \"type\":\"B\", co:{type: 'Point', coordinates : [4, 23]}}\n\n    ```", "```go\n    $ mongoimport -c areaMapGeoJSON -d test --drop 2dMapGeoJSONData.json\n    $ mongoimport -c worldMap -d test --drop countries.geo.json\n\n    ```", "```go\n    > db.areaMapGeoJSON.ensureIndex({\"co\" : \"2dsphere\"})\n    > db.worldMap.ensureIndex({geometry:'2dsphere'})\n\n    ```", "```go\n    > db.areaMapGeoJSON.find(\n    {  co:{\n     $near:{\n     $geometry:{\n     type:'Point',\n     coordinates:[12, 8]\n     }\n     }\n     },\n     type:'R'\n    }).limit(3)\n\n    ```", "```go\n    > db.areaMapGeoJSON.find(\n    {  co:{\n     $geoIntersects:{\n     $geometry:{\n     type:'Polygon',\n     coordinates:[[[0, 0], [0, 11], [11, 11], [11, 0], [0, 0]]]\n     }\n     }\n     },\n     type:'R'\n    })\n\n    ```", "```go\n    > db.areaMapGeoJSON.find(\n     {co:{\n     $geoWithin:{\n     $geometry:{ type: 'Polygon', coordinates : [[ [3, 9], [3, 24], [6, 24], [6, 9], [3, 9]] ]}\n     }\n     },\n     type:'B'\n     }\n    )\n\n    ```", "```go\n    > db.runCommand({ \n     geoNear: \"areaMapGeoJSON\",\n     near: [ 12, 8 ],\n     spherical: true,\n     limit:3,\n     query:{type:'R'}\n     }\n    )\n\n    ```", "```go\n    > db.worldMap.find(\n     {geometry:{\n     $geoWithin:{\n     $geometry:{\n     type:'Point',\n     coordinates:[7, 52]\n     }\n     }\n     }\n     }\n     ,{properties:1, _id:0}\n    )\n\n    ```", "```go\n> db.areaMapGeoJSON.ensureIndex({\"co\" : \"2dsphere\"})\n\n```", "```go\n{\n  type:'Polygon',\n  coordinates:[[[0, 0], [0, 11], [11, 11], [11, 0], [0, 0]]]\n}\n```", "```go\n    $ mongoimport -d test -c userBlog --drop BlogEntries.json\n\n    ```", "```go\n    $ mongo\n\n    ```", "```go\n    > db.userBlog.findOne()\n\n    ```", "```go\n    > db.userBlog.ensureIndex({'blog_text':'text'})\n\n    ```", "```go\n    $ db.userBlog.find({$text: {$search : 'plot zoo'}})\n\n    ```", "```go\n    $ db.userBlog.find({$text: {$search : 'Zoo -plot'}})\n\n    ```", "```go\ndb.userBlog.find({$text:{$search:'plot zoo'}}, {score: { $meta: \"textScore\"}})\n\n```", "```go\ndb.userBlog.find({$text:{$search:'plot zoo'}}, { score: { $meta: \"textScore\" }}).sort({score: { $meta: \"textScore\"}})\n\n```", "```go\ndb.collection.ensureIndex(\n  {\n    blog_text1: \"text\", blog_text2: \"text\"\n  },\n  {\n    weights: {\n      blog_text1: 2,\n      blog_text2: 1,\n    },\n    name: \"MyCustomIndexName\"\n  }\n)\n```", "```go\ndb.userBlog.ensureIndex({'text':'text'}, {'default_language':'french'})\n\n```", "```go\n[\n  {\n    \"v\" : 1,\n    \"key\" : {\n      \"_id\" : 1\n    },\n    \"ns\" : \"test.userBlog\",\n    \"name\" : \"_id_\"\n  },\n  {\n    \"v\" : 1,\n    \"key\" : {\n      \"_fts\" : \"text\",\n      \"_ftsx\" : 1\n    },\n    \"ns\" : \"test.userBlog\",\n    \"name\" : \"text_text\",\n    \"default_language\" : \"french\",\n    \"weights\" : {\n      \"text\" : 1\n    },\n    \"language_override\" : \"language\",\n    \"textIndexVersion\" : 1\n  }\n]\n```", "```go\n{_id:1, language:'english', text: \u2026.}  //Language is English\n{_id:2, language:'german', text: \u2026.}  //Language is German\n{_id:3, text: \u2026.}      //Language is the default one, French in this case\n```", "```go\npip install mongo-connector\n\n```", "```go\n    $ git clone https://github.com/10gen-labs/mongo-connector.git\n    $ cd mongo-connector\n    $ python setup.py install\n\n    ```", "```go\n    $  mongod --dbpath /data/mongo/db --replSet textSearch --smallfiles --oplogSize 50\n\n    ```", "```go\n    $ mongo\n\n    ```", "```go\n    > rs.initiate()\n\n    ```", "```go\n    $ elasticsearch\n\n    ```", "```go\n    {\n     \"cluster_name\" : \"elasticsearch\",\n     \"nodes\" : {\n     \"p0gMLKzsT7CjwoPdrl-unA\" : {\n     \"name\" : \"Zaladane\",\n     \"transport_address\" : \"inet[/192.168.2.3:9300]\",\n     \"host\" : \"Amol-PC\",\n     \"ip\" : \"192.168.2.3\",\n     \"version\" : \"1.0.1\",\n     \"build\" : \"5c03844\",\n     \"http_address\" : \"inet[/192.168.2.3:9200]\",\n     \"process\" : {\n     \"refresh_interval\" : 1000,\n     \"id\" : 5628,\n     \"max_file_descriptors\" : -1,\n     \"mlockall\" : false\n     }\n     }\n     }\n    }\n\n    ```", "```go\n    $ python mongo_connector/connector.py -m localhost:27017 -t http://localhost:9200 -n test.user_blog --fields blog_text -d mongo_connector/doc_managers/elastic_doc_manager.py\n\n    ```", "```go\n    $ mongoimport -d test -c user_blog BlogEntries.json --drop\n\n    ```"]