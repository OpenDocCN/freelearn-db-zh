- en: Chapter 8. Integration with Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。与Hadoop集成
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下示例：
- en: Executing our first sample MapReduce job using the mongo-hadoop connector
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用mongo-hadoop连接器执行我们的第一个样本MapReduce作业
- en: Writing our first Hadoop MapReduce job
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写我们的第一个Hadoop MapReduce作业
- en: Running MapReduce jobs on Hadoop using streaming
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hadoop上使用流式处理运行MapReduce作业
- en: Running a MapReduce job on Amazon EMR
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon EMR上运行MapReduce作业
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Hadoop is a well-known open source software to process large datasets. It also
    has an API for the MapReduce programming model, which is widely used. Nearly all
    the big data solutions have some sort of support to integrate them with Hadoop
    in order to use its MapReduce framework. MongoDB has a connector as well that
    integrates with Hadoop and lets us write MapReduce jobs using the Hadoop MapReduce
    API, process the data residing in the MongoDB/MongoDB dumps, and write the result
    to the MongoDB/MongoDB dump files. In this chapter, we will look at some recipes
    about the basic MongoDB and Hadoop integration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个众所周知的用于处理大型数据集的开源软件。它还有一个用于MapReduce编程模型的API，被广泛使用。几乎所有的大数据解决方案都有某种支持，以便将它们与Hadoop集成，以使用其MapReduce框架。MongoDB也有一个连接器，可以与Hadoop集成，让我们使用Hadoop
    MapReduce API编写MapReduce作业，处理驻留在MongoDB/MongoDB转储中的数据，并将结果写入MongoDB/MongoDB转储文件。在本章中，我们将看一些关于基本MongoDB和Hadoop集成的示例。
- en: Executing our first sample MapReduce job using the mongo-hadoop connector
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用mongo-hadoop连接器执行我们的第一个样本MapReduce作业
- en: In this recipe, we will see how to build the mongo-hadoop connector from the
    source and set up Hadoop just for the purpose of running the examples in the standalone
    mode. The connector is the backbone that runs Hadoop MapReduce jobs on Hadoop
    using the data in Mongo.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将看到如何从源代码构建mongo-hadoop连接器，并设置Hadoop，以便仅用于在独立模式下运行示例。连接器是在Mongo中使用数据运行Hadoop
    MapReduce作业的支柱。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: There are various distributions of Hadoop; however, we will use Apache Hadoop
    ([http://hadoop.apache.org/](http://hadoop.apache.org/)). The installation will
    be done on Ubuntu Linux. Apache Hadoop always runs on the Linux environment for
    production, and Windows is not tested for production systems. For development
    purposes, Windows can be used. If you are a Windows user, I would recommend that
    you install a virtualization environment such as VirtualBox ([https://www.virtualbox.org/](https://www.virtualbox.org/)),
    set up a Linux environment, and then install Hadoop on it. Setting up VirtualBox
    and Linux on it is not shown in this recipe, but this is not a tedious task. The
    prerequisite for this recipe is a machine with the Linux operating system on it
    and an Internet connection. The version that we will set up here is 2.4.0 of Apache
    Hadoop. At the time of writing of this book, the latest version of Apache Hadoop,
    which is supported by the mongo-hadoop connector, is 2.4.0.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop有各种发行版；但是，我们将使用Apache Hadoop ([http://hadoop.apache.org/](http://hadoop.apache.org/))。安装将在Ubuntu
    Linux上进行。Apache Hadoop始终在Linux环境下运行用于生产，Windows未经过生产系统测试。开发目的可以使用Windows。如果您是Windows用户，我建议您安装虚拟化环境，如VirtualBox
    ([https://www.virtualbox.org/](https://www.virtualbox.org/))，设置Linux环境，然后在其上安装Hadoop。在这个示例中没有展示设置VirtualBox和Linux，但这不是一项繁琐的任务。这个示例的先决条件是一台安装了Linux操作系统的机器和一个互联网连接。我们将在这里设置Apache
    Hadoop的2.4.0版本。在撰写本书时，mongo-hadoop连接器支持的最新版本是2.4.0。
- en: A Git client is needed to clone the repository of the mongo-hadoop connector
    to the local filesystem. Refer to [http://git-scm.com/book/en/Getting-Started-Installing-Git](http://git-scm.com/book/en/Getting-Started-Installing-Git)
    to install Git.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 需要Git客户端来克隆mongo-hadoop连接器的存储库到本地文件系统。参考[http://git-scm.com/book/en/Getting-Started-Installing-Git](http://git-scm.com/book/en/Getting-Started-Installing-Git)来安装Git。
- en: You will also need MongoDB to be installed on your operating system. Refer to
    [http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)
    and install it accordingly. Start the `mongod` instance listening to port `27017`.
    It is not expected for you to be an expert in Hadoop but some familiarity with
    it will be helpful. Knowing the concept of MapReduce is important and knowing
    the Hadoop MapReduce API will be an advantage. In this recipe, we will explain
    what is needed to get the work done. You can get more details on Hadoop and its
    MapReduce API from other sources. The wiki page at [http://en.wikipedia.org/wiki/MapReduce](http://en.wikipedia.org/wiki/MapReduce)
    gives some good information about the MapReduce programming.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要在操作系统上安装MongoDB。参考[http://docs.mongodb.org/manual/installation/](http://docs.mongodb.org/manual/installation/)并相应地安装它。启动监听端口`27017`的`mongod`实例。不需要您成为Hadoop的专家，但对它有一些了解会有所帮助。了解MapReduce的概念很重要，了解Hadoop
    MapReduce API将是一个优势。在这个示例中，我们将解释完成工作所需的内容。您可以从其他来源获取有关Hadoop及其MapReduce API的更多详细信息。维基页面[http://en.wikipedia.org/wiki/MapReduce](http://en.wikipedia.org/wiki/MapReduce)提供了有关MapReduce编程的一些很好的信息。
- en: How to do it…
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'We will first install Java, Hadoop, and the required packages. We will start
    with installing JDK on the operating system. Type the following on the command
    prompt of the operating system:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先安装Java、Hadoop和所需的软件包。我们将从在操作系统上安装JDK开始。在操作系统的命令提示符上键入以下内容：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If the program doesn''t execute and you are told about various packages that
    contain javac and program, then we need to install Java as follows:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果程序无法执行，并告知您包含javac和程序的各种软件包，则需要按照以下方式安装Java：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is all we need to do to install Java.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们安装Java所需要做的一切。
- en: Download the current version of Hadoop from [http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/)
    and download version 2.4.0 (or the latest mongo-hadoop connector support).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/)下载当前版本的Hadoop，并下载2.4.0版本（或最新的mongo-hadoop连接器支持）。
- en: 'After the `.tar.gz` file is downloaded, execute the following on the command
    prompt:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下载`.tar.gz`文件后，在命令提示符上执行以下操作：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Open the `etc/hadoop/hadoop-env.sh` file and replace export `JAVA_HOME = ${JAVA_HOME}`
    with export `JAVA_HOME = /usr/lib/jvm/default-java`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`etc/hadoop/hadoop-env.sh`文件，并将`export JAVA_HOME = ${JAVA_HOME}`替换为`export
    JAVA_HOME = /usr/lib/jvm/default-java`。
- en: 'We will now get the mongo-hadoop connector code from GitHub on our local filesystem.
    Note that you don''t need a GitHub account to clone a repository. Clone the Git
    project from the operating system command prompt as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在本地文件系统上从GitHub获取mongo-hadoop连接器代码。请注意，您无需GitHub帐户即可克隆存储库。请按照以下操作系统命令提示符中的Git项目进行克隆：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a soft link—the Hadoop installation directory is the same as the one
    that we extracted in step 3:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建软链接- Hadoop安装目录与我们在第3步中提取的目录相同：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For example, if Hadoop is extracted/installed in the home directory, then this
    is the command to be executed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果Hadoop在主目录中提取/安装，则应执行以下命令：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By default, the mongo-hadoop connector will look for a Hadoop distribution under
    the `~/hadoop-binaries` folder. So, even if the Hadoop archive is extracted elsewhere,
    we can create a soft link to it. Once this link has been created, we should have
    the Hadoop binaries in the `~/hadoop-binaries/hadoop-2.4.0/bin` path.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，mongo-hadoop连接器将在`〜/hadoop-binaries`文件夹下查找Hadoop分发。因此，即使Hadoop存档在其他位置提取，我们也可以创建软链接。创建软链接后，我们应该在`〜/hadoop-binaries/hadoop-2.4.0/bin`路径中拥有Hadoop二进制文件。
- en: We will now build the mongo-hadoop connector from the source for the Apache
    Hadoop version 2.4.0\. The build-by-default builds for the latest version, so
    as of now, the `-Phadoop_version` parameter can be left out as 2.4 is the latest.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从源代码为Apache Hadoop版本2.4.0构建mongo-hadoop连接器。默认情况下，构建最新版本，因此现在可以省略`-Phadoop_version`参数，因为2.4是最新版本。
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This build process will take some time to get completed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此构建过程将需要一些时间才能完成。
- en: Once the build completes successfully, we will be ready to execute our first
    MapReduce job. We will do this using a `treasuryYield` sample provided with the
    mongo-hadoop connector project. The first activity is to import the data to a
    collection in Mongo.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建成功后，我们将准备执行我们的第一个MapReduce作业。我们将使用mongo-hadoop连接器项目提供的`treasuryYield`示例来执行此操作。第一步是将数据导入Mongo的集合中。
- en: 'Assuming that the `mongod` instance is up and running and listening to port
    `27017` for connections and the current directory is the root of the mongo-hadoop
    connector code base, execute the following command:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设`mongod`实例正在运行并监听端口`27017`进行连接，并且当前目录是mongo-hadoop连接器代码库的根目录，请执行以下命令：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the import action is successful, we are left with copying two jar files
    to the `lib` directory. Execute the following in the operating system shell:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入操作成功后，我们需要将两个jar文件复制到`lib`目录中。在操作系统shell中执行以下操作：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The JAR built for the mongo-hadoop core to be copied was named as shown in the
    preceding section for the trunk version of the code and built for Hadoop-2.4.0\.
    Change the name of the JAR accordingly when you build it yourself for a different
    version of the connector and Hadoop. The Mongo driver can be the latest version.
    Version 2.12.0 is the latest version at the time of writing of this book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了mongo-hadoop核心构建的JAR文件要复制，根据代码的前面部分和为Hadoop-2.4.0构建的版本，更改JAR的名称。当您为连接器和Hadoop的不同版本自行构建时，Mongo驱动程序可以是最新版本。在撰写本书时，版本2.12.0是最新版本。
- en: 'Now, execute the following command on the command prompt of the operating system
    shell:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在操作系统shell的命令提示符上执行以下命令：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output should print out a lot of things; however, the following line in
    the output tells us that the map reduce job is successful:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出应该打印出很多内容；但是，输出中的以下行告诉我们MapReduce作业成功：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Connect the `mongod` instance running on localhost from the mongo client and
    execute a find on the following collection:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从mongo客户端连接运行在本地主机上的`mongod`实例，并对以下集合执行查找：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Installing Hadoop is not a trivial task and we don't need to get into this to
    try our samples for the hadoop-mongo connector. To learn about Hadoop, its installation,
    and other things, there are dedicated books and articles available. For the purpose
    of this chapter, we will simply download the archive and extract and run the MapReduce
    jobs in the standalone mode. This is the quickest way to get going with Hadoop.
    All the steps up to step 6 are needed to install Hadoop. In the next couple of
    steps, we will clone the mongo-hadoop connector recipe. You can also download
    a stable version for your version of Hadoop at [https://github.com/mongodb/mongo-hadoop/releases](https://github.com/mongodb/mongo-hadoop/releases)
    if you prefer not to build from the source. We then build the connector for our
    version of Hadoop (2.4.0) till step 13\. Step 14 onward is what we will do to
    run the actual MapReduce job to work on the data in MongoDB. We imported the data
    to the `yield_historical.in` collection, which will be used as an input for the
    MapReduce job. Go ahead and query the collection in the mongo shell using the
    `mongo_hadoop` database to see a document. Don't worry if you don't understand
    the contents; we want to see what we intend to do with this data in this example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Hadoop并不是一项简单的任务，我们不需要进行这项工作来尝试hadoop-mongo连接器的示例。有专门的书籍和文章可供学习Hadoop、其安装和其他内容。在本章中，我们将简单地下载存档文件，提取并以独立模式运行MapReduce作业。这是快速入门Hadoop的最快方式。在步骤6之前的所有步骤都是安装Hadoop所需的。在接下来的几个步骤中，我们将克隆mongo-hadoop连接器配方。如果您不想从源代码构建，也可以在[https://github.com/mongodb/mongo-hadoop/releases](https://github.com/mongodb/mongo-hadoop/releases)下载适用于您Hadoop版本的稳定版本。然后，我们为我们的Hadoop版本（2.4.0）构建连接器，直到第13步。从第14步开始，我们将运行实际的MapReduce作业来处理MongoDB中的数据。我们将数据导入到`yield_historical.in`集合中，这将作为MapReduce作业的输入。继续使用`mongo_hadoop`数据库在mongo
    shell中查询集合，以查看文档。如果您不理解内容，不用担心；我们想要看到这个示例中的数据意图。
- en: The next step was to invoke the MapReduce operation on the data. The Hadoop
    command was executed giving one jar's path, (`examples/treasury_yield/build/libs/treasury_yield-1.2.1-SNAPSHOT-hadoop_2.4.jar`).
    This is the jar that contains the classes implementing the sample MapReduce operation
    for the treasury yield. The `com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`
    class in this JAR file is the Bootstrap class containing the main method. We will
    visit this class soon. There are lots of configurations supported by the connector.
    The complete list of configurations can be found at [https://github.com/mongodb/mongo-hadoop/](https://github.com/mongodb/mongo-hadoop/).
    For now, we will just remember that `mongo.input.uri` and `mongo.output.uri` are
    the collections for input and output for the map reduce operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是在数据上调用MapReduce操作。执行Hadoop命令，给出一个jar的路径（`examples/treasury_yield/build/libs/treasury_yield-1.2.1-SNAPSHOT-hadoop_2.4.jar`）。这个jar包含了实现国库收益率样本MapReduce操作的类。在这个JAR文件中的`com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`类是包含主方法的引导类。我们很快就会访问这个类。连接器支持许多配置。完整的配置列表可以在[https://github.com/mongodb/mongo-hadoop/](https://github.com/mongodb/mongo-hadoop/)找到。现在，我们只需要记住`mongo.input.uri`和`mongo.output.uri`是map
    reduce操作的输入和输出集合。
- en: With the project cloned, you can now import it to any Java IDE of your choice.
    We are particularly interested in the project at `/examples/treasury_yield` and
    core present in the root of the cloned repository.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 项目克隆后，您现在可以将其导入到您选择的任何Java IDE中。我们特别感兴趣的是位于`/examples/treasury_yield`的项目和位于克隆存储库根目录中的核心。
- en: Let's look at the `com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`
    class. This is the entry point for the MapReduce method and has a main method
    in it. To write MapReduce jobs for mongo using the mongo-hadoop connector, the
    main class always has to extend from `com.mongodb.hadoop.util.MongoTool`. This
    class implements the `org.apache.hadoop.Tool` interface, which has the run method
    and is implemented for us by the `MongoTool` class. All that the main method needs
    to do is execute this class using the `org.apache.hadoop.util.ToolRunner` class
    by invoking its static `run` method passing the instance of our main class (which
    is an instance of `Tool`).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig`类。这是MapReduce方法的入口点，并在其中有一个主方法。要使用mongo-hadoop连接器为mongo编写MapReduce作业，主类始终必须扩展自`com.mongodb.hadoop.util.MongoTool`。这个类实现了`org.apache.hadoop.Tool`接口，该接口具有run方法，并由`MongoTool`类为我们实现。主方法需要做的就是使用`org.apache.hadoop.util.ToolRunner`类执行这个类，通过调用其静态`run`方法传递我们的主类的实例（这是`Tool`的实例）。
- en: 'There is a static block that loads some configurations from two XML files,
    `hadoop-local.xml` and `mongo-defaults.xml`. The format of these files (or any
    XML file) is as follows. The root node of the file is the configuration node with
    multiple property nodes under it:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个静态块，从两个XML文件`hadoop-local.xml`和`mongo-defaults.xml`中加载一些配置。这些文件（或任何XML文件）的格式如下。文件的根节点是具有多个属性节点的配置节点：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The property values that make sense in this context are all those that we mentioned
    in the URL provided earlier. We instantiate `com.mongodb.hadoop.MongoConfig` wrapping
    an instance of `org.apache.hadoop.conf.Configuration` in the constructor of the
    bootstrap class, `TreasuryYieldXmlConfig`. The `MongoConfig` class provides sensible
    defaults, which are enough to satisfy majority of the use cases. Some of the most
    important things that we need to set in the `MongoConfig` instance are the output
    and input format, `mapper` and `reducer` classes, output key and value of the
    mapper, and output key and value of the reducer. The input format and output format
    will always be the `com.mongodb.hadoop.MongoInputFormat` and `com.mongodb.hadoop.MongoOutputFormat`
    classes, which are provided by the mongo-hadoop connector library. For the mapper
    and reducer output key and value, we have any of the `org.apache.hadoop.io.Writable`
    implementations. Refer to the Hadoop documentation for different types of the
    Writable implementations in the `org.apache.hadoop.io` package. Apart from these,
    the mongo-hadoop connector also provides us with some implementations in the `com.mongodb.hadoop.io`
    package. For the treasury yield example, we used the `BSONWritable` instance.
    These configurable values can either be provided in the XML file that we saw earlier
    or be programmatically set. Finally, we have the option to provide them as `vm`
    arguments that we did for `mongo.input.uri` and `mongo.output.uri`. These parameters
    can be provided either in the XML or invoked directly from the code on the `MongoConfig`
    instance; the two methods are `setInputURI` and `setOutputURI`, respectively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下有意义的属性值是我们之前提到的URL中提供的所有值。我们在引导类`TreasuryYieldXmlConfig`的构造函数中实例化`com.mongodb.hadoop.MongoConfig`，将`org.apache.hadoop.conf.Configuration`的实例包装起来。`MongoConfig`类提供了合理的默认值，这足以满足大多数用例。我们需要在`MongoConfig`实例中设置的一些最重要的事情是输出和输入格式、`mapper`和`reducer`类、mapper的输出键和值，以及reducer的输出键和值。输入格式和输出格式将始终是`com.mongodb.hadoop.MongoInputFormat`和`com.mongodb.hadoop.MongoOutputFormat`类，这些类由mongo-hadoop连接器库提供。对于mapper和reducer的输出键和值，我们有任何`org.apache.hadoop.io.Writable`实现。有关`org.apache.hadoop.io`包中不同类型的Writable实现，请参考Hadoop文档。除此之外，mongo-hadoop连接器还在`com.mongodb.hadoop.io`包中为我们提供了一些实现。对于国库收益率示例，我们使用了`BSONWritable`实例。这些可配置的值可以在之前看到的XML文件中提供，也可以以编程方式设置。最后，我们可以选择将它们作为`vm`参数提供，就像我们为`mongo.input.uri`和`mongo.output.uri`所做的那样。这些参数可以在XML中提供，也可以直接从代码中在`MongoConfig`实例上调用；这两种方法分别是`setInputURI`和`setOutputURI`。
- en: 'We will now look at the `mapper` and `reducer` class implementations. We will
    copy the important portion of the class here in order to analyze. Refer to the
    cloned project for the entire implementation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下`mapper`和`reducer`类的实现。我们将在这里复制类的重要部分以进行分析。有关整个实现，请参考克隆的项目：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our mapper extends the `org.apache.hadoop.mapreduce.Mapper` class. The four
    generic parameters are the key class, type of the input value, type of the output
    key, and output value. The body of the map method reads the `_id` value from the
    input document, which is the date, and extracts the year out of it. Then, it gets
    the double value from the document for the `bc10Year` field and simply writes
    to the context key-value pair where key is the year and value of the double to
    the context key value pair. The implementation here doesn't rely on the value
    of the `pKey` parameter passed, which can be used as the key instead of hardcoding
    the `_id` value in the implementation. This value is basically the same field
    that would be set using the `mongo.input.key` property in the XML or the `MongoConfig.setInputKey`
    method. If none is set, `_id` is the default value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的mapper扩展了`org.apache.hadoop.mapreduce.Mapper`类。四个通用参数是键类、输入值类型、输出键类型和输出值类型。map方法的主体从输入文档中读取`_id`值，即日期，并从中提取年份。然后，它从文档中获取`bc10Year`字段的双值，并简单地写入上下文键值对，其中键是年份，双值是上下文键值对的值。这里的实现不依赖于传递的`pKey`参数的值，可以使用该值作为键，而不是在实现中硬编码`_id`值。该值基本上是使用XML中的`mongo.input.key`属性或`MongoConfig.setInputKey`方法设置的相同字段。如果没有设置，`_id`是默认值。
- en: 'Let''s look at the reducer implementation (with the logging statements removed):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下reducer的实现（删除了日志记录语句）：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This class extends from `org.apache.hadoop.mapreduce.Reducer` and has four
    generic parameters: the input key, input value, output key, and output value.
    The input to the reducer is the output from the mapper, and thus, if you notice
    carefully, the type of the first two generic parameters is the same as the last
    two generic parameters of the mapper that we saw earlier. The third and fourth
    parameters are the type of the key and value emitted from the reduce. The type
    of the value is `BSONDocument` and thus we have `BSONWritable` as the type.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类扩展自`org.apache.hadoop.mapreduce.Reducer`，有四个通用参数：输入键、输入值、输出键和输出值。reducer的输入是mapper的输出，因此，如果你仔细观察，你会发现前两个通用参数的类型与我们之前看到的mapper的最后两个通用参数相同。第三和第四个参数是从reduce中发出的键和值的类型。值的类型是`BSONDocument`，因此我们有`BSONWritable`作为类型。
- en: 'We now have the reduce method that has two parameters: the first one is the
    key, which is the same as the key emitted from the map function, and the second
    parameter is `java.lang.Iterable` of the values emitted for the same key. This
    is how standard map reduce functions work. For instance, if the map function gave
    the following key value pairs, (1950, 10), (1960, 20), (1950, 20), (1950, 30),
    then reduce will be invoked with two unique keys, 1950 and 1960, and the values
    for the key 1950 will be `Iterable` with (10, 20, 30), whereas that of 1960 will
    be `Iterable` of a single element (20). The reducer''s reduce function simply
    iterates though `Iterable` of the doubles, finds the sum and count of these numbers,
    and writes one key value pair where the key is the same as the incoming key and
    the output value is `BasicBSONObject` with the sum, count, and average for the
    computed values.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了reduce方法，它有两个参数：第一个是键，与map函数发出的键相同，第二个参数是发出的相同键的值的`java.lang.Iterable`。这就是标准的map
    reduce函数的工作原理。例如，如果map函数给出以下键值对，(1950, 10), (1960, 20), (1950, 20), (1950, 30)，那么reduce将使用两个唯一的键1950和1960进行调用，并且键1950的值将是`Iterable`，包括(10,
    20, 30)，而1960的值将是单个元素(20)的`Iterable`。reducer的reduce函数简单地迭代双值的`Iterable`，找到这些数字的和与计数，并写入一个键值对，其中键与传入的键相同，输出值是`BasicBSONObject`，其中包括计算值的和、计数和平均值。
- en: There are some good samples including the Enron dataset in the examples of the
    cloned mongo-hadoop connector. If you would like to play around a bit, I would
    recommend that you take a look at these example projects and run them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在克隆的mongo-hadoop连接器示例中，包括Enron数据集在内有一些很好的示例。如果你想玩一下，我建议你看看这些示例项目并运行它们。
- en: There's more…
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: What we saw here is a readymade sample that we executed. There is nothing like
    writing one MapReduce job ourselves to clear our understanding. In the next recipe,
    we will write one sample MapReduce job using the Hadoop API in Java and see it
    in action.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的是一个现成的示例，我们执行了它。没有什么比自己编写一个MapReduce作业来澄清我们的理解更好。在下一个示例中，我们将使用Java中的Hadoop
    API编写一个MapReduce作业，并看到它的运行情况。
- en: See also…
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅…
- en: 'If you''re wondering what the `Writable` interface is all about and why you
    should not use plain old serialization instead, then refer to this URL that gives
    the explanation by the creator of Hadoop himself: [http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html](http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道`Writable`接口是什么，为什么不应该使用普通的旧序列化，那么请参考这个URL，由Hadoop的创建者解释：[http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html](http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00378.html)。
- en: Writing our first Hadoop MapReduce job
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写我们的第一个Hadoop MapReduce作业
- en: In this recipe, we will write our first MapReduce job using the Hadoop MapReduce
    API and run it using the mongo-hadoop connector getting the data from MongoDB.
    Refer to the *Executing MapReduce in Mongo using a Java client* recipe in [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* to see how MapReduce is implemented using a Java client, test data creation,
    and problem statement.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用Hadoop MapReduce API编写我们的第一个MapReduce作业，并使用mongo-hadoop连接器从MongoDB获取数据运行它。请参考[第3章](ch03.html
    "第3章。编程语言驱动程序")中的*使用Java客户端在Mongo中执行MapReduce*示例，了解如何使用Java客户端实现MapReduce、测试数据创建和问题陈述。
- en: Getting ready
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Refer to the previous *Executing our first sample MapReduce job using the mongo-hadoop
    connector* recipe to set up the mongo-hadoop connector. The prerequisites of this
    recipe and the *Executing MapReduce in Mongo using a Java client* recipe from
    [Chapter 3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming
    Language Drivers* are all that we need for this recipe. This is a maven project
    and thus maven needs to be set up and installed. Refer to the *Connecting to the
    Single node from a Java client* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server* where we provided
    the steps to set up maven in Windows; this project is built on Ubuntu Linux and
    the following is the command that you need to execute in the operating system
    shell to get maven:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考之前的*使用mongo-hadoop连接器执行我们的第一个样本MapReduce作业*食谱来设置mongo-hadoop连接器。此食谱的先决条件和[第3章](ch03.html
    "第3章。编程语言驱动程序")中的*使用Java客户端在Mongo中执行MapReduce*食谱是我们此食谱所需的全部内容。这是一个maven项目，因此需要设置和安装maven。请参考[第1章](ch01.html
    "第1章。安装和启动服务器")中的*从Java客户端连接到单节点*食谱，在那里我们提供了在Windows上设置maven的步骤；该项目是在Ubuntu Linux上构建的，以下是您需要在操作系统shell中执行的命令：
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How to do it…
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤如下...
- en: We have a Java `mongo-hadoop-mapreduce-test` project, which can be downloaded
    from the Packt website. The project is targeted to achieve the same use case that
    we achieved in the recipes in [Chapter 3](ch03.html "Chapter 3. Programming Language
    Drivers"), *Programming Language Drivers* where we used MongoDB's MapReduce framework.
    We had invoked that MapReduce job using the Python and Java client on previous
    occasions.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个Java `mongo-hadoop-mapreduce-test`项目，可以从Packt网站下载。该项目旨在实现我们在[第3章](ch03.html
    "第3章。编程语言驱动程序")中实现的用例，即在MongoDB的MapReduce框架中使用Python和Java客户端调用MapReduce作业。
- en: 'On the command prompt with the current directory in the root of the project,
    where the `pom.xml` file is present, execute the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目根目录中的当前目录中的命令提示符下，执行以下命令：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The JAR file, `mongo-hadoop-mapreduce-test-1.0.jar`, will be built and kept
    in the target directory.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: JAR文件`mongo-hadoop-mapreduce-test-1.0.jar`将被构建并保存在目标目录中。
- en: 'With the assumption that the CSV file is already imported to the `postalCodes`
    collection, execute the following command with the current directory still in
    the root of the `mongo-hadoop-mapreduce-test` project that we just built:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设CSV文件已经导入到`postalCodes`集合中，请在仍然位于我们刚构建的`mongo-hadoop-mapreduce-test`项目根目录中的当前目录中执行以下命令：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once the MapReduce job is completed, open the mongo shell by typing the following
    on the operating system command prompt and execute the following query in the
    shell:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MapReduce作业完成后，通过在操作系统命令提示符上键入以下内容打开mongo shell，并在shell中执行以下查询：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Compare the output to the one that we got earlier when we executed the MapReduce
    jobs using mongo's map reduce framework (in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出与我们之前使用mongo的map reduce框架执行MapReduce作业时获得的输出进行比较（在[第3章](ch03.html "第3章。编程语言驱动程序")中，*编程语言驱动程序*）。
- en: How it works…
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We have kept the classes very simple and with the bare minimum things that
    we need. We just have three classes in our project: `TopStateMapReduceEntrypoint`,
    `TopStateReducer`, and `TopStatesMapper`, all in the same `com.packtpub.mongo.cookbook`
    package. The mapper''s `map` function just writes a key value pair to the context,
    where the key is the name of the state and value is an integer value, one. The
    following is the code snippet from the `mapper` function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将类保持得非常简单，只包含我们需要的最少内容。我们的项目中只有三个类：`TopStateMapReduceEntrypoint`、`TopStateReducer`和`TopStatesMapper`，都在同一个`com.packtpub.mongo.cookbook`包中。mapper的`map`函数只是将键值对写入上下文，其中键是州的名称，值是整数值1。以下是来自`mapper`函数的代码片段：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'What the reducer gets is the same key that is the list of the states and Iterable
    of integer value, one. All that we do is write the same name of the state and
    sum of the iterables to the context. Now, as there is no size method in Iterable
    that can give the count in constant time, we are left with adding up all the ones
    that we get in linear time. The following is the code in the reducer method:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer获得的是相同的键，即州的列表和整数值，为1。我们所做的就是将相同州的名称和可迭代的总和写入上下文。现在，由于在Iterable中没有size方法可以在常数时间内给出计数，我们只能在线性时间内将所有得到的1相加。以下是reducer方法中的代码：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We write the text string that is the key and value that is the JSON document
    containing the count to the context. The mongo-hadoop connector is then responsible
    for writing the `postalCodesHadoopmrOut` document to the output collection that
    we have, with the `_id` field the same as the key emitted. Thus, when we execute
    the following, we get the top five states with the most number of cities in our
    database:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本字符串写入键，将包含计数的JSON文档写入上下文。然后，mongo-hadoop连接器负责将`postalCodesHadoopmrOut`文档写入我们拥有的输出集合，其中`_id`字段与发射的键相同。因此，当我们执行以下操作时，我们将获得数据库中拥有最多城市的前五个州：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, the main method of the main entry point class is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，主入口类的主方法如下：
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: All we do is wrap the `org.apache.hadoop.conf.Configuration` object with the
    `com.mongodb.hadoop.MongoConfig` instance to set various properties and then submit
    the MapReduce job for execution using ToolRunner.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的就是使用`com.mongodb.hadoop.MongoConfig`实例将`org.apache.hadoop.conf.Configuration`对象包装起来，以设置各种属性，然后使用ToolRunner提交MapReduce作业以执行。
- en: See also
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We executed a simple MapReduce job on Hadoop using the Hadoop API, sourcing
    the data from MongoDB, and writing the data to the MongoDB collection. What if
    we want to write the `map` and `reduce` functions in a different language? Fortunately,
    this is possible using a concept called Hadoop streaming where `stdout` is used
    as a means to communicate between the program and Hadoop MapReduce framework.
    In the next recipe, we will demonstrate how to use Python to implement the same
    use case that we did in this recipe using Hadoop streaming.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Hadoop API在Hadoop上执行了一个简单的MapReduce作业，从MongoDB获取数据，并将数据写入MongoDB集合。如果我们想要用不同的语言编写`map`和`reduce`函数怎么办？幸运的是，使用一个称为Hadoop
    streaming的概念是可能的，其中`stdout`用作程序和Hadoop MapReduce框架之间的通信手段。在下一个示例中，我们将演示如何使用Python来实现与本示例中相同的用例，使用Hadoop
    streaming。
- en: Running MapReduce jobs on Hadoop using streaming
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流式传输在Hadoop上运行MapReduce作业
- en: In our previous recipe, we implemented a simple MapReduce job using the Java
    API of Hadoop. The use case was the same as what we did in the recipes in [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* where we implemented MapReduce using the Mongo client APIs in Python
    and Java. In this recipe, we will use Hadoop streaming to implement MapReduce
    jobs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中，我们使用Hadoop的Java API实现了一个简单的MapReduce作业。用例与我们在[第3章](ch03.html "第3章。编程语言驱动程序")的示例中使用Python和Java中的Mongo客户端API实现MapReduce相同。在这个示例中，我们将使用Hadoop
    streaming来实现MapReduce作业。
- en: The concept of streaming works on communication using `stdin` and `stdout`.
    You can get more information on Hadoop streaming and how it works at [http://hadoop.apache.org/docs/r1.2.1/streaming.html](http://hadoop.apache.org/docs/r1.2.1/streaming.html).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 流式传输的概念是基于使用`stdin`和`stdout`进行通信。您可以在[http://hadoop.apache.org/docs/r1.2.1/streaming.html](http://hadoop.apache.org/docs/r1.2.1/streaming.html)上获取有关Hadoop
    streaming及其工作原理的更多信息。
- en: Getting ready…
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作…
- en: Refer to the *Executing our first sample MapReduce job using the mongo-hadoop
    connector* recipe in this chapter to see how to set up Hadoop for development
    purposes and build the mongo-hadoop project using Gradle. As far as the Python
    libraries are concerned, we will be installing the required library from the source;
    however, you can use `pip` (Python's package manager) to set up if you do not
    wish to build from the source. We will also see how to set up pymongo-hadoop using
    `pip`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章中的*使用mongo-hadoop连接器执行我们的第一个示例MapReduce作业*示例，了解如何为开发目的设置Hadoop并使用Gradle构建mongo-hadoop项目。就Python库而言，我们将从源代码安装所需的库；但是，如果您不希望从源代码构建，可以使用`pip`（Python的软件包管理器）进行设置。我们还将看到如何使用`pip`设置pymongo-hadoop。
- en: Refer to recipe *Connecting to a single node using a Python client*, in [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* on how to install PyMongo for your host operating system.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[第1章](ch01.html "第1章。安装和启动服务器")中的*使用Python客户端连接到单个节点*示例，了解如何为您的主机操作系统安装PyMongo。
- en: How it works…
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'We will first build pymongo–hadoop from the source. With the project cloned
    to the local filesystem, execute the following in the root of the cloned project:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从源代码构建pymongo-hadoop。将项目克隆到本地文件系统后，在克隆项目的根目录中执行以下操作：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: After you enter the password, the setup will continue to be installed on pymongo-hadoop
    on your machine.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入密码后，设置将继续在您的计算机上安装pymongo-hadoop。
- en: 'This is all we need to do to build pymongo-hadoop from the source. However,
    if you had chosen not to build from the source, you can execute the following
    command in the operating system shell:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是我们需要从源代码构建pymongo-hadoop的全部内容。但是，如果您选择不从源代码构建，可以在操作系统shell中执行以下命令：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After installing pymongo-hadoop in either way, we will now implement our `mapper`
    and `reducer` function in Python. The `mapper` function is as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以任何方式安装pymongo-hadoop后，我们将在Python中实现我们的`mapper`和`reducer`函数。`mapper`函数如下：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now for the `reducer` function, which will look like the following:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是`reducer`函数，将如下所示：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The environment variables, `$HADOOP_HOME` and `$HADOOP_CONNECTOR_HOME`, should
    point to the base directory of Hadoop and the mongo-hadoop connector project,
    respectively. Now, we will invoke the `MapReduce` function using the following
    command in the operating system shell. The code available with the book on the
    Packt website has the `mapper`, `reduce` Python script, and shell script that
    will be used to invoke the `mapper` and `reducer` function:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境变量`$HADOOP_HOME`和`$HADOOP_CONNECTOR_HOME`应该分别指向Hadoop和mongo-hadoop连接器项目的基本目录。现在，我们将在操作系统shell中使用以下命令调用`MapReduce`函数。书中提供的代码在Packt网站上有`mapper`，`reduce`
    Python脚本和shell脚本，将用于调用`mapper`和`reducer`函数：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `mapper.py` and `reducer.py` files are present in the current directory
    when executing this command.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行此命令时，`mapper.py`和`reducer.py`文件位于当前目录中。
- en: 'On executing the command, which should take some time for the successful execution
    of the MapReduce job, open the mongo shell by typing the following command on
    the operating system command prompt and execute the following query from the shell:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行该命令时，应该需要一些时间来成功执行MapReduce作业，在操作系统命令提示符上键入以下命令打开mongo shell，并从shell执行以下查询：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Compare the output to the one that we got earlier when we executed the MapReduce
    jobs using mongo's MapReduce framework in [Chapter 3](ch03.html "Chapter 3. Programming
    Language Drivers"), *Programming Language Drivers*.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出与我们之前在[第3章](ch03.html "第3章。编程语言驱动程序")中使用mongo的MapReduce框架执行MapReduce作业时获得的输出进行比较，*编程语言驱动程序*。
- en: How to do it…
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: Let's look at steps 5 and 6 where we write the `mapper` and `reducer` functions.
    We define a `map` function that accepts a list of all the documents. We iterate
    through these and yield documents, where the `_id` field is the name of the key
    and the count value field has a value of one. There will be the same number of
    documents yielded as the total number of input documents.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下步骤5和6，我们编写`mapper`和`reducer`函数。我们定义了一个接受所有文档列表的`map`函数。我们遍历这些文档，并产生文档，其中`_id`字段是键的名称，计数值字段的值为1。产生的文档数量将与输入文档的总数相同。
- en: We instantiate `BSONMapper` finally, which accepts the `mapper` function as
    the parameter. The function returns a generator object, which is then used by
    this `BSONMapper` class to feed the value to the MapReduce framework. All we need
    to remember is that the `mapper` function needs to return a generator (which is
    returned as we call yield in the loop) and then instantiate the `BSONMapper` class,
    which is provided to us by the `pymongo_hadoop` module. For the intrigued, you
    can choose to look at the source code under the project cloned on our local filesystem
    in the `streaming/language_support/python/pymongo_hadoop/mapper.py` file and see
    what it does. It is a small and simple-to-understand piece of code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实例化了`BSONMapper`，它接受`mapper`函数作为参数。该函数返回一个生成器对象，然后该`BSONMapper`类使用它来向MapReduce框架提供值。我们需要记住的是，`mapper`函数需要返回一个生成器（在循环中调用`yield`时返回），然后实例化`BSONMapper`类，这是由`pymongo_hadoop`模块提供给我们的。如果你感兴趣，你可以选择查看我们本地文件系统中克隆的项目中的`streaming/language_support/python/pymongo_hadoop/mapper.py`文件的源代码，看看它是做什么的。这是一段小而简单易懂的代码。
- en: For the `reducer` function, we get the key and list of documents for this key
    as the value. The key is the same as the value of the `_id` field emitted from
    the document in the `map` function. We simply return a new document here with
    `_id` as the name of the state and count is the number of documents for this state.
    Remember that we return a document and not emit one as we did in map. Finally,
    we instantiate `BSONReducer` and pass the `reducer` function. The source code
    under the project cloned on our local filesystem in the `streaming/language_support/python/pymongo_hadoop/reducer.py`
    file has the implementation of the `BSONReducer` class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`reducer`函数，我们得到了键和该键对应的文档列表作为值。键与`map`函数中发出的文档的`_id`字段的值相同。我们在这里简单地返回一个新文档，其中`_id`是州的名称，计数是该州的文档数。记住，我们返回一个文档，而不是像在map中那样发出一个文档。最后，我们实例化`BSONReducer`并传递`reducer`函数。在我们本地文件系统中克隆的项目中的`streaming/language_support/python/pymongo_hadoop/reducer.py`文件中有`BSONReducer`类的实现。
- en: 'We finally invoked the command in the shell to initiate the MapReduce job that
    uses streaming. A few things to note here are that we need two JAR files: one
    in `share/hadoop/tools/lib` of the Hadoop distribution and one in the mongo-hadoop
    connector, which is present in the `streaming/build/libs/` directory. The input
    and output formats are `com.mongodb.hadoop.mapred.MongoInputFormat` and `com.mongodb.hadoop.mapred.MongoOutputFormat`,
    respectively.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在shell中调用命令来启动使用流处理的MapReduce作业。这里需要注意的几点是，我们需要两个JAR文件：一个在Hadoop分发的`share/hadoop/tools/lib`目录中，另一个在mongo-hadoop连接器中，位于`streaming/build/libs/`目录中。输入和输出格式分别是`com.mongodb.hadoop.mapred.MongoInputFormat`和`com.mongodb.hadoop.mapred.MongoOutputFormat`。
- en: As we saw earlier, `sysout` and `sysin` forms the backbone of streaming. So,
    basically, we need to encode our BSON objects to write to `sysout`, and then,
    we should be able to read `sysin` to convert the content to the BSON objects again.
    For this purpose, the mongo-hadoop connector provides us with two framework classes,
    `com.mongodb.hadoop.streaming.io.MongoInputWriter` and `com.mongodb.hadoop.streaming.io.MongoOutputReader`
    to encode and decode from and to the BSON objects. These classes extend from `org.apache.hadoop.streaming.io.InputWriter`
    and `org.apache.hadoop.streaming.io.OutputReader`, respectively.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，`sysout`和`sysin`构成了流处理的基础。所以，基本上，我们需要对我们的BSON对象进行编码以写入`sysout`，然后，我们应该能够读取`sysin`以将内容再次转换为BSON对象。为此，mongo-hadoop连接器为我们提供了两个框架类，`com.mongodb.hadoop.streaming.io.MongoInputWriter`和`com.mongodb.hadoop.streaming.io.MongoOutputReader`，用于对BSON对象进行编码和解码。这些类分别扩展自`org.apache.hadoop.streaming.io.InputWriter`和`org.apache.hadoop.streaming.io.OutputReader`。
- en: The value of the `stream.io.identifier.resolver.class` property is given as
    `com.mongodb.hadoop.streaming.io.MongoIdentifierResolver`. This class extends
    from `org.apache.hadoop.streaming.io.IdentifierResolver` and gives us a chance
    to register our implementations of `org.apache.hadoop.streaming.io.InputWriter`
    and `org.apache.hadoop.streaming.io.OutputReader` with the framework. We also
    register the output key and output value class using our custom `IdentifierResolver`.
    Just remember to use this resolver always in case you are using streaming with
    the mongo-hadoop connector.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream.io.identifier.resolver.class`属性的值是`com.mongodb.hadoop.streaming.io.MongoIdentifierResolver`。这个类继承自`org.apache.hadoop.streaming.io.IdentifierResolver`，并且让我们有机会注册我们的`org.apache.hadoop.streaming.io.InputWriter`和`org.apache.hadoop.streaming.io.OutputReader`的实现到框架中。我们还使用我们自定义的`IdentifierResolver`注册输出键和输出值类。只要记住，如果你正在使用mongo-hadoop连接器进行流处理，一定要始终使用这个解析器。'
- en: We finally execute the `mapper` and `reducer` python functions, which we discussed
    earlier. An important thing to remember is that do not print out logs to `sysout`
    from the `mapper` and `reducer` functions. The `sysout` and `sysin` mapper and
    reducer are the means of communication, and writing logs to it can yield undesirable
    behavior. As we can see in the example, write either to standard error (`stderr`)
    or a log file.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终执行了之前讨论过的`mapper`和`reducer`的Python函数。要记住的一件重要的事情是，不要从`mapper`和`reducer`函数中向`sysout`打印日志。`sysout`和`sysin`的mapper和reducer是通信的手段，向其中写入日志可能会产生不良行为。正如我们在示例中看到的，要么写入标准错误（`stderr`），要么写入日志文件。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When using a multiline command in Unix, you continue the command on the next
    line using `\`. However, remember not to have spaces after `\`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Unix中使用多行命令时，可以使用`\`在下一行继续命令。但是，记住在`\`后面不要有空格。
- en: Running a MapReduce job on Amazon EMR
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon EMR上运行MapReduce作业
- en: 'This recipe involves running the MapReduce job on the cloud using AWS. You
    will need an AWS account in order to proceed. Register with AWS at [http://aws.amazon.com/](http://aws.amazon.com/).
    We will see how to run a MapReduce job on the cloud using **Amazon Elastic Map
    Reduce** (**Amazon EMR**). Amazon EMR is a managed MapReduce service provided
    by Amazon on the cloud. Refer to [https://aws.amazon.com/elasticmapreduce/](https://aws.amazon.com/elasticmapreduce/)
    for more details. Amazon EMR consumes data, binaries/JARs, and so on from AWS
    S3 bucket, processes them and writes the results back to S3 bucket. **Amazon Simple
    Storage Service** (**Amazon S3**) is another service by AWS for data storage on
    the cloud. Refer to [http://aws.amazon.com/s3/](http://aws.amazon.com/s3/) for
    more details on Amazon S3\. Though we will use the mongo-hadoop connector, an
    interesting fact is that we won''t require a MongoDB instance to be up and running.
    We will use the MongoDB data dump stored in an S3 bucket for our data analysis.
    The MapReduce program will run on the input BSON dump and generate the result
    BSON dump in the output bucket. The logs of the MapReduce program will be written
    to another bucket dedicated for logs. The following figure gives us an idea of
    how our setup would look at a high level:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程涉及在AWS上使用云来运行MapReduce作业。您需要一个AWS账户才能继续。在[http://aws.amazon.com/](http://aws.amazon.com/)注册AWS。我们将看到如何在云上使用Amazon
    Elastic Map Reduce (Amazon EMR)运行MapReduce作业。Amazon EMR是亚马逊在云上提供的托管MapReduce服务。更多详情请参考[https://aws.amazon.com/elasticmapreduce/](https://aws.amazon.com/elasticmapreduce/)。Amazon
    EMR从AWS S3存储桶中获取数据、二进制文件/JAR等，处理它们并将结果写回S3存储桶。Amazon Simple Storage Service (Amazon
    S3)是AWS提供的另一个用于云上数据存储的服务。更多关于Amazon S3的详情请参考[http://aws.amazon.com/s3/](http://aws.amazon.com/s3/)。虽然我们将使用mongo-hadoop连接器，有趣的是我们不需要一个MongoDB实例在运行。我们将使用存储在S3存储桶中的MongoDB数据转储进行数据分析。MapReduce程序将在输入的BSON转储上运行，并在输出存储桶中生成结果BSON转储。MapReduce程序的日志将被写入另一个专门用于日志的存储桶。下图给出了我们的设置在高层次上的样子：
- en: '![Running a MapReduce job on Amazon EMR](img/B04831_08_12.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: 在Amazon EMR上运行MapReduce作业
- en: Getting ready
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the same Java sample as we did in the *Writing our first Hadoop
    MapReduce job* recipe for this recipe. To know more about the `mapper` and `reducer`
    class implementation, you can refer to the *How It works* section of the same
    recipe. We have a `mongo-hadoop-emr-test` project available with the code that
    can be downloaded from the Packt website, which is used to create a MapReduce
    job on the cloud using the AWS EMR APIs. To simplify things, we will upload just
    one JAR to the S3 bucket to execute the MapReduce job. This JAR will be assembled
    using a BAT file for Windows and a shell script on Unix-based operating systems.
    The `mongo-hadoop-emr-test` Java project has the `mongo-hadoop-emr-binaries` subdirectory
    containing the necessary binaries along with the scripts to assemble them in one
    JAR.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与*编写我们的第一个Hadoop MapReduce作业*教程相同的Java示例。要了解更多关于`mapper`和`reducer`类实现的信息，您可以参考同一教程的*它是如何工作的*部分。我们有一个`mongo-hadoop-emr-test`项目，其中包含可以从Packt网站下载的代码，用于使用AWS
    EMR API在云上创建MapReduce作业。为了简化事情，我们将只上传一个JAR到S3存储桶来执行MapReduce作业。这个JAR将使用BAT文件在Windows上组装，使用Unix操作系统上的shell脚本。`mongo-hadoop-emr-test`Java项目有一个`mongo-hadoop-emr-binaries`子目录，其中包含必要的二进制文件以及将它们组装成一个JAR的脚本。
- en: The assembled `mongo-hadoop-emr-assembly.jar` file is also provided in the subdirectory.
    Running the `.bat` or `.sh` file will delete this JAR and regenerate the assembled
    JAR, which is not mandatory. The already provided assembled JAR is good enough
    and will work just fine. The Java project contains subdirectory data with a `postalCodes.bson`
    file in it. This is the BSON dump generated out of the database containing the
    `postalCodes` collection. The `mongodump` utility provided with the mongo distribution
    is used to extract this dump.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 已组装的`mongo-hadoop-emr-assembly.jar`文件也提供在子目录中。运行`.bat`或`.sh`文件将删除这个JAR并重新生成已组装的JAR，这并不是必需的。已提供的已组装的JAR足够好，可以正常工作。Java项目包含一个`data`子目录，其中包含一个`postalCodes.bson`文件。这是从包含`postalCodes`集合的数据库中生成的BSON转储。mongo分发提供的`mongodump`实用程序用于提取这个转储。
- en: How to do it…
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The first step of this exercise is to create a bucket on S3\. You can choose
    to use an existing bucket; however, for this recipe, I am creating a `com.packtpub.mongo.cookbook.emr-in`
    bucket. Remember that the name of the bucket has to be unique across all the S3
    buckets and you will not be able to create a bucket with this very name. You will
    have to create one with a different name and use it in place of `com.packtpub.mongo.cookbook.emr-in`
    that is used in this recipe.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个练习的第一步是在S3上创建一个存储桶。您可以选择使用现有的存储桶；但是，对于这个教程，我创建了一个`com.packtpub.mongo.cookbook.emr-in`存储桶。请记住，存储桶的名称必须在所有S3存储桶中是唯一的，您将无法创建一个具有相同名称的存储桶。您将不得不创建一个不同名称的存储桶，并在这个教程中使用它来代替`com.packtpub.mongo.cookbook.emr-in`。
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Do not create bucket names with an underscore (`_`); instead, use a hyphen (`-`).
    The bucket creation with an underscore will not fail; however, the MapReduce job
    later will fail as it doesn't accept underscores in the bucket names.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用下划线(`_`)创建存储桶名称；而是使用连字符(`-`)。使用下划线创建存储桶名称不会失败；但是后来的MapReduce作业会失败，因为它不接受存储桶名称中的下划线。
- en: We will upload the assembled JAR files and a `.bson` file for the data to the
    newly created (or existing) S3 bucket. To upload the files, we will use the AWS
    web console. Click on the **Upload** button and select the assembled JAR file
    and the `postalCodes.bson` file to be uploaded to the S3 bucket. After uploading,
    the contents of the bucket should look as follows:![How to do it…](img/B04831_08_01.jpg)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将上传已组装的JAR文件和一个`.bson`文件到新创建（或现有）的S3存储桶。要上传文件，我们将使用AWS网络控制台。点击**上传**按钮，选择已组装的JAR文件和`postalCodes.bson`文件上传到S3存储桶。上传后，存储桶的内容应该如下所示：![如何操作...](img/B04831_08_01.jpg)
- en: The following steps are to initiate the EMR job from the AWS console without
    writing a single line of code. We will also see how to initiate this using AWS
    Java SDK. Follow steps 4 to 9 if you are looking to initiate the EMR job from
    the AWS console. Follow steps 10 and 11 to start the EMR job using the Java SDK.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的步骤是从AWS控制台启动EMR作业，而不需要编写一行代码。我们还将看到如何使用AWS Java SDK启动此作业。如果您希望从AWS控制台启动EMR作业，请按照步骤4到9进行。如果要使用Java
    SDK启动EMR作业，请按照步骤10和11进行。
- en: We will first initiate a MapReduce job from the AWS console. Visit [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)
    and click on the **Create Cluster** button. In the **Cluster Configuration** screen,
    enter the details as shown in the image, except for the logging bucket, which
    you need to select as your bucket to which the logs need to be written. You can
    also click on the folder icon next to the textbox for the bucket name and select
    the bucket present for your account to be used as the logging bucket.![How to
    do it…](img/B04831_08_07.jpg)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从AWS控制台启动一个MapReduce作业。访问[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)并点击**创建集群**按钮。在**集群配置**屏幕中，输入图中显示的细节，除了日志桶，您需要选择作为日志需要写入的桶。您还可以点击文本框旁边的文件夹图标，选择您的帐户中存在的桶作为日志桶。![操作步骤…](img/B04831_08_07.jpg)
- en: Note
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The termination protection option is set to **No** as this is a test instance.
    In case of any error, we would rather want the instances to terminate in order
    to avoid keeping them running and incurring charges.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 终止保护选项设置为**否**，因为这是一个测试实例。如果出现任何错误，我们宁愿希望实例终止，以避免保持运行并产生费用。
- en: In the **Software Configuration** section, select the **Hadoop version** as
    **2.4.0** and **AMI version** as **3.1.0 (hadoop 2.4.0)**. Remove the additional
    applications by clicking on the cross next to their names, as shown in the following
    image:![How to do it…](img/B04831_08_08.jpg)
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**软件配置**部分，选择**Hadoop版本**为**2.4.0**，**AMI版本**为**3.1.0 (hadoop 2.4.0)**。通过点击其名称旁边的叉号来移除额外的应用程序，如下图所示：![操作步骤…](img/B04831_08_08.jpg)
- en: In the **Hardware Configuration** section, select the **EC2 instance type**
    as **m1.medium**. This is the minimum that we need to select for the Hadoop version
    2.4.0\. The number of instances for the slave and task instances is zero. The
    following image shows the configuration that is selected:![How to do it…](img/B04831_08_09.jpg)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**硬件配置**部分，选择**EC2实例类型**为**m1.medium**。这是我们需要为Hadoop版本2.4.0选择的最低配置。从下图中可以看到所选择的从属和任务实例的数量为零：![操作步骤…](img/B04831_08_09.jpg)
- en: In the **Security and Access** section, leave all the default values. We also
    have no need for a **Bootstrap Action**, so leave this as well.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**安全和访问**部分，保留所有默认值。我们也不需要**引导操作**，所以也保持不变。
- en: The final step is to set up **Steps** for the MapReduce job. In the **Add step**
    drop down, select the **Custom JAR** option, and then select the **Auto-terminate**
    option as **Yes**, as shown in the following image:![How to do it…](img/B04831_08_10.jpg)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是为MapReduce作业设置**步骤**。在**添加步骤**下拉菜单中，选择**自定义JAR**选项，然后选择**自动终止**选项为**是**，如下图所示：![操作步骤…](img/B04831_08_10.jpg)
- en: Now click on the **Configure** and **Add** button and enter the details.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在点击**配置**和**添加**按钮并输入细节。
- en: The value of the **JAR S3 Location** is given as `s3://com.packtpub.mongo.cookbook.emr-in/mongo-hadoop-emr-assembly.jar`.
    This is the location in my input bucket; you need to change the input bucket as
    per your own input bucket. The name of the JAR file would be same.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**JAR S3位置**的值为`s3://com.packtpub.mongo.cookbook.emr-in/mongo-hadoop-emr-assembly.jar`。这是我输入桶中的位置；您需要根据自己的输入桶更改输入桶。JAR文件的名称将保持不变。'
- en: 'Enter the following arguments in the **Arguments** text area; the name of the
    main class is the first in the list:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在**参数**文本区域中输入以下参数；主类的名称在列表中排在第一位：
- en: '`com.packtpub.mongo.cookbook.TopStateMapReduceEntrypoint`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`com.packtpub.mongo.cookbook.TopStateMapReduceEntrypoint`'
- en: '`-Dmongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat`'
- en: '`-Dmongo.job.mapper=com.packtpub.mongo.cookbook.TopStatesMapper`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.mapper=com.packtpub.mongo.cookbook.TopStatesMapper`'
- en: '`-Dmongo.job.reducer=com.packtpub.mongo.cookbook.TopStateReducer`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: -Dmongo.job.reducer=com.packtpub.mongo.cookbook.TopStateReducer
- en: '`-Dmongo.job.output=org.apache.hadoop.io.Text`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.output=org.apache.hadoop.io.Text`'
- en: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
- en: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.output.value=org.apache.hadoop.io.IntWritable`'
- en: '`-Dmongo.job.output.format=com.mongodb.hadoop.BSONFileOutputFormat`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmongo.job.output.format=com.mongodb.hadoop.BSONFileOutputFormat`'
- en: '`-Dmapred.input.dir=s3://com.packtpub.mongo.cookbook.emr-in/postalCodes.bson`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmapred.input.dir=s3://com.packtpub.mongo.cookbook.emr-in/postalCodes.bson`'
- en: '`-Dmapred.output.dir=s3://com.packtpub.mongo.cookbook.emr-out/`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dmapred.output.dir=s3://com.packtpub.mongo.cookbook.emr-out/`'
- en: The value of the final two arguments contains the input and output bucket used
    for my MapReduce sample; this value will change according to your own input and
    output buckets. The value of Action on failure would be Terminate. The following
    image shows the values filled in; click on **Save** after all these details have
    been entered:![How to do it…](img/B04831_08_11.jpg)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后两个参数的值包含了我MapReduce样本使用的输入和输出桶；这个值将根据您自己的输入和输出桶而改变。失败时的操作值将为终止。在填写完所有这些细节后，点击**保存**：![操作步骤…](img/B04831_08_11.jpg)
- en: Now click on the **Create Cluster** button. This will take some time to provision
    and start the cluster.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在点击**创建集群**按钮。这将需要一些时间来配置和启动集群。
- en: In the following few steps, we will create a MapReduce job on EMR using the
    AWS Java API. Import the `EMRTest` project provided with the code samples to your
    favorite IDE. Once imported, open the `com.packtpub.mongo.cookbook.AWSElasticMapReduceEntrypoint`
    class.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的几步中，我们将使用AWS Java API在EMR上创建一个MapReduce作业。将提供的代码示例中的`EMRTest`项目导入到您喜欢的IDE中。导入后，打开`com.packtpub.mongo.cookbook.AWSElasticMapReduceEntrypoint`类。
- en: There are five constants that need to be changed in the class. They are the
    Input, Output, and Log bucket that you will use for your example and the AWS access
    and secret key. The access key and secret key act as the username and password
    when you use AWS SDK. Change these values accordingly and run the program. On
    successful execution, it should give you a job ID for the newly initiated job.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类中有五个常量需要更改。它们是您将用于示例的输入、输出和日志存储桶，以及AWS访问密钥和秘密密钥。访问密钥和秘密密钥在您使用AWS SDK时充当用户名和密码。相应地更改这些值并运行程序。成功执行后，它应该为您新启动的作业提供一个作业ID。
- en: Irrespective of how you initiated the EMR job, visit the EMR console at [https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)
    to see the status of your submitted ID. The Job ID that you can see in the second
    column of your initiated job will be same as the job ID printed to the console
    when you executed the Java program (if you initiated using the Java program).
    Click on the name of the job initiated, which should direct you to the job details
    page. The hardware provisioning will take some time, and then finally, your map
    reduce step will run. Once the job has been completed, the status of the job should
    look as follows on the Job details screen:![How to do it…](img/B04831_08_04.jpg)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无论您如何启动EMR作业，请访问EMR控制台[https://console.aws.amazon.com/elasticmapreduce/](https://console.aws.amazon.com/elasticmapreduce/)以查看您提交的ID的状态。您在启动的作业的第二列中可以看到作业ID，它将与您执行Java程序时在控制台上打印的作业ID相同（如果您使用Java程序启动）。单击启动的作业的名称，这应该将您引导到作业详细信息页面。硬件配置将需要一些时间，然后最终，您的MapReduce步骤将运行。作业完成后，作业的状态应在作业详细信息屏幕上如下所示:![操作方法…](img/B04831_08_04.jpg)
- en: 'When expanded, the **Steps** section should look as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 展开后，**步骤**部分应如下所示：
- en: '![How to do it…](img/B04831_08_05.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![操作方法…](img/B04831_08_05.jpg)'
- en: Click on the stderr link below the Log files section to view all the logs' output
    for the MapReduce job.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击日志文件部分下方的stderr链接，以查看MapReduce作业的所有日志输出。
- en: Now that the MapReduce job is complete, our next step is to see the results
    of it. Visit the S3 console at [https://console.aws.amazon.com/s3](https://console.aws.amazon.com/s3)
    and visit the output bucket. In my case, the following is the content of the out
    bucket:![How to do it…](img/B04831_08_06.jpg)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在MapReduce作业已完成，我们的下一步是查看其结果。访问S3控制台[https://console.aws.amazon.com/s3](https://console.aws.amazon.com/s3)并访问输出存储桶。在我的情况下，以下是输出存储桶的内容:![操作方法…](img/B04831_08_06.jpg)
- en: The `part-r-0000.bson` file is of our interest. This file contains the results
    of our MapReduce job.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`part-r-0000.bson`文件是我们感兴趣的。这个文件包含了我们的MapReduce作业的结果。'
- en: 'Download the file to your local filesystem and import to a running mongo instance
    locally, using the mongorestore utility. Note that the restore utility for the
    following command expects a mongod instance to be up and running and listening
    to port `27017` with the `part-r-0000.bson` file in the current directory:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件下载到本地文件系统，并使用mongorestore实用程序导入到本地运行的mongo实例中。请注意，以下命令的还原实用程序期望mongod实例正在运行并侦听端口`27017`，并且当前目录中有`part-r-0000.bson`文件：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, connect to the `mongod` instance using the mongo shell and execute the
    following query:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用mongo shell连接到`mongod`实例并执行以下查询：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will see the following results for the query:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询，我们将看到以下结果：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is the expected result for the top five results. If we compare the results
    that we got in *Executing MapReduce in Mongo using a Java client* from [Chapter
    3](ch03.html "Chapter 3. Programming Language Drivers"), *Programming Language
    Drivers* using Mongo's MapReduce framework and the *Writing our first Hadoop MapReduce
    job* recipe in this chapter, we can see that the results are identical.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是前五个结果的预期结果。如果我们比较在*在Java客户端中执行Mongo的MapReduce*中得到的结果，来自[第3章](ch03.html "第3章。编程语言驱动程序")的*编程语言驱动程序*，使用Mongo的MapReduce框架和本章中的*编写我们的第一个Hadoop
    MapReduce作业*配方，我们可以看到结果是相同的。
- en: How it works…
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: Amazon EMR is a managed Hadoop service that takes care of the hardware provisioning
    and keeps you away from the hassle of setting up your own cluster. The concepts
    related to our MapReduce program have already been covered in the *Writing our
    first Hadoop MapReduce job* recipe and there is nothing more to mention. One thing
    that we did was to assemble the JARs that we need in one big fat JAR to execute
    our MapReduce job. This approach is okay for our small MapReduce job; in case
    of larger jobs where a lot of third-party JARs are needed, we will have to go
    for an approach where we will add the JARs to the `lib` directory of the Hadoop
    installation and execute in the same way as we did in our MapReduce job that we
    executed locally. Another thing that we did differently from our local setup was
    not to use a `mongid` instance to source the data and write the data to, but instead,
    we used the BSON dump files from the mongo database as an input and wrote the
    output to the BSON files. The output dump will then be imported to a mongo database
    locally and the results will be analyzed. It is pretty common to have the data
    dumps uploaded to S3 buckets, and running analytics jobs on this data that has
    been uploaded to S3 on the cloud using cloud infrastructure is a good option.
    The data accessed from the buckets by the EMR cluster need not have public access
    as the EMR job runs using our account's credentials; we are good to access our
    own buckets to read and write data/logs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EMR是一项托管的Hadoop服务，负责硬件配置，并让您远离设置自己的集群的麻烦。与我们的MapReduce程序相关的概念已经在“编写我们的第一个Hadoop
    MapReduce作业”一文中进行了介绍，没有更多要提到的了。我们所做的一件事是将我们需要的JAR文件组装成一个大的JAR文件来执行我们的MapReduce作业。这种方法对于我们的小型MapReduce作业来说是可以的；对于需要大量第三方JAR文件的大型作业，我们将不得不采用一种方法，将JAR文件添加到Hadoop安装的`lib`目录中，并以与我们在本地执行的MapReduce作业相同的方式执行。我们与本地设置不同的另一件事是不使用`mongid`实例来获取数据和写入数据，而是使用mongo数据库中的BSON转储文件作为输入，并将输出写入BSON文件。然后将输出转储导入到本地mongo数据库，并对结果进行分析。将数据转储上传到S3存储桶并在云上使用云基础设施对已上传到S3的数据运行分析作业是一个不错的选择。EMR集群从存储桶访问的数据不需要公共访问权限，因为EMR作业使用我们账户的凭据运行；我们可以访问我们自己的存储桶来读取和写入数据/日志。
- en: See also
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: After trying out this simple MapReduce job, it is highly recommended that you
    get to know about the Amazon EMR service and all its features. The developer's
    guide for EMR can be found at [http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试了这个简单的MapReduce作业之后，强烈建议您了解亚马逊EMR服务及其所有功能。EMR的开发人员指南可以在[http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/)找到。
- en: There is a sample MapReduce job in the Enron dataset given as part of the mongo-hadoop
    connector's examples. It can be found at [https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce](https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce).
    You can choose to implement this example as well on Amazon EMR as per the given
    instructions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Enron数据集中提供了mongo-hadoop连接器示例中的一个MapReduce作业。它可以在[https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce](https://github.com/mongodb/mongo-hadoop/tree/master/examples/elastic-mapreduce)找到。您也可以选择根据给定的说明在亚马逊EMR上实现此示例。
