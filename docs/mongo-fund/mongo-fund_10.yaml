- en: 10\. Replication
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10. 复制
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will introduce MongoDB cluster concepts and administration. It
    starts with a discussion on the concepts of high availability and the load sharing
    of a MongoDB database. You will configure and install MongoDB replica sets in
    different environments, manage and monitor MongoDB replica set clusters, and practice
    cluster switchover and failover steps. You will explore high-availability clusters
    in MongoDB and connect to a MongoDB cluster to perform typical administration
    tasks on MongoDB cluster deployments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍MongoDB集群的概念和管理。它从讨论高可用性的概念和MongoDB数据库的负载共享开始。您将在不同环境中配置和安装MongoDB副本集，管理和监控MongoDB副本集群，并练习集群切换和故障转移步骤。您将探索MongoDB中的高可用性集群，并连接到MongoDB集群以执行MongoDB集群部署的典型管理任务。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: From a MongoDB developer perspective, it is probably true that the MongoDB database
    server is some sort of black box, living somewhere in the cloud or in a data center
    room. Details are not important if the database is up and running when needed.
    From a business perspective though, things look slightly different. For example,
    when a production application needs to be available online for customers 24/7,
    those details are very important. Any outage can have a negative impact on service
    availability for customers, and ultimately, if the failure is not recovered quickly,
    the business' financial results.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB开发人员的角度来看，MongoDB数据库服务器可能是某种黑匣子，在云端或数据中心的机房中。如果数据库在需要时处于运行状态，细节并不重要。但从商业角度来看，情况略有不同。例如，当生产应用程序需要24/7在线为客户提供服务时，这些细节就非常重要。任何中断都可能对客户的服务可用性产生负面影响，最终，如果故障不能迅速恢复，将影响业务的财务结果。
- en: Outages happen from time to time, and they can be attributed to a wide variety
    of reasons. These are often the result of common hardware failures, such as disk
    or memory failures, but they may also be caused by network failures, software
    failures, or even application failures. For example, a software failure such as
    an OS bug can render the server unresponsive to users and applications. Outages
    can also be caused by disasters such as flooding and earthquakes. Even though
    the probability of a disaster is much smaller, they could still have a devastating
    impact on businesses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔会发生中断，这可能是由各种原因引起的。这些通常是常见硬件故障的结果，例如磁盘或内存故障，但也可能是由网络故障、软件故障甚至应用程序故障引起的。例如，操作系统错误等软件故障可能导致服务器对用户和应用程序无响应。中断也可能是由洪水和地震等灾难引起的。尽管灾难发生的概率要小得多，但它们仍可能对企业产生毁灭性的影响。
- en: Predicting failures and disasters is an impossible task, as it is not possible
    to guess the exact time when they will strike. Therefore, the business strategy
    should focus on solutions for these, by allocating redundant hardware and software
    resources. In the case of MongoDB, the solution to high availability and disaster
    recovery is to deploy MongoDB clusters instead of a single-server database. As
    opposed to other third-party database solutions, MongoDB doesn't require expensive
    hardware to build high-availability clusters, and they are relatively easy to
    deploy. This is where replication comes in handy. This chapter explores the idea
    of replication in detail.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 预测故障和灾难是一项不可能的任务，因为无法猜测它们将发生的确切时间。因此，业务策略应该专注于为这些问题提供解决方案，通过分配冗余的硬件和软件资源。在MongoDB的情况下，实现高可用性和灾难恢复的解决方案是部署MongoDB集群，而不是单服务器数据库。与其他第三方数据库解决方案不同，MongoDB不需要昂贵的硬件来构建高可用性集群，而且它们相对容易部署。这就是复制派上用场的地方。本章将详细探讨复制的概念。
- en: First, it is important to learn about the basics of high-availability clusters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，了解高可用性集群的基础知识非常重要。
- en: High-Availability Clusters
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高可用性集群
- en: Before we delve into the technical details of MongoDB clusters, let's first
    clarify the basic concepts. There are many different technical implementations
    of high-availability clusters, and it is important to find out how a MongoDB cluster
    solution is different from other third-party cluster implementations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解MongoDB集群的技术细节之前，让我们首先澄清基本概念。高可用性集群有许多不同的技术实现，重要的是要了解MongoDB集群解决方案与其他第三方集群实现的区别。
- en: Computer clusters are a group of computers, connected to provide a common service.
    Compared to single servers, clusters are designed to provide better availability
    and performance. Clusters have redundant hardware and software that permits the
    continuation of services in the event of failures, so that, from the user perspective,
    the cluster appears as a single unified system rather than a group of different
    computers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机集群是一组连接在一起以提供共同服务的计算机。与单个服务器相比，集群旨在提供更好的可用性和性能。集群具有冗余的硬件和软件，允许在发生故障时继续提供服务，因此，从用户的角度来看，集群看起来像是一个统一的系统，而不是一组不同的计算机。
- en: Cluster Nodes
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群节点
- en: A cluster node is a server computer system (or virtual server) that is part
    of the cluster. It takes at least two different servers to make a cluster, with
    each cluster node having its own hostname and IP address. MongoDB 4.2 clusters
    can have a maximum of 50 nodes. In practice, most MongoDB clusters have at least
    3 members and they rarely reach more than 10 nodes, even for very large clusters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集群节点是集群的一部分的服务器计算机系统（或虚拟服务器）。至少需要两个不同的服务器才能组成一个集群，每个集群节点都有自己的主机名和IP地址。MongoDB
    4.2集群最多可以有50个节点。在实践中，大多数MongoDB集群至少有3个成员，即使对于非常大的集群，它们也很少超过10个节点。
- en: Share-Nothing
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无共享
- en: In other third-party clusters, cluster nodes share common cluster resources,
    such as disk storage. MongoDB has a "share-nothing" cluster model instead, where
    nodes are independent computers. Cluster nodes are connected only by the MongoDB
    software, and data replication is performed over the internet. The advantage of
    this model is that MongoDB clusters are easier to build with just commodity server
    hardware, which is not expensive.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他第三方集群中，集群节点共享公共集群资源，如磁盘存储。相反，MongoDB采用了“无共享”集群模型，其中节点是独立的计算机。集群节点仅通过MongoDB软件连接，并且数据复制是通过互联网执行的。这种模型的优势在于，MongoDB集群更容易使用廉价的服务器硬件构建。
- en: Cluster Names
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集群名称
- en: A cluster name is defined in the Atlas Console, and it is used to manage the
    cluster from the Atlas web interface. As mentioned in some of the previous chapters,
    in Atlas Free Tier, you can create only one cluster (M0), which has three cluster
    nodes. The default name for a new cluster is `Cluster0`. The name of the cluster
    cannot be changed after the cluster is created.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 集群名称在Atlas控制台中定义，并且用于从Atlas Web界面管理集群。如前几章中提到的，在Atlas免费版中，只能创建一个集群（M0），其中有三个集群节点。新集群的默认名称为`Cluster0`。集群的名称在创建后无法更改。
- en: Replica Sets
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 副本集
- en: A MongoDB cluster is based on data replication between cluster nodes. Data is
    replicated among nodes or replica set members with the purpose of keeping data
    in sync across all MongoDB database instances.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB集群基于集群节点之间的数据复制。数据在所有MongoDB数据库实例之间同步复制。
- en: Primary-Secondary
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主-从
- en: Data replication in MongoDB replica set clusters is a master-slave replication
    architecture. The primary node sends data to secondary nodes. The replication
    is always unidirectional, from primary to secondary. There is no option for multi-master
    replication in MongoDB, so there can be only one primary node at a time. All other
    members of the MongoDB replica set cluster must be secondary nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB副本集群中的数据复制是一种主从复制架构。主节点将数据发送到从节点。复制始终是单向的，从主节点到从节点。在MongoDB中没有多主复制的选项，因此一次只能有一个主节点。MongoDB副本集群的所有其他成员必须是从节点。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible to have multiple `mongod` processes on the same server. Each
    `mongod` process can be a standalone database instance, or it can be a member
    of a replica set cluster. For production servers, it is recommended to deploy
    just one `mongod` process per server.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同一服务器上可以有多个`mongod`进程。每个`mongod`进程可以是独立的数据库实例，也可以是副本集群的成员。对于生产服务器，建议每台服务器只部署一个`mongod`进程。
- en: The Oplog
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Oplog
- en: 'One database component that is essential for MongoDB replication is the **Oplog**
    (**Operation Log**). The Oplog is a special circular buffer in which all data
    changes are saved for cluster replication. Data changes are generated by CRUD
    operations (insert/update/delete) on the primary database. Nevertheless, database
    queries don''t generate any Oplog records because queries don''t modify any data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MongoDB复制而言，一个至关重要的数据库组件是**Oplog**（**操作日志**）。Oplog是一个特殊的循环缓冲区，用于保存集群复制的所有数据更改。数据更改是由主数据库上的CRUD操作（插入/更新/删除）生成的。然而，数据库查询不会生成任何Oplog记录，因为查询不会修改任何数据。
- en: '![Figure 10.1: Mongo DB Oplog'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1：Mongo DB Oplog'
- en: '](img/B15507_10_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_01.jpg)'
- en: 'Figure 10.1: Mongo DB Oplog'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：Mongo DB Oplog
- en: Therefore, all CRUD database writes are applied to datafiles by changing JSON
    data in database collections (just like on non-clustered databases) and are saved
    in the Oplog buffer for replication. Data change operations are converted into
    a special idempotent format that can be applied multiple times with the same result.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有CRUD数据库写入都通过更改数据库集合中的JSON数据应用到数据文件中（就像在非集群数据库上一样），并保存在Oplog缓冲区中进行复制。数据更改操作被转换为一种特殊的幂等格式，可以多次应用并产生相同的结果。
- en: At the database logical level, the Oplog appears as a capped (circular) collection
    in the local system database. The size of the Oplog collection is particularly
    important for cluster operations and maintenance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库逻辑级别上，Oplog显示为本地系统数据库中的一个有上限（循环）的集合。Oplog集合的大小对于集群操作和维护非常重要。
- en: 'By default, the maximum allocated size for the Oplog is 5% of the server''s
    free disk space. To check the size of the currently allocated Oplog (in bytes),
    use the **local** database to query replication stats, as shown in the following
    example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Oplog的最大分配大小为服务器空闲磁盘空间的5%。要检查当前分配的Oplog大小（以字节为单位），请使用**local**数据库查询复制统计信息，如下例所示：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following JS script will print the size of the Oplog in megabytes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下JS脚本将打印Oplog的大小（以兆字节为单位）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 10.2: Output after running the JS script'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2：运行JS脚本后的输出'
- en: '](img/B15507_10_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_02.jpg)'
- en: 'Figure 10.2: Output after running the JS script'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：运行JS脚本后的输出
- en: As shown in *Figure 10.2*, the Oplog size for this Atlas cluster is `3258 MB`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.2*所示，此Atlas集群的Oplog大小为`3258 MB`。
- en: Note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes, the Oplog is mistaken for WiredTiger journaling. Journaling is also
    a log for database changes, but with a different scope. While the Oplog is designed
    for cluster data replication, database journaling is a low-level log needed for
    database recovery. For example, if MongoDB crashes unexpectedly, datafiles can
    become corrupted because the last changes were not saved. Journal records are
    needed to perform database recovery after the instance restarts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，Oplog被误认为是WiredTiger日志记录。日志记录也是数据库更改的日志，但范围不同。虽然Oplog是为集群数据复制而设计的，但数据库日志记录是为数据库恢复所需的低级日志。例如，如果MongoDB意外崩溃，数据文件可能会损坏，因为最后的更改没有保存。在实例重新启动后，需要日志记录来执行数据库恢复。
- en: Replication Architecture
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制架构
- en: 'The following diagram depicts the architecture diagram of a simple replica
    set cluster with only three server nodes – one primary node and two secondary
    nodes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了一个简单的副本集群架构图，只有三个服务器节点 - 一个主节点和两个从节点：
- en: '![Figure 10.3: MongoDB replication'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3：MongoDB复制'
- en: '](img/B15507_10_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_03.jpg)'
- en: 'Figure 10.3: MongoDB replication'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：MongoDB复制
- en: In the preceding model, the PRIMARY database is the only active replica set
    member that receives write operations from database clients. The PRIMARY database
    saves data changes in the Oplog. Changes saved in the Oplog are sequential—that
    is, saved in the order that they are received and executed.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上述模型中，PRIMARY数据库是唯一从数据库客户端接收写操作的活动副本集成员。PRIMARY数据库保存Oplog中的数据更改。在Oplog中保存的更改是顺序的，即按照它们接收和执行的顺序保存。
- en: The SECONDARY database is querying the PRIMARY database for new changes in the
    Oplog. If there are any changes, then Oplog entries are copied from PRIMARY to
    SECONDARY as soon as they are created on the PRIMARY node.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SECONDARY数据库正在查询PRIMARY数据库中Oplog的新更改。如果有任何更改，那么Oplog条目将立即从PRIMARY复制到SECONDARY上。
- en: Then, the SECONDARY database applies changes from the Oplog to its own datafiles.
    Oplog entries are applied in the same order they were inserted in the log. As
    a result, datafiles on SECONDARY are kept in sync with changes on PRIMARY.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，SECONDARY数据库将Oplog中的更改应用到自己的数据文件中。Oplog条目按照它们在日志中插入的顺序应用。因此，SECONDARY上的数据文件与PRIMARY上的更改保持同步。
- en: Usually, SECONDARY databases copy data changes directly from PRIMARY. Sometimes
    a SECONDARY database can replicate data from another SECONDARY. This type of replication
    is called *Chained Replication* because it is a two-step replication process.
    Chained replication is useful in certain replication topologies, and it is enabled
    by default in MongoDB.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，SECONDARY数据库直接从PRIMARY复制数据更改。有时，SECONDARY数据库可以从另一个SECONDARY复制数据。这种复制类型称为*链式复制*，因为它是一个两步复制过程。链式复制在某些复制拓扑中很有用，并且在MongoDB中默认启用。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to understand that, once a MongoDB instance is part of a replica
    set cluster, all changes are copied to the Oplog for data replication. It is not
    possible to use a replica set to replicate only some parts, such as just a few
    database collections. For this reason, all user data is replicated and kept in
    sync across all cluster members.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，一旦MongoDB实例成为副本集集群的一部分，所有更改都会被复制到Oplog以进行数据复制。不可能仅复制一些部分，例如仅复制几个数据库集合。因此，所有用户数据都会被复制并在所有集群成员之间保持同步。
- en: Cluster members can have different states, such as PRIMARY and SECONDARY in
    the preceding diagram. Node states can change in time, depending on cluster activity.
    For example, a node can be in the PRIMARY state at one point in time, and in the
    SECONDARY state, another time. PRIMARY and SECONDARY are the most common states
    of a node in the cluster configuration, although other states are possible. To
    understand their possible roles and how they can change, let's explore the technical
    details of cluster election.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 集群成员可以具有不同的状态，例如上图中的PRIMARY和SECONDARY。节点状态可以随着时间的推移而改变，取决于集群活动。例如，一个节点可以在某个时间点处于PRIMARY状态，而在另一个时间点处于SECONDARY状态。PRIMARY和SECONDARY是集群配置中节点最常见的状态，尽管可能存在其他状态。为了理解它们可能的角色以及它们如何改变，让我们探索集群选举的技术细节。
- en: Cluster Members
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群成员
- en: 'In Atlas, you can see the cluster member list from the `Clusters` page, as
    shown in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atlas中，您可以从`Clusters`页面查看集群成员列表，如下截图所示：
- en: '![Figure 10.4: Atlas web interface'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4：Atlas web界面'
- en: '](img/B15507_10_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_04.jpg)'
- en: 'Figure 10.4: Atlas web interface'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：Atlas web界面
- en: 'Click on the cluster name `Cluster0` from `SANDBOX`. Then the list of servers
    and their roles will be displayed in the Atlas application:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从`SANDBOX`中点击集群名称`Cluster0`。然后在Atlas应用程序中将显示服务器及其角色的列表：
- en: '![Figure 10.5: Atlas web interface'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5：Atlas web界面'
- en: '](img/B15507_10_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_05.jpg)'
- en: 'Figure 10.5: Atlas web interface'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：Atlas web界面
- en: 'As shown in *Figure 10.5*, this cluster has three cluster members, which are
    named with the same prefix as the Atlas cluster name (in this case, `Cluster0`).
    For MongoDB clusters that are installed without using the Atlas PaaS web interface
    (or that are installed locally, on premises), you can check the cluster members
    using the following mongo shell command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.5*所示，此集群有三个集群成员，它们的名称与Atlas集群名称具有相同的前缀（在本例中为`Cluster0`）。对于未使用Atlas PaaS
    web界面（或在本地安装）安装的MongoDB集群，可以使用以下mongo shell命令检查集群成员：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: An example of using the cluster status command will be provided in *Exercise
    10.01*, *Checking Atlas Cluster Members*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将在*练习10.01* *检查Atlas集群成员*中提供使用集群状态命令的示例。
- en: The Election Process
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选举过程
- en: One feature specific to all cluster implementations is the ability to survive
    (or fail over) in the event of failures. The MongoDB replica set is protected
    against any type of failure, be it a hardware failure, software failure, or network
    outage. The MongoDB software responsible for this process is called **cluster
    election**—a name derived from the action of electing using votes. The purpose
    of a cluster election is to "elect" a new primary.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集群实现的一个特点是在发生故障时能够生存（或故障转移）。MongoDB副本集受到任何类型的故障的保护，无论是硬件故障、软件故障还是网络中断。负责此过程的MongoDB软件称为**集群选举**，这个名字来源于使用选票进行选举的行为。集群选举的目的是“选举”一个新的主节点。
- en: 'The election process is initiated by an event. For example, consider that the
    primary member is lost. Analogous to political elections, the MongoDB cluster
    members participate in a vote to elect a new primary member. The election is validated
    only if it obtains the majority of all votes in the cluster. The formula is remarkably
    simple: the surviving cluster has a majority of (*N/2 + 1*), where *N* is the
    total number of nodes. Therefore, half plus one of the votes is enough to elect
    a new primary. This majority is necessary to avoid split-brain syndrome:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 选举过程是由事件发起的。例如，考虑主节点丢失的情况。类似于政治选举，MongoDB集群成员参与投票选举新的主节点。只有获得集群中所有选票的多数的选举才被验证。这个公式非常简单：幸存的集群有(*N/2
    + 1*)的多数，其中*N*是节点的总数。因此，一半加一的选票足以选举出一个新的主节点。这个多数是为了避免分裂脑综合症而必要的。
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Split-brain syndrome is the terminology used to define a situation where two
    parts of the same cluster are isolated and they both "believe" that they are the
    only surviving part of the cluster. Enforcing the "half plus one" rule ensures
    that only the largest part of the cluster can elect a new primary.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂脑综合症是用来定义同一集群的两个部分被隔离并且它们都“相信”它们是集群中唯一幸存的部分的术语。强制执行“半加一”规则确保只有集群中最大的部分才能选举新的主节点。
- en: '![Figure 10.6: MongoDB election'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6：MongoDB选举'
- en: '](img/B15507_10_06.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_06.jpg)'
- en: 'Figure 10.6: MongoDB election'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：MongoDB选举
- en: Consider the preceding diagram. After a network partition incident, nodes 3
    and 5 are isolated from the rest of the cluster. In this situation, the left side
    (nodes 1, 2, and 4) form a majority, whereas nodes 3 and 5 form a minority. So,
    nodes 1, 2, and 4 can elect a primary, since they form the majority cluster. Nevertheless,
    there are situations where a network partition could split the cluster into halves,
    with identical numbers of nodes. In this case, none of the halves have a majority
    necessary to elect a new primary. Therefore, one of the key factors in MongoDB
    cluster design is that clusters should always be configured with an odd number
    of nodes to avoid a perfect half split.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑前面的图表。在网络分区事件发生后，节点3和5与集群的其余部分隔离。在这种情况下，左侧（节点1、2和4）形成多数，而节点3和5形成少数。因此，节点1、2和4可以选举出一个主节点，因为它们形成了多数集群。然而，也有一些情况，网络分区可能将集群分成两半，节点数量相同。在这种情况下，没有一半具有足够多的节点来选举出一个新的主节点。因此，MongoDB集群设计的一个关键因素是，集群应始终配置为奇数节点，以避免完美的一半分裂。
- en: Not all cluster members can participate in an election. There can be a maximum
    of seven votes, regardless of the total number of members in a MongoDB cluster.
    This is designed to limit the network traffic between cluster nodes during the
    election process. Non-voting members cannot participate in elections, but they
    can replicate data from the primary as secondary nodes. By default, each node
    can have one vote.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有集群成员都可以参与选举。在MongoDB集群中，最多可以有七个选票，而成员总数不影响这一规定。这是为了限制选举过程中集群节点之间的网络流量。非投票成员不能参与选举，但它们可以作为辅助节点从主节点复制数据。默认情况下，每个节点可以有一个选票。
- en: 'Exercise 10.01: Checking Atlas Cluster Members'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习10.01：检查Atlas集群成员
- en: 'In this exercise, you will connect to the Atlas cluster using mongo shell and
    identify the cluster name and all cluster members, together with their current
    state. Use JavaScript to list the cluster members:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您将使用mongo shell连接到Atlas集群，并识别集群名称和所有集群成员，以及它们当前的状态。使用JavaScript列出集群成员：
- en: 'Connect to your Atlas database using mongo shell:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到Atlas数据库使用mongo shell：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The replica set status function `rs.status()` gives detailed information about
    the cluster that is not visible from the Atlas web interface. A simple JS script
    to list all nodes and their member roles for `rs.status` is as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 副本集状态函数`rs.status()`提供了有关集群的详细信息，这些信息在Atlas Web界面上是不可见的。列出所有节点及其`rs.status`成员角色的简单JS脚本如下：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The script can run from any node of the cluster if you are connected to one
    secondary instead of the primary.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您连接到一个辅助节点而不是主节点，则可以从集群的任何节点运行该脚本。
- en: 'The output for this is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出如下：
- en: '![Figure 10.7: Output after running the JS script'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7：运行JS脚本后的输出'
- en: '](img/B15507_10_07.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_07.jpg)'
- en: 'Figure 10.7: Output after running the JS script'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：运行JS脚本后的输出
- en: We have learned about the basic concepts of MongoDB replica set clusters. The
    MongoDB primary-secondary replication technology protects the database from any
    hardware and software failures. In addition to providing high availability and
    disaster recovery for applications and users, MongoDB clusters are also easy to
    deploy and manage. Thanks to the Atlas managed database service, users can easily
    connect to Atlas and test applications, without the need to install and configure
    the cluster locally.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了MongoDB副本集集群的基本概念。MongoDB的主从复制技术保护数据库免受任何硬件和软件故障的影响。除了为应用程序和用户提供高可用性和灾难恢复外，MongoDB集群还易于部署和管理。由于Atlas托管数据库服务，用户可以轻松连接到Atlas并测试应用程序，而无需在本地安装和配置集群。
- en: Client Connections
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端连接
- en: The MongoDB connection string was covered in *Chapter 3*, *Servers and Clients*.
    Database services deployed in Atlas are always replica set clusters, and the connection
    string can be copied from the Atlas interface. In this section, we will explore
    the connections between clients and MongoDB clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB连接字符串在*第3章*，*服务器和客户端*中有介绍。在Atlas部署的数据库服务始终是副本集集群，并且连接字符串可以从Atlas界面复制。在本节中，我们将探讨客户端与MongoDB集群之间的连接。
- en: Connecting to a Replica Set
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接到副本集
- en: 'In general, the same rules apply for the MongoDB connection string. Consider
    the following screenshot, which shows such a connection:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况下，MongoDB连接字符串适用相同的规则。请考虑以下屏幕截图，显示了这样一个连接：
- en: '![Figure 10.8: An example of a connection string in mongo shell'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.8：mongo shell中连接字符串的示例'
- en: '](img/B15507_10_08.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_08.jpg)'
- en: 'Figure 10.8: An example of a connection string in mongo shell'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：mongo shell中连接字符串的示例
- en: 'As shown in *Figure10.6*, the connection string looks like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.6*所示，连接字符串如下所示：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As explained in *Chapter 3*, *Servers and Clients*, this type of string needs
    DNS to resolve the actual server names or IP addresses. In this example, the connection
    string contains the Atlas cluster name `cluster0` and the ID number `u7n6b`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第3章*中所述，*服务器和客户端*，这种类型的字符串需要DNS来解析实际的服务器名称或IP地址。在本例中，连接字符串包含Atlas集群名称`cluster0`和ID号`u7n6b`。
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In your case, the connection string could be different. That is because your
    Atlas cluster deployment is likely to have a different ID number and/or a different
    cluster name. Your actual connection string can be copied from your Atlas web
    console.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的情况下，连接字符串可能会有所不同。这是因为您的Atlas集群部署可能具有不同的ID号和/或不同的集群名称。您实际的连接字符串可以从Atlas web控制台中复制。
- en: 'Following a careful inspection of the text in the shell, we see the following
    details:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细检查shell中的文本后，我们看到以下细节：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first thing to notice is that the second string is significantly longer
    than the first. That is because the original connection string is substituted
    (after a successful DNS SRV lookup) into the equivalent string with the `mongodb://`
    URI prefix. The following table explains the structure of the cluster connection
    string:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，第二个字符串比第一个字符串要长得多。这是因为原始连接字符串（成功进行DNS SRV查找后）被替换为具有`mongodb://`URI前缀的等效字符串。以下表格解释了集群连接字符串的结构：
- en: '![Figure 10.9: Structure of the collection string'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：连接字符串的结构
- en: '](img/B15507_10_09.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_09.jpg)'
- en: 'Figure 10.9: Structure of the collection string'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：连接字符串的结构
- en: 'Following a successful connection and user authentication, the shell prompt
    will have the following format:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 成功连接和用户认证后，shell提示将具有以下格式：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`MongoDB Enterprise` here specifies the version of the MongoDB server running
    in the cloud.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MongoDB Enterprise`在这里指定了在云中运行的MongoDB服务器的版本。'
- en: '`atlas-rzhbg7-shard-0` indicates the MongoDB replica set name. Note that in
    the current version of Atlas, the MongoDB replica set name is different from the
    cluster name, which is `Cluster0` in this case.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`atlas-rzhbg7-shard-0`表示MongoDB副本集名称。请注意，在当前版本的Atlas中，MongoDB副本集名称与集群名称不同，本例中为`Cluster0`。'
- en: '`PRIMARY` refers to the database instance role.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PRIMARY`指的是数据库实例的角色。'
- en: 'There is a clear distinction in MongoDB between a cluster connection and a
    single server connection. The connection shows the MongoDB cluster, in the following
    form:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中，集群连接和单个服务器连接之间有明显的区别。连接显示为以下形式的MongoDB集群：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To verify the current connection from mongo shell, use the following function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证从mongo shell的当前连接，请使用以下函数：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 10.10: Verifying the connection string in mongo shell'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：验证mongo shell中的连接字符串
- en: '](img/B15507_10_10.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_10.jpg)'
- en: 'Figure 10.10: Verifying the connection string in mongo shell'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：验证mongo shell中的连接字符串
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The replica set name connection parameter `replicaSet` indicates that the connection
    string is for a cluster instead of a simple MongoDB server instance. In this case,
    the shell will attempt to connect to all server members of the cluster. From the
    application perspective, the replica set is behaving as a single system, rather
    than a collection of separate servers. When connected to a cluster, the shell
    will always indicate the `PRIMARY` read-write instance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集名称连接参数`replicaSet`表示连接字符串是用于集群而不是简单的MongoDB服务器实例。在这种情况下，shell将尝试连接到集群的所有服务器成员。从应用程序的角度来看，副本集的行为就像是一个单一的系统，而不是一组独立的服务器。连接到集群时，shell将始终指示`PRIMARY`读写实例。
- en: The next section looks at single-server connections.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将介绍单服务器连接。
- en: Single-Server Connections
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单服务器连接
- en: 'In the same way we connect to a non-clustered MongoDB database, we have the
    option to connect to individual cluster members separately. In this case, the
    target server name (cluster member) needs to be contained in the connection string.
    Also, the `replicaSet` parameter needs to be removed. Here is an example for the
    Atlas cluster:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与连接到非集群MongoDB数据库的方式相同，我们有选择地连接到单个集群成员。在这种情况下，目标服务器名称（集群成员）需要包含在连接字符串中。此外，需要删除`replicaSet`参数。以下是Atlas集群的示例：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The other two parameters, `authSource` and `ssl`, need to be retained for Atlas
    server connections. As described in *Chapter 3*, *Servers and Clients*, Atlas
    has authorization and SSL network encryption activated for cloud security protection.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个参数`authSource`和`ssl`需要保留用于Atlas服务器连接。如*第3章*中所述，*服务器和客户端*，Atlas已激活授权和SSL网络加密以提供云安全保护。
- en: 'The following screenshot shows an example of this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个示例：
- en: '![Figure 10.11: Connecting to individual cluster members'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：连接到单个集群成员
- en: '](img/B15507_10_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_11.jpg)'
- en: 'Figure 10.11: Connecting to individual cluster members'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：连接到单个集群成员
- en: This time, the shell prompt indicates `SECONDARY`, which indicates that we are
    connected to the secondary node. Also, the `db.getMongo()` function returns a
    simple server and port number connection.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，shell提示显示`SECONDARY`，表示我们连接到了辅助节点。此外，`db.getMongo()`函数返回一个简单的服务器和端口号连接。
- en: 'As described earlier, data changes are not allowed on secondary members. This
    is because a MongoDB cluster needs to maintain a consistent copy of data across
    all cluster nodes. Therefore, changing data is allowed only on the primary node
    of the cluster. For example, if we try to modify, insert, or update a collection
    while connected on a secondary member, we will get the `not master` error message,
    as shown in the following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，不允许在辅助成员上进行数据更改。这是因为MongoDB集群需要在所有集群节点上保持一致的数据副本。因此，只允许在集群的主节点上更改数据。例如，如果我们尝试在连接到辅助成员时修改、插入或更新集合，将会收到`not
    master`错误消息，如下截图所示：
- en: '![Figure 10.12: Getting the "not master" error message in mongo shell'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.12：在mongo shell中获取“not master”错误消息'
- en: '](img/B15507_10_12.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_12.jpg)'
- en: 'Figure 10.12: Getting the "not master" error message in mongo shell'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：在mongo shell中获取“not master”错误消息
- en: However, read-only operations are allowed on secondary members, and this is
    precisely the scope of the next exercise. In this exercise, you will learn how
    to read collections while connected on secondary cluster members.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，辅助成员允许只读操作，这正是下一个练习的范围。在这个练习中，您将学习如何在连接到辅助集群成员时读取集合。
- en: Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To enable read operations while connected to a secondary node, it is necessary
    to run the shell command `rs.slaveOk()`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要在连接到辅助节点时启用读操作，需要运行shell命令`rs.slaveOk()`。
- en: 'Exercise 10.02: Checking the Cluster Replication'
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习10.02：检查集群复制
- en: 'In this exercise, you will connect to the Atlas cluster database using mongo
    shell and observe the data replication between the primary and secondary cluster
    nodes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您将使用mongo shell连接到Atlas集群数据库，并观察主要和辅助集群节点之间的数据复制：
- en: 'Connect to your Atlas cluster with mongo shell and user `admindb`:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用mongo shell和用户`admindb`连接到您的Atlas集群：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The connection string could be different in your case. You can copy the connection
    string from the Atlas web interface.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的情况下，连接字符串可能会有所不同。您可以从Atlas Web界面复制连接字符串。
- en: 'Execute the following script to create a new collection on the primary node
    and insert a few new documents with random numbers:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本在主节点上创建一个新集合，并插入一些具有随机数字的新文档：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output for this is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 10.13: Inserting new documents with random numbers'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.13：插入具有随机数字的新文档'
- en: '](img/B15507_10_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_13.jpg)'
- en: 'Figure 10.13: Inserting new documents with random numbers'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：插入具有随机数字的新文档
- en: 'Connect to a secondary node by entering the following code:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输入以下代码连接到辅助节点：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The connection string could be different in your case. Make sure you edit the
    correct server node in the connection string. The connection should indicate a
    `SECONDARY` member.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的情况下，连接字符串可能会有所不同。确保您在连接字符串中编辑正确的服务器节点。连接应指示`SECONDARY`成员。
- en: 'Query the collection to see whether data is replicated on the secondary nodes.
    To enable the reading of data on the secondary nodes, run the following command:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询集合，查看辅助节点上是否复制了数据。要在辅助节点上读取数据，请运行以下命令：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output for this is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 10.14: Reading data on the secondary nodes'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.14：在辅助节点上读取数据'
- en: '](img/B15507_10_14.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_14.jpg)'
- en: 'Figure 10.14: Reading data on the secondary nodes'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：在辅助节点上读取数据
- en: In this exercise, you verified the cluster MongoDB replication by inserting
    documents on the primary node and querying them on secondary nodes. You may notice
    that the replication is almost instantaneous, even though MongoDB replication
    is asynchronous.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您通过在主节点上插入文档并在辅助节点上查询它们来验证了集群MongoDB复制。您可能会注意到，即使MongoDB复制是异步的，复制几乎是瞬时的。
- en: Read Preference
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取偏好设置
- en: While it is possible to read data from a secondary node (as shown in the previous
    exercise), it is not ideal for applications because it requires a separate connection.
    **Read preference** is a term in MongoDB that defines how clients can redirect
    read operations to secondary nodes automatically, without connecting to individual
    nodes. There are a few reasons why the client may choose to redirect read operations
    to secondary nodes. For example, running large queries on the primary node will
    slow down overall performance for all operations. Offloading the primary node
    by running queries on secondary nodes is a good idea to optimize performance for
    inserts and updates.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以从辅助节点读取数据（如前面的练习所示），但这对应用程序来说并不理想，因为它需要单独的连接。**读取偏好设置**是MongoDB中定义客户端如何自动将读操作重定向到辅助节点的术语，而无需连接到各个节点。客户端可能选择将读操作重定向到辅助节点的原因有几个。例如，在主节点上运行大型查询会减慢所有操作的整体性能。通过在辅助节点上运行查询来卸载主节点是优化插入和更新性能的好方法。
- en: 'By default, all operations are performed on the primary node. While write operations
    must be executed only on the primary node, read operations can be performed on
    any secondary node (except an arbiter node). The client can set a read preference
    at the session or statement level while connected to a MongoDB cluster. The following
    command helps check the current read preference:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有操作都在主节点上执行。虽然写操作必须仅在主节点上执行，但读操作可以在任何辅助节点上执行（除了仲裁者节点）。客户端可以在连接到MongoDB集群时在会话或语句级别设置读取偏好设置。以下命令可帮助检查当前的读取偏好设置：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following table shows the various **read preferences** in MongoDB:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了MongoDB中各种**读取偏好设置**：
- en: '![Figure 10.15: Read preferences in MongoDB'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.15：MongoDB中的读取偏好设置'
- en: '](img/B15507_10_15.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_15.jpg)'
- en: 'Figure 10.15: Read preferences in MongoDB'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：MongoDB中的读取偏好设置
- en: 'The following code shows an example of setting the read preference (in this
    case, `secondary`):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了设置读取偏好设置的示例（在本例中为`secondary`）：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you have a current cluster connection, with DNS SRV or a cluster/server
    list. The read preference setting doesn't work correctly with a single node connection.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您有当前的集群连接，使用DNS SRV或集群/服务器列表。读取偏好设置在单节点连接中无法正常工作。
- en: 'The following is an example of using a read preference from mongo shell:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从mongo shell使用读取偏好设置的示例：
- en: '![Figure 10.16: Read preference from mongo shell'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.16：从mongo shell读取偏好设置'
- en: '](img/B15507_10_16.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_16.jpg)'
- en: 'Figure 10.16: Read preference from mongo shell'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：从mongo shell读取偏好设置
- en: 'Note that once the read preference is set to `secondary`, the shell client
    automatically redirects the read operations to secondary nodes. After the query
    is performed, the shell returns to `primary` (shell prompt: `PRIMARY`). All further
    queries will be redirected to `secondary`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦读取偏好设置为`secondary`，shell客户端会自动将读取操作重定向到次要节点。执行查询后，shell会返回到`primary`（shell提示：`PRIMARY`）。所有后续查询将被重定向到`secondary`。
- en: Note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The read preference is lost if the client disconnects from the replica set.
    This is because the read preference is a client-side setting (not server). In
    this case, you will need to set the read preference again, after reconnecting
    to the MongoDB cluster.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 读取偏好设置在客户端从副本集断开连接时会丢失。这是因为读取偏好设置是客户端设置（而不是服务器）。在这种情况下，您需要在重新连接到MongoDB集群后再次设置读取偏好设置。
- en: 'The read preference can also be set as an option in the connection string URI,
    with the `?readPreference` parameter. For example, consider the following connection string:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 读取偏好设置也可以作为连接字符串URI的选项进行设置，使用`?readPreference`参数。例如，考虑以下连接字符串：
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: MongoDB offers even more sophisticated features for setting the read preference
    in a cluster. In more advanced configurations, the administrator can set tag names
    for each cluster member. For example, a tag name can indicate that the cluster
    member is located in a specific geographical region or data center. The tag name
    can then be used as a parameter to the `db.setReadPref()` function to redirect
    reads to a specific geographical region in the proximity of the client's location.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB在集群中为设置读取偏好提供了更复杂的功能。在更高级的配置中，管理员可以为每个集群成员设置标记名称。例如，标记名称可以指示集群成员位于特定的地理区域或数据中心。然后，标记名称可以作为`db.setReadPref()`函数的参数，将读取重定向到客户端位置附近的特定地理区域。
- en: Write Concern
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入关注
- en: By default, a Mongo client receives a confirmation for each write operation
    (insert/update/delete) on the primary node. The confirmation return code can be
    used in applications to make sure that data is securely written into the database.
    In the case of replica set clusters, though, the situation is more complex. For
    example, it is possible to insert rows in a primary instance, but if the primary
    node crashes before replication Oplog records are applied to secondary nodes,
    then there is a risk of data loss. Write concern addresses this issue by ensuring
    that the write is confirmed on multiple cluster nodes. Therefore, in the event
    of an unexpected crash of the primary node, the inserted data will not be lost.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Mongo客户端在主节点上每次写入操作（插入/更新/删除）都会收到确认。确认返回代码可以在应用程序中使用，以确保数据安全写入数据库。然而，在副本集集群中，情况更为复杂。例如，可能在主实例中插入行，但如果在副本节点应用复制Oplog记录之前主节点崩溃，那么存在数据丢失的风险。写入关注通过确保在多个集群节点上确认写入来解决这个问题。因此，在主节点意外崩溃的情况下，插入的数据不会丢失。
- en: 'By default, the write concern is `{w: 1}`, which indicates acknowledgment from
    the primary instance only. `{w: 2}` will require confirmation from two nodes for
    each write operation. Multiple node confirmation comes at a cost, however. A large
    number for the write concern can lead to slower write operations on the cluster.
    `(w: "majority")` indicates the majority of cluster nodes. This setting helps
    ensure data safety in unexpected failure scenarios.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '默认情况下，写入关注为`{w: 1}`，表示仅从主实例获得确认。`{w: 2}`将要求每个写入操作的两个节点进行确认。然而，多个节点的确认会带来成本。写入关注的大数字可能导致集群上的写入操作变慢。`(w:
    "majority")`表示大多数集群节点。此设置有助于确保在意外故障情况下数据的安全性。'
- en: 'Write concern can be set at the cluster level or at the write statement level.
    In Atlas, we cannot see or configure the write concern, as it is preset by MongoDB
    to `{w: "majority"}`. The following is an example of write concern at the statement
    level:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '写入关注可以在集群级别或写入语句级别进行设置。在Atlas中，我们无法看到或配置写入关注，因为MongoDB预设为`{w: "majority"}`。以下是语句级别的写入关注示例：'
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'All CRUD operations (except queries) have an option for write concern. Optionally,
    a second parameter can be set, `wtimeout: 1000`, to configure the maximum timeout
    in milliseconds.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '所有CRUD操作（除查询外）都有写入关注的选项。可以选择设置第二个参数`wtimeout: 1000`，以配置最大超时时间（以毫秒为单位）。'
- en: 'The following screenshot shows an example of this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了一个示例：
- en: '![Figure 10.17: Write concern in mongo shell'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.17：mongo shell中的写入关注'
- en: '](img/B15507_10_17.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_17.jpg)'
- en: 'Figure 10.17: Write concern in mongo shell'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17：mongo shell中的写入关注
- en: The MongoDB client has many options for replication-set clusters. Understanding
    the basics of a client session in the cluster environment is essential for application
    development. It can lead to mistakes if developers overlook the cluster configuration.
    For example, one common mistake is to run all queries on the primary node or to
    assume that secondary reads are executed by default without any configuration.
    Setting up the read preference can significantly improve the performance of applications
    while reducing the load on the primary cluster node.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB客户端在复制集群中有许多选项。了解集群环境中客户端会话的基础知识对应用程序开发至关重要。如果开发人员忽视了集群配置，可能会导致错误。例如，一个常见的错误是在主节点上运行所有查询，或者假设默认情况下执行次要读取而无需任何配置。设置读取偏好可以显著提高应用程序的性能，同时减少主集群节点的负载。
- en: Deploying Clusters
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署集群
- en: Setting up a new MongoDB replica set cluster is an operational task that is
    usually required at the start of a new development project. Depending on the complexity
    of the new environment, the deployment of a new replica set cluster can vary from
    a relatively easy, straightforward, simple configuration to more complex and enterprise-grade
    cluster deployments. In general, deploying MongoDB clusters requires more technical
    and operational knowledge than installing a single server database. Planning and
    preparation are essential and should never be overlooked before cluster deployments.
    That is because users need to carefully plan the cluster architecture, the underlying
    infrastructure, and database security to provide the best performance and availability
    for their database.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 设置新的MongoDB副本集集群是一个通常在新开发项目开始时需要的操作任务。根据新环境的复杂程度，部署新的副本集集群可能从相对简单、直接、简单的配置到更复杂和企业级的集群部署。一般来说，部署MongoDB集群需要比安装单个服务器数据库更多的技术和操作知识。规划和准备是必不可少的，在部署集群之前绝不能忽视。这是因为用户需要仔细规划集群架构、基础设施和数据库安全性，以提供最佳的数据库性能和可用性。
- en: Regarding the method used for MongoDB replica set cluster deployments, there
    are a few tools that can help with the automatization and management of the deployments.
    The most common method is manual deployment. Nevertheless, the manual method is
    probably the most laborious option—especially for complex clusters. Automatization
    tools are available from MongoDB and other third-party software providers. The
    next section looks at the most common methods used for MongoDB cluster deployments
    and the advantages of each method.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用于MongoDB副本集集群部署的方法，有一些工具可以帮助自动化和管理部署。最常见的方法是手动部署。然而，手动方法可能是最费力的选择，尤其是对于复杂的集群。MongoDB和其他第三方软件提供商提供了自动化工具。接下来的部分将介绍用于MongoDB集群部署的最常见方法以及每种方法的优势。
- en: Atlas Deployment
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Atlas部署
- en: Deploying MongoDB clusters on the Atlas cloud is the easiest option available
    for developers as it saves on effort and money. The MongoDB company manages the
    infrastructure, including the server hardware, OS, network, and `mongod` instances.
    As a result, users can focus on application development and DevOps, rather than
    spending time on the infrastructure. In many cases, this is the perfect solution
    for fast-delivery projects.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atlas云上部署MongoDB集群是开发人员可以选择的最简单选项，因为它节省了精力和金钱。MongoDB公司管理基础设施，包括服务器硬件、操作系统、网络和`mongod`实例。因此，用户可以专注于应用程序开发和DevOps，而不是花时间在基础设施上。在许多情况下，这是快速交付项目的完美解决方案。
- en: Deploying a cluster on Atlas requires nothing more than a few clicks in the
    Atlas web application. You are already familiar with database deployments in Atlas
    from *Chapter 1*, *Introduction to MongoDB*. The free-tier Atlas M0 cluster is
    a great free-of-charge environment for learning and testing. As a matter of fact,
    all deployments in Atlas are replica set clusters. In the current Atlas version,
    it is not possible to deploy single-server clusters in Atlas.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atlas上部署集群只需要在Atlas Web应用程序中点击几下即可。您已经熟悉了在Atlas中进行数据库部署的方法，这是从*第一章*《MongoDB简介》中学到的。免费的Atlas
    M0集群是一个非常适合学习和测试的免费环境。事实上，在Atlas中的所有部署都是副本集集群。在当前的Atlas版本中，不可能在Atlas中部署单服务器集群。
- en: Atlas offers more cluster options for larger deployments, which are charged
    services. If required, Atlas clusters can scale up easily—both vertically (adding
    server resources) and horizontally (adding more members). It is possible to build
    multi-region, replica set clusters on dedicated Atlas servers M10 and higher.
    Therefore, high availability can extend across geographical regions, between Europe
    and North America. This option is ideal for allocating read-only secondary nodes
    in a remote data center.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Atlas为更大规模的部署提供了更多的集群选项，这是收费服务。如果需要，Atlas集群可以轻松扩展——无论是纵向（增加服务器资源）还是横向（增加更多成员）。在专用的Atlas服务器M10及更高版本上，可以构建多区域的副本集集群。因此，高可用性可以跨地理区域，覆盖欧洲和北美。这个选项非常适合在远程数据中心分配只读次要节点。
- en: 'The following screenshot shows an example of a multi-region cluster configuration:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个多区域集群配置的示例：
- en: '![Figure 10.18: Multi-region cluster configuration'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.18：多区域集群配置'
- en: '](img/B15507_10_18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_18.jpg)'
- en: 'Figure 10.18: Multi-region cluster configuration'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：多区域集群配置
- en: In the preceding example, the primary database is in London, together with two
    other secondary nodes, while in Sydney, Australia, one additional secondary node
    is configured for read-only access.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，主数据库在伦敦，还有两个次要节点，而在澳大利亚的悉尼，还配置了一个额外的只读次要节点。
- en: Manual Deployment
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动部署
- en: Manual deployment is the most common form of MongoDB cluster deployment. For
    many developers, building a MongoDB cluster manually is also the preferred option
    for database installation because this method gives them full control over the
    infrastructure and cluster configuration. Manual deployment is more laborious
    compared with other methods, however, which makes this method less scalable for
    large environments.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 手动部署是MongoDB集群部署最常见的形式。对于许多开发人员来说，手动构建MongoDB集群也是首选的数据库安装方法，因为这种方法可以让他们完全控制基础设施和集群配置。然而，与其他方法相比，手动部署更费力，这使得这种方法在大型环境中不太可扩展。
- en: 'You would perform the following steps to manually deploy MongoDB clusters:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤手动部署MongoDB集群：
- en: Choose the server members of the new cluster. Whether they are physical servers
    or virtual, they must meet the minimum requirements for the MongoDB database.
    Also, all cluster members should have identical hardware and software specifications
    (CPU, memory, disk, and OS).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择新集群的服务器成员。无论是物理服务器还是虚拟服务器，它们都必须满足MongoDB数据库的最低要求。此外，所有集群成员的硬件和软件规格（CPU、内存、磁盘和操作系统）应该是相同的。
- en: MongoDB binaries must be installed on each server. Use the same installation
    path on all servers.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每台服务器上都必须安装MongoDB二进制文件。在所有服务器上使用相同的安装路径。
- en: Run one `mongod` instance per server. Servers should be on separate hardware
    with a separate power supply and network connections. For testing, however, it
    is possible to deploy all cluster members on a single physical server.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每台服务器上运行一个`mongod`实例。服务器应该在单独的硬件上，具有单独的电源和网络连接。但是，对于测试，可以将所有集群成员部署在单个物理服务器上。
- en: Start the Mongo server with the `--bind_ip` parameter. By default, `mongod`
    binds only to the localhost IP address (`127.0.0.1`). In order to communicate
    with other cluster members, `mongod` must bind to external private or public IP addresses.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`--bind_ip`参数启动Mongo服务器。默认情况下，`mongod`仅绑定到本地IP地址（`127.0.0.1`）。为了与其他集群成员通信，`mongod`必须绑定到外部私有或公共IP地址。
- en: Set the network properly. Each server must be able to communicate freely with
    other members without firewalls. Also, servers' IPs and DNS names must match in
    the DNS domain configuration.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确设置网络。每台服务器必须能够自由地与其他成员通信，而无需防火墙。此外，服务器的IP和DNS名称必须在DNS域配置中匹配。
- en: Create the directory structure for database files and database instance logs.
    Use the same path on all servers. For example, use `/data/db` for database files
    (WiredTiger storage) and `/var/log/mongodb` for log files on Unix/macOS systems,
    and in the case of Windows OSes, use `C:\data\db` directories for datafiles and
    `C:\log\mongo` for log files. Directories must be empty (create a new database
    cluster).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据库文件和数据库实例日志创建目录结构。在所有服务器上使用相同的路径。例如，在Unix/macOS系统上使用`/data/db`用于数据库文件（WiredTiger存储），使用`/var/log/mongodb`用于日志文件，在Windows操作系统的情况下，使用`C:\data\db`目录用于数据文件，使用`C:\log\mongo`用于日志文件。目录必须为空（创建新的数据库集群）。
- en: 'Start up the `mongod` instance on each server with the replica set parameter
    `replSet`. To start a `mongod` instance, start an OS Command Prompt or terminal
    and execute the following command for Linux and macOS:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每台服务器上使用副本集参数`replSet`启动`mongod`实例。要启动`mongod`实例，请启动操作系统命令提示符或终端，并对Linux和macOS执行以下命令：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For Windows OSes, the command is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows操作系统，命令如下：
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following table lists the parameters and the description for each:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了每个参数及其描述：
- en: '![Figure 10.19: Description of the parameters in the commands'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.19：命令中参数的描述'
- en: '](img/B15507_10_19.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_19.jpg)'
- en: 'Figure 10.19: Description of the parameters in the commands'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19：命令中参数的描述
- en: 'Connect to the new cluster with mongo shell:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用mongo shell连接到新的集群：
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create the cluster config JSON document and save it in a JS variable (`cfg`):'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建集群配置JSON文档并将其保存在JS变量（`cfg`）中：
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding configuration steps are not real commands. `hostname1.domain`
    should be replaced with the real hostname and domain that matches DNS records.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置步骤不是真实的命令。`hostname1.domain`应替换为与DNS记录匹配的真实主机名和域名。
- en: 'Activate the cluster as follows:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按以下方式激活集群：
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Cluster activation saves the configuration and starts the cluster configuration.
    During the cluster configuration, there is an election process where member nodes
    decide on the new primary instance.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 集群激活保存配置并启动集群配置。在集群配置期间，成员节点进行选举过程，决定新的主实例。
- en: 'Once the configuration is activated, the shell prompt will display the cluster
    name (for example, `cluster0 : PRIMARY>`). Moreover, you can check the cluster
    status with the `rs.status()` command, which gives detailed information about
    the cluster and member servers. In the next exercise, you will set up a MongoDB
    cluster.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 配置激活后，shell提示将显示集群名称（例如，`cluster0：PRIMARY>`）。此外，您可以使用`rs.status()`命令检查集群状态，该命令提供有关集群和成员服务器的详细信息。在下一个练习中，您将设置一个MongoDB集群。
- en: 'Exercise 10.03: Building Your Own MongoDB Cluster'
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习10.03：构建您自己的MongoDB集群
- en: 'In this exercise, you will set up a new MongoDB cluster that will have three
    members. All `mongod` instances will be started on the local computer, and you
    need to set different directories for each server so that instances will not clash
    on the same datafiles. You will also need to use a different TCP port for each
    instance:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您将设置一个新的MongoDB集群，该集群将有三个成员。所有`mongod`实例将在本地计算机上启动，并且您需要为每台服务器设置不同的目录，以便实例不会在相同的数据文件上发生冲突。您还需要为每个实例使用不同的TCP端口：
- en: 'Create the file directories. For Windows OSes, this should be as follows:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文件目录。对于Windows操作系统，应该如下所示：
- en: '`C:\data\inst1`: For instance 1 datafiles'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`C:\data\inst1`：用于实例1数据文件'
- en: '`C:\data\inst2`: For instance 2 datafiles'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`C:\data\inst2`：用于实例2数据文件'
- en: '`C:\data\inst3`: For instance 3 datafiles'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`C:\data\inst3`：用于实例3数据文件'
- en: '`C:\data\log`: Log file destination'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`C:\data\log`：日志文件目的地'
- en: For Linux, the file directories are the following. Note that for MacOS, you
    can use any directory name of your choice instead of `/data`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux，文件目录如下。请注意，对于MacOS，您可以使用任何您选择的目录名称，而不是`/data`。
- en: '`/data/db/inst1`: For instance 1 datafiles'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`/data/db/inst1`：用于实例1数据文件'
- en: '`/data/db/inst2`: For instance 2 datafiles'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`/data/db/inst2`：用于实例2数据文件'
- en: '`/data/db/inst3`: For instance 3 datafiles'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`/data/db/inst3`：用于实例3数据文件'
- en: '`/var/log/mongodb`: Log file destination'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`/var/log/mongodb`：日志文件目的地'
- en: 'The following screenshot shows an example of this in Windows Explorer:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了Windows资源管理器中的示例：
- en: '![Figure 10.20: Directory structure'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.20：目录结构'
- en: '](img/B15507_10_20.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_20.jpg)'
- en: 'Figure 10.20: Directory structure'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20：目录结构
- en: 'For the various instances, use the following TCP ports:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于各个实例，使用以下TCP端口：
- en: 'Instance 1: 27001'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 实例1：27001
- en: 'Instance 2: 27002'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实例2：27002
- en: 'Instance 3: 27003'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 实例3：27003
- en: Use the replica set name `my_cluster`. The Oplog size should be 50 MB.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用副本集名称`my_cluster`。Oplog大小应为50 MB。
- en: Start the `mongod` instances from Windows Command Prompt. Use `start` to run
    the `mongod` startup command. This will create a new window for the process. Otherwise,
    the `start mongod` command might hang, and you will need to use another Command
    Prompt window. Note that you will need to use `sudo` instead of `start` for MacOS.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Windows命令提示符启动`mongod`实例。使用`start`来运行`mongod`启动命令。这将为该进程创建一个新窗口。否则，`start mongod`命令可能会挂起，您将需要使用另一个命令提示符窗口。请注意，对于MacOS，您需要使用`sudo`而不是`start`。
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `--logappend` parameter adds log messages at the end of the log file. Otherwise,
    the log file will be truncated each time you start the `mongod` instance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`--logappend`参数会在日志文件末尾添加日志消息。否则，每次启动`mongod`实例时，日志文件都会被截断。'
- en: 'Check the startup messages in the log destination folder (`C:\data\log`). Each
    instance has a separate log file, and at the end of the log, there should be a
    message as shown in the following code snippet:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查日志目标文件夹（`C:\data\log`）中的启动消息。每个实例都有一个单独的日志文件，在日志的末尾应该有一条消息，如下面的代码片段所示：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In a separate terminal (or Windows Command Prompt), connect to the cluster
    using mongo shell using the following command:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个单独的终端（或Windows命令提示符）中，使用以下命令连接到集群，使用mongo shell：
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following screenshot shows an example using mongo shell:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了使用mongo shell的示例：
- en: '![Figure 10.21: Output in mongo shell'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.21：mongo shell中的输出'
- en: '](img/B15507_10_21.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_21.jpg)'
- en: 'Figure 10.21: Output in mongo shell'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21：mongo shell中的输出
- en: Notice that the shell command prompt is just `>`, even though you connected
    with the `replicaSet` parameter in the connection string. That is because the
    cluster is not configured yet.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，尽管您在连接字符串中使用了`replicaSet`参数，但shell命令提示符只是`>`。这是因为集群尚未配置。 '
- en: 'Edit the cluster configuration JSON document (in the JS variable `cfg`):'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑集群配置JSON文档（在JS变量`cfg`中）：
- en: '[PRE27]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This code can be typed directly into mongo shell.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以直接输入到mongo shell中。
- en: 'Activate the cluster configuration as follows:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活集群配置如下：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that it usually takes some time for the cluster to activate the configuration
    and elect a new primary:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，集群通常需要一些时间来激活配置并选举新的主节点：
- en: '![Figure 10.22: Output in mongo shell'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.22：mongo shell中的输出'
- en: '](img/B15507_10_22.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_22.jpg)'
- en: 'Figure 10.22: Output in mongo shell'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.22：mongo shell中的输出
- en: 'The shell prompt should indicate the cluster connection (initially `mycluster:
    SECONDARY` and then `PRIMARY`) after the election process is completed and successful.
    If your prompt still shows `SECONDARY`, then try to reconnect or check the server
    logs for errors.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '在选举过程完成并成功后，shell提示应指示集群连接（最初为`mycluster: SECONDARY`，然后为`PRIMARY`）。如果您的提示仍然显示`SECONDARY`，请尝试重新连接或检查服务器日志以查找错误。'
- en: 'Verify the cluster configuration. For this, connect with mongo shell and verify
    that the prompt is `PRIMARY>`, and then run the following command to check the
    cluster status:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集群配置。为此，使用mongo shell连接并验证提示符是否为`PRIMARY>`，然后运行以下命令来检查集群状态：
- en: '[PRE29]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Run the following command to verify the current cluster configuration:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来验证当前的集群配置：
- en: '[PRE30]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Both commands return a long output with many details. The expected results
    are in the following screenshot (which shows a partial output):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令都返回了很多细节的长输出。预期结果如下截图所示（显示了部分输出）：
- en: '![Figure 10.23: Output in mongo shell'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.23：mongo shell中的输出'
- en: '](img/B15507_10_23.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_23.jpg)'
- en: 'Figure 10.23: Output in mongo shell'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.23：mongo shell中的输出
- en: In this exercise, you manually deployed all members of a replica set cluster
    on your local system. This exercise is for testing purposes only and should not
    be used for real applications. In real life, MongoDB cluster nodes should be deployed
    on separate servers, but the exercise gave a good inside look at a replica set's
    initial configuration and is especially useful for quick tests.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您手动部署了副本集群的所有成员到您的本地系统。这个练习仅用于测试目的，不应用于真实应用程序。在现实生活中，MongoDB集群节点应该部署在单独的服务器上，但这个练习为副本集的初始配置提供了一个很好的内部视图，对于快速测试特别有用。
- en: Enterprise Deployment
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 企业部署
- en: For large-scale enterprise applications, MongoDB provides integrated tools for
    managing deployments. It is easy to imagine why deploying and managing hundreds
    of MongoDB cluster servers could be an incredibly challenging task. Therefore,
    the ability to manage all deployments in an integrated interface is essential
    for large, enterprise-scale MongoDB environments.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模企业应用程序，MongoDB提供了用于管理部署的集成工具。可以想象，为何部署和管理数百个MongoDB集群服务器可能是一个非常具有挑战性的任务。因此，在大型企业规模的MongoDB环境中，能够在集成界面中管理所有部署是至关重要的。
- en: 'MongoDB provides two different interfaces:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB提供了两种不同的接口：
- en: '**MongoDB OPS Manager** is a package available for MongoDB Enterprise Advanced.
    It typically requires installation on-premises.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MongoDB OPS Manager**是MongoDB Enterprise Advanced可用的一个包。通常需要在本地安装。'
- en: '**MongoDB Cloud Manager** is a cloud-hosted service to manage MongoDB Enterprise
    deployments.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MongoDB Cloud Manager**是一个云托管服务，用于管理MongoDB企业部署。'
- en: Note
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both Cloud Manager and Atlas are cloud applications, but they provide different
    services. While Atlas is a fully managed database service, Cloud Manager is a
    service to manage database deployments, including local server infrastructure.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Manager和Atlas都是云应用程序，但它们提供不同的服务。虽然Atlas是一个完全托管的数据库服务，Cloud Manager是一个用于管理数据库部署的服务，包括本地服务器基础设施。
- en: Both applications provide similar functionality for enterprise users, with integrated
    automation for deployments, advanced graphical monitoring, and backup management.
    Using Cloud Manager, administrators are able to deploy all types of MongoDB servers
    (both single and clusters), while maintaining full control over the underlying
    infrastructure.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个应用程序为企业用户提供了类似的功能，包括部署的集成自动化、高级图形监控和备份管理。使用Cloud Manager，管理员可以部署所有类型的MongoDB服务器（单个和集群），同时保持对基础架构的完全控制。
- en: 'The following diagram shows the Cloud Manager architecture:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Cloud Manager的架构：
- en: '![Figure 10.24: Cloud Manager architecture'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.24：云管理器架构'
- en: '](img/B15507_10_24.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_24.jpg)'
- en: 'Figure 10.24: Cloud Manager architecture'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24：云管理器架构
- en: The architecture is based on a central management server and MongoDB Agent.
    Before a server can be managed in Cloud Manager, the MongoDB Agent needs to be
    deployed on the server.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构基于中央管理服务器和MongoDB代理。在Cloud Manager中管理服务器之前，需要在服务器上部署MongoDB代理。
- en: Note
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: MongoDB Agent software should not be confused with MongoDB database software.
    MongoDB Agent software is used for Cloud Manager and OPS Manager centralized management.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB代理软件不应与MongoDB数据库软件混淆。MongoDB代理软件用于Cloud Manager和OPS Manager的集中管理。
- en: With regard to Cloud Manager, users are not actually required to download and
    install MongoDB databases. All MongoDB versions are managed automatically by the
    deployment server once the agent is installed and the server is added to Cloud
    Manager configuration. MongoDB Agent will automatically download, stage, and install
    MongoDB server binaries on the server.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Cloud Manager，实际上并不需要用户下载和安装MongoDB数据库。一旦代理安装并将服务器添加到Cloud Manager配置中，所有MongoDB版本都将由部署服务器自动管理。MongoDB代理将自动下载、分阶段和安装服务器上的MongoDB服务器二进制文件。
- en: 'The following screenshot shows an example from MongoDB Cloud Manager:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了MongoDB Cloud Manager的一个示例：
- en: '![Figure 10.25: Cloud Manager screenshot'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.25：云管理器截图'
- en: '](img/B15507_10_25.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_25.jpg)'
- en: 'Figure 10.25: Cloud Manager screenshot'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25：云管理器截图
- en: The Cloud Manager web interface is similar to the Atlas application. One major
    difference between them is that Cloud Manager has more features. While Cloud Manager
    can manage your Atlas deployments, it has more complex options available for MongoDB
    Enterprise deployments.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Manager的Web界面类似于Atlas应用程序。它们之间的一个主要区别是Cloud Manager具有更多功能。虽然Cloud Manager可以管理Atlas部署，但对于MongoDB企业部署，它提供了更复杂的选项。
- en: The first step is to add a deployment (the `New Replica Set` button), and then
    to add servers to the deployment and install MongoDB agents. Once the MongoDB
    agent is installed on cluster members, the deployment is performed automatically
    by the agent.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是添加部署（`New Replica Set`按钮），然后向部署添加服务器并安装MongoDB代理。一旦MongoDB代理安装在集群成员上，部署将由代理自动执行。
- en: Note
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can test Cloud Manager for free for 30 days on MongoDB Cloud. The registration
    process is similar to the steps were shown in *Chapter 1*, *Introduction to MongoDB*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在MongoDB Cloud上免费测试Cloud Manager 30天。注册过程类似于*第1章*中展示的步骤，*MongoDB简介*。
- en: The MongoDB Atlas managed DBaaS cloud service is a great platform for quick
    and easy deployments. Most users will find Atlas their preferred choice for database
    deployments because the cloud environment is fully managed, secure, and always
    available. On the downside, the Atlas cloud service has some limitations for users
    when compared with Mongo DB on-premises. For example, Atlas does not allow users
    to access or tune the hardware and software infrastructure. If users want to have
    full control over the infrastructure, they can choose to manually deploy MongoDB
    databases. In the case of large enterprise database deployments, MongoDB provides
    software solutions such as Cloud Manager, which is useful for managing many cluster
    deployments while still having full control of the underlying infrastructure.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Atlas托管的DBaaS云服务是一个快速且易于部署的平台。大多数用户会发现Atlas是他们首选的数据库部署选择，因为云环境是完全托管的、安全的，并且始终可用。然而，与MongoDB本地部署相比，Atlas云服务对用户有一些限制。例如，Atlas不允许用户访问或调整硬件和软件基础设施。如果用户希望对基础设施拥有完全控制权，他们可以选择手动部署MongoDB数据库。对于大型企业数据库部署，MongoDB提供了Cloud
    Manager等软件解决方案，用于管理许多集群部署，同时仍然完全控制基础设施。
- en: Cluster Operations
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群操作
- en: Consider a scenario where one of your servers that is running a MongoDB database
    has reported memory errors. You are a bit worried because the computer is running
    the primary active member of your cluster. The server needs maintenance to replace
    the faulty **DIMM** (**Dual In-Line Memory Module**). You decide to switch over
    the primary instance to another server. The maintenance should take less than
    an hour, but you want to make sure that users can use their applications during
    the maintenance.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的运行MongoDB数据库的服务器之一报告了内存错误。您有点担心，因为该计算机正在运行您集群的主活动成员。服务器需要维护以更换故障的DIMM（双列直插式内存模块）。您决定将主实例切换到另一台服务器。维护应该不到一个小时，但您希望确保用户在维护期间可以使用他们的应用程序。
- en: MongoDB cluster operations refer to such day-to-day administration tasks that
    are necessary for cluster maintenance and monitoring. This is especially important
    for clusters deployed manually, where users must fully manage and operate replica
    set clusters. In the case of the Atlas DBaaS managed service, the only interaction
    is through the Atlas web application and most of the work is done behind the scenes
    by MongoDB. Therefore, our discussion will be limited to MongoDB clusters deployed
    manually, either in the local infrastructure or in cloud IaaS (Infrastructure
    as a Service).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB集群操作是指为了集群维护和监控而必要的日常管理任务。这对于手动部署的集群尤为重要，用户必须完全管理和操作副本集集群。在Atlas DBaaS托管服务的情况下，唯一的交互是通过Atlas
    Web应用程序进行的，大部分工作都是由MongoDB在后台完成的。因此，我们的讨论将局限于手动部署的MongoDB集群，无论是在本地基础设施还是在云IaaS（基础设施即服务）中。
- en: Adding and Removing Members
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加和移除成员
- en: 'New members can be added to replica sets with the command `rs.add()`. Before
    we can add a new member, the `mongod` instance needs to be prepared and started
    with the same `—replSet` cluster name option. The same rules apply to new cluster
    members. For example, starting the new `mongod` instance would look as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用命令`rs.add()`将新成员添加到副本集。在添加新成员之前，需要准备并使用相同的`—replSet`集群名称选项启动`mongod`实例。新集群成员也适用相同的规则。例如，启动新的`mongod`实例如下所示：
- en: '[PRE31]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before we add a new member to an existing replica set, though, we need to decide
    on the type of cluster member. The following options are available for this:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在向现有副本集添加新成员之前，我们需要决定成员的类型。有以下选项可供选择：
- en: '![Figure 10.26: Descriptions for the member types'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.26：成员类型的描述'
- en: '](img/B15507_10_26.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_26.jpg)'
- en: 'Figure 10.26: Descriptions for the member types'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.26：成员类型的描述
- en: Adding a Member
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加成员
- en: 'There are a few arguments that can be passed when we add a new cluster member,
    depending on the member type. In its simplest form, the `add` command has only
    one parameter—a string containing the hostname and port of the new instance:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加新的集群成员时，可以传递一些参数，这取决于成员类型。在最简单的形式中，“add”命令只有一个参数——包含新实例的主机名和端口的字符串：
- en: '[PRE32]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Keep in mind the following while adding a member:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加成员时请记住以下事项：
- en: A `SECONDARY` member should be added to the cluster.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应向集群添加`SECONDARY`成员。
- en: Priority can be any number between `0` and `1000`. If this instance were to
    be elected as the primary, the priority must be set greater than `0`. Otherwise,
    the instance is considered `READ ONLY`. Moreover, the priority must be `0` for
    the `HIDDEN`, `DELAY`, and `ARBITER` instance types. The default value is `1`.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先级可以是`0`到`1000`之间的任何数字。如果此实例被选为主节点，则优先级必须大于`0`。否则，该实例被视为`只读`。此外，`HIDDEN`、`DELAY`和`ARBITER`实例类型的优先级必须为`0`。默认值为`1`。
- en: All nodes have one vote by default. In version 4.4, a node can have either 0
    votes or 1 vote. There can be a maximum of 7 voting members—with one vote each.
    The rest of the nodes are not participating in the election process, having 0
    votes. The default value is 1.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点默认都有一票。在4.4版本中，节点可以有0票或1票。最多可以有7个投票成员，每个成员一票。其余节点不参与选举过程，票数为0。默认值为1。
- en: 'The following screenshot shows an example of adding a member:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了添加成员的示例：
- en: '![Figure 10.27: Example of adding a member'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.27：添加成员示例'
- en: '](img/B15507_10_27.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_27.jpg)'
- en: 'Figure 10.27: Example of adding a member'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.27：添加成员示例
- en: 'In the preceding screenshot, `"ok" : 1` indicates that the add member operation
    was successful. In the new instance logs, the initial sync (database copy) is
    started for the new replica set member:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，“ok”：1表示添加成员操作成功。在新实例日志中，新的副本集成员的初始同步（数据库复制）已经开始：
- en: '[PRE33]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`0` adds a different member type, but the `add` command can be different. For
    example, to add a hidden member with a vote, add the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`0`添加了不同的成员类型，但`add`命令可能不同。例如，要添加一个带有投票的隐藏成员，请添加以下内容：'
- en: '[PRE34]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If successful, the `add` command will do the following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，`add`命令将执行以下操作：
- en: Change the cluster configuration by adding the new member node
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加新成员节点更改集群配置
- en: Perform the initial sync—the database is copied to the new member instance (except
    in the case of `ARBITER`)
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行初始同步——数据库被复制到新的成员实例（除了`ARBITER`的情况）
- en: In some situations, adding a new member can change the current primary.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，添加新成员可能会改变当前的主节点。
- en: Note
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The new member cluster must have an empty database (empty data directory) before
    joining the replica set cluster. Oplog operations that are generated on the primary
    node during the sync process are also copied and applied to the new cluster member.
    The synchronization process may take a long time, especially if synchronization
    is running over the internet.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 新成员集群在加入副本集集群之前必须具有空数据库（空数据目录）。在同步过程中在主节点上生成的Oplog操作也会被复制并应用到新的集群成员上。同步过程可能需要很长时间，特别是如果同步是通过互联网进行的。
- en: Removing a Member
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除成员
- en: 'Cluster members can be removed by connecting to the cluster and running the
    following command:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过连接到集群并运行以下命令来移除集群成员：
- en: '[PRE35]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Removing a cluster member does not remove the instance and datafiles. The instance
    can be started in single-server mode (without the `—replSet` option), and datafiles
    will contain the latest updates from before it was removed.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 移除集群成员不会移除实例和数据文件。实例可以在单服务器模式下启动（不带`—replSet`选项），数据文件将包含被移除之前的最新更新。
- en: Reconfiguring a Cluster
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新配置集群
- en: 'Cluster reconfiguration may be necessary if you want to make more complex changes
    to a replica set, such as adding multiple nodes in one step or editing the default
    values for votes and priority. Clusters can be reconfigured by running the following
    command:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要对副本集进行更复杂的更改，例如一次添加多个节点或编辑投票和优先级的默认值，则可能需要重新配置集群。可以通过运行以下命令来重新配置集群：
- en: '[PRE36]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is a step-by-step breakdown of a cluster reconfiguration with
    a different priority for each node:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对具有不同优先级的每个节点进行集群重新配置的逐步分解：
- en: 'Save the configuration in a JS variable as follows:'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将配置保存在JS变量中，如下所示：
- en: '[PRE37]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Edit `new_conf` to change the default priority by adding the following snippet:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑`new_conf`以通过添加以下片段更改默认优先级：
- en: '[PRE38]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Enable the new configuration as follows:'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用新配置如下：
- en: '[PRE39]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following screenshot shows an example of cluster reconfiguration:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了集群重新配置的示例：
- en: '![Figure 10.28: Example of cluster reconfiguration'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.28：集群重新配置示例'
- en: '](img/B15507_10_28.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_28.jpg)'
- en: 'Figure 10.28: Example of cluster reconfiguration'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.28：集群重新配置示例
- en: Failover
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障转移
- en: 'In certain situations, the MongoDB cluster could initiate an election process.
    In data center operations terminology, these types of events are usually called
    **Failover** and **Switchover**:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，MongoDB集群可能会启动选举过程。在数据中心运营术语中，这些类型的事件通常称为**故障转移**和**切换**：
- en: '**Failover** is always a result of an incident. When one or more cluster members
    become unavailable (usually because of a failure or network outage) the cluster
    fails over. The replica set detects that some of the nodes become unavailable,
    and the replica set election is automatically started.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移**总是由事件引起的。当一个或多个集群成员变得不可用（通常是因为故障或网络中断）时，集群将进行故障转移。副本集检测到一些节点变得不可用，并自动启动副本集选举。'
- en: Note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: How does a replica set cluster detect an incident? Member servers regularly
    communicate between themselves—sending/receiving a heartbeat network request every
    couple of seconds. If one member does not reply for a longer time (the default
    is 10 seconds), then the member is declared unavailable and a new cluster election
    is initiated.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 复制集群如何检测故障？成员服务器定期进行通信，每隔几秒发送/接收心跳网络请求。如果一个成员在较长时间内没有回复（默认为10秒），则该成员被宣布不可用，并启动新的集群选举。
- en: '**Switchover** is a user-initiated process (that is, initiated by a server
    command). The purpose of switchover is to perform planned maintenance on the cluster.
    For example, the server running the primary member needs to restart for OS patching,
    and the administrator switches the primary over to another cluster member.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切换**是用户发起的过程（即由服务器命令发起）。切换的目的是对集群进行计划维护。例如，运行主成员的服务器需要重新启动进行操作系统修补，管理员将主切换到另一个集群成员。'
- en: Regardless of whether it is a failover or a switchover, the election mechanism
    is started, and the cluster aims to achieve a new majority and, if successful,
    a new primary node. During the election process, there is a transition period
    when writes are not possible on the database and client sessions will reconnect
    to the new primary member. Application coding should be able to handle MongoDB
    failover events transparently for users.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是故障转移还是切换，选举机制都会启动，集群旨在实现新的多数，并在成功时成为新的主节点。在选举过程中，存在一个过渡期，在此期间数据库上无法进行写操作，客户端会重新连接到新的主成员。应用程序编码应能够透明地处理MongoDB故障转移事件。
- en: In Atlas, failovers are managed automatically by MongoDB, so no user involvement
    is required. In larger Atlas deployments (such as M10+), the `Test Failover` button
    is available in the Atlas application. The `Test Failover` button will force a
    cluster failover for application testing. If the new cluster majority cannot be
    achieved, then all nodes will stay in the secondary state and no primary will
    be elected. In this situation, the clients will not be able to modify any data
    in the database. However, the read-only operations are still possible on all secondary
    nodes regardless of the cluster status.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atlas中，MongoDB会自动管理故障转移，因此不需要用户参与。在较大的Atlas部署中（例如M10+），Atlas应用程序中提供了“测试故障转移”按钮。该按钮将强制应用程序测试进行集群故障转移。如果无法实现新的集群多数，那么所有节点将保持次要状态，不会选举出主节点。在这种情况下，客户端将无法修改数据库中的任何数据。但是，无论集群状态如何，所有次要节点上仍然可以进行只读操作。
- en: Failover (Outage)
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障转移（故障）
- en: 'In the event of outages, usually, messages such as the one in the following
    code snippet can be seen in the instance logs:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在发生故障时，通常可以在实例日志中看到以下代码片段中的消息：
- en: '[PRE40]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The client session (in other words, the connection pool) will automatically
    reconnect to the remaining nodes, and the activity can continue as normal. Once
    the missing node is restarted, it will rejoin the cluster automatically. If the
    cluster cannot successfully complete election with the available nodes, then the
    failover is not considered successful. In the logs, we can see a message like
    this:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端会自动重新连接到剩余节点，并且活动可以像往常一样继续。一旦缺失的节点重新启动，它将自动重新加入集群。如果集群无法成功完成选举，则故障转移不被视为成功。在日志中，我们可以看到这样的消息：
- en: '[PRE41]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In this case, the client connection is dropped, and users are not able to reconnect
    unless the read preference is set to `secondary`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，客户端连接会中断，并且用户无法重新连接，除非读取偏好设置为`secondary`：
- en: '[PRE42]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Even if election is not successful, the users are able to connect with a read
    preference `secondary` setting, as in the following connection string:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 即使选举不成功，用户也可以使用读取偏好`secondary`设置进行连接，如以下连接字符串所示：
- en: '[PRE43]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is not possible to open the database instance in read-write mode (the primary
    state) unless there are sufficient nodes to form a cluster majority. One typical
    mistake is to reboot many secondary members at the same time. If the cluster detects
    that the majority is lost, then the primary state member will step down to secondary.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 除非有足够的节点形成集群多数，否则不可能以读写模式（主状态）打开数据库实例。一个典型的错误是同时重新启动多个次要成员。如果集群检测到多数丢失，那么主状态成员将降级为次要成员。
- en: Rollback
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回滚
- en: In some situations, failover events could generate rollbacks of writes on the
    former primary node. This may happen if writes on the primary were performed with
    the default write concern (`w:1`), and the former primary crashed before it had
    the chance to replicate changes to any secondary node. The cluster forms a new
    majority, and the activity will continue with a new primary. When the former primary
    is back up, it needs to roll back those (previously un-replicated) transactions
    before it can get in sync with the new primary.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，故障转移事件可能会导致在以前的主节点上回滚写操作。如果在主节点上使用默认的写关注（`w:1`）执行写操作，并且以前的主节点在有机会将更改复制到任何次要节点之前崩溃，则可能会发生这种情况。集群形成新的多数，活动将继续进行，并且会有一个新的主节点。以前的主节点恢复后，需要回滚这些（以前未复制的）事务，然后才能与新的主节点同步。
- en: 'The chances of rollback could be reduced by setting write concern to `majority`
    (`w: ''majority''`)—that is, by obtaining acknowledgment from most cluster nodes
    (the majority) for every database write. On the downside, this could slow down
    the writes for the application.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '通过将写关注设置为`majority`（`w: ''majority''`）可以减少回滚的可能性，即通过从大多数集群节点（多数）获得每个数据库写操作的确认。不利的一面是，这可能会减慢应用程序的写入速度。'
- en: Normally, failures and outages are remedied quickly, and the affected nodes
    rejoin the cluster when they are back up. However, if the outage is taking a long
    time (for example, a week), then the secondary instances could become **stale**.
    A stale instance will not be able to resynchronize data with the primary member
    after a restart. In that case, the instance should be added as a new member (empty
    data directory) or from a recent database backup.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，故障和停机会很快得到解决，并且受影响的节点在恢复时重新加入集群。但是，如果停机时间很长（例如一周），那么辅助实例可能会变得过时。过时的实例在重新启动后将无法与主成员重新同步数据。在这种情况下，该实例应被添加为新成员（空数据目录）或从最近的数据库备份中添加。
- en: Switchover (Stepdown)
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 切换（Stepdown）
- en: 'For maintenance activities, we often need to transfer the primary state from
    one instance to another. For this, the user admin command to be executed on the
    primary is as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 对于维护活动，我们经常需要将主状态从一个实例转移到另一个实例。为此，在主节点上要执行的用户admin命令如下：
- en: '[PRE44]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `stepDown` command will force the primary node to step down and cause the
    secondary node with the highest priority to step up as the new primary node. The
    primary node will step down only if the secondary node is up to date. Therefore,
    switchover is a safer operation compared to failover. There is no risk of losing
    writes on a former primary member.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`stepDown`命令将强制主节点下台，并导致优先级最高的辅助节点上台成为新的主节点。只有在辅助节点是最新的情况下，主节点才会下台。因此，与故障切换相比，切换是一种更安全的操作。在以前的主成员上没有丢失写入的风险。'
- en: 'The following screenshot shows an example of this:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了一个示例：
- en: '![Figure 10.29: Using the stepDown command'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.29：使用stepDown命令'
- en: '](img/B15507_10_29.jpg)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_29.jpg)'
- en: 'Figure 10.29: Using the stepDown command'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.29：使用stepDown命令
- en: 'You can verify the current master node by running the following command:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令来验证当前的主节点：
- en: '[PRE45]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that in order for a switchover to be successful, the target cluster member
    must be configured with a higher priority. A member with a default priority (`priority
    = 0`) will never become a primary.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使切换成功，目标集群成员必须配置为具有更高的优先级。具有默认优先级（`priority = 0`）的成员永远不会成为主要成员。
- en: 'Exercise 10.04: Performing Database Maintenance'
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习10.04：执行数据库维护
- en: 'In this exercise, you will perform cluster maintenance on a primary node. First,
    you will switch over to the secondary server, `inst2`, so that the current primary
    server will become secondary. Then, you will shut down the former primary server
    for maintenance and restart the former primary and switch over:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您将在主节点上执行集群维护。首先，您将切换到辅助服务器`inst2`，以便当前的主服务器将变为辅助服务器。然后，您将关闭以前的主服务器进行维护，并重新启动以前的主服务器并进行切换：
- en: Note
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before you start this exercise, prepare the cluster script and directories as
    per the steps given in *Exercise 10.02*, *Checking the Cluster Replication*.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个练习之前，按照*练习10.02*中给出的步骤准备好集群脚本和目录。
- en: Start up all cluster members (if not already started), connect with mongo shell,
    and verify the configuration and the current master node with `rs.isMaster().primary`.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动所有集群成员（如果尚未启动），连接到mongo shell，并使用`rs.isMaster().primary`验证配置和当前主节点。
- en: Reconfigure the cluster. For this, copy the existing cluster configuration into
    a variable, `sw_over`, and set the read-only member priority. For `inst3`, the
    priority should be set to `0` (read-only).
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新配置集群。为此，将现有的集群配置复制到一个变量`sw_over`中，并设置只读成员的优先级。对于`inst3`，优先级应设置为`0`（只读）。
- en: '[PRE46]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Switch over to `inst2`. On the primary node, run the `stepDown` command as follows:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到`inst2`。在主节点上，运行以下`stepDown`命令：
- en: '[PRE47]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Verify that the new primary is `inst2` by using the following command:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证新的主节点是否为`inst2`：
- en: '[PRE48]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, `inst1` can be stopped for hardware maintenance.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`inst1`可以停止进行硬件维护。
- en: 'Shut down the instance locally using the following command:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在本地关闭实例：
- en: '[PRE49]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output for this should be as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出应该是这样的：
- en: '![Figure 10.30: Output in mongo shell'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.30：在mongo shell中的输出'
- en: '](img/B15507_10_30.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15507_10_30.jpg)'
- en: 'Figure 10.30: Output in mongo shell'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.30：在mongo shell中的输出
- en: In this exercise, you practiced the switchover steps in a cluster. The commands
    are quite simple. Switchover is a good practice to test how applications handle
    MongoDB cluster events.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您练习了集群中的切换步骤。命令非常简单。切换是一个很好的实践，可以测试应用程序如何处理MongoDB集群事件。
- en: 'Activity 10.01: Testing a Disaster Recovery Procedure for a MongoDB Database'
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动10.01：测试MongoDB数据库的灾难恢复程序
- en: 'Your company is about to become public, and as a result, some certifications
    are necessary to prove that a business continuity plan is in place in case of
    disaster. One of the requirements is to implement and test a disaster recovery
    procedure for a MongoDB database. The cluster architecture is distributed between
    the main office (primary instance) and a remote office (secondary instance), which
    is the disaster recovery location. To help with MongoDB replica set elections
    in case of a network split, an arbiter node is installed in a third separate location.
    Once a year, the DR plan is tested by simulating a crash of all cluster members
    in the main office, and this year, that task falls to you. The following steps
    will help you to complete this activity:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 您的公司即将上市，因此需要一些证书来证明在灾难发生时已经制定了业务连续性计划。其中一个要求是为MongoDB数据库实施并测试灾难恢复程序。集群架构分布在主办公室（主实例）和远程办公室（辅助实例）之间，后者是灾难恢复位置。为了帮助在网络分裂的情况下进行MongoDB副本集选举，还在第三个独立位置安装了一个仲裁者节点。每年一次，灾难恢复计划都会通过模拟主办公室中所有集群成员的崩溃来进行测试，而今年，这项任务就落在了您身上。以下步骤将帮助您完成此活动：
- en: Note
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you have multiple computers, it is a good idea to try the activity with two
    or three computers, with each computer emulating a physical location. In the solution,
    however, this activity will be completed by starting all instances on the same
    local computer. All secondary databases (including DR) should be in sync with
    the primary database when the activity is started.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有多台计算机，最好尝试使用两台或三台计算机进行此操作，每台计算机模拟一个物理位置。但是，在解决方案中，此操作将通过在同一台本地计算机上启动所有实例来完成。所有辅助数据库（包括DR）在启动活动时应与主数据库同步。
- en: 'Configure a `sale-cluster` cluster with three members:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用三个成员配置`sale-cluster`集群：
- en: '`sale-prod`: Primary'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '`sale-prod`：主要'
- en: '`sale-dr`: Secondary'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '`sale-dr`：次要'
- en: '`sale-ab`: Arbiter (third location)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '`sale-ab`：仲裁者（第三位置）'
- en: Insert test data records into the primary collection.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将测试数据记录插入主要集合。
- en: Simulate a disaster. Reboot the primary node (that is, kill the current `mongod`
    primary instance).
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟灾难。重新启动主节点（即，终止当前的`mongod`主实例）。
- en: Perform testing on DR by inserting a few documents.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过插入一些文档在DR上执行测试。
- en: Shut down the DR instance.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭DR实例。
- en: Restart all nodes for the main office.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动主办公室的所有节点。
- en: After 10 minutes, start up the DR instance.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10分钟后，启动DR实例。
- en: Observe the rollback of inserted test records and re-sync with the primary.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察插入的测试记录的回滚并重新与主数据库同步。
- en: 'After restarting `sales_dr`, you should see a rollback message in the logs.
    The following code snippet shows an example of this:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动`sales_dr`后，您应该在日志中看到回滚消息。以下代码片段显示了一个示例：
- en: '[PRE50]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via [this link](B15507_Solution_Final_SZ_ePub.xhtml#_idTextAnchor479).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过[此链接](B15507_Solution_Final_SZ_ePub.xhtml#_idTextAnchor479)找到此活动的解决方案。
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned that MongoDB replica sets are essential for providing
    high availability and load sharing in a MongoDB database environment. While Atlas
    transparently provides support for infrastructure and software (including for
    replica set cluster management), not all MongoDB clusters are deployed in Atlas.
    In this chapter, we discussed the concepts and operations of replica set clusters.
    Learning about simple concepts for clusters, such as read preference, can help
    developers build more reliable, high-performance applications in the cloud. In
    the next chapter, you will learn about backup and restore operations in MongoDB.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解到MongoDB副本集对于在MongoDB数据库环境中提供高可用性和负载共享至关重要。虽然Atlas透明地为基础设施和软件（包括副本集群管理）提供支持，但并非所有MongoDB集群都部署在Atlas中。在本章中，我们讨论了副本集群的概念和操作。了解有关集群的简单概念，例如读取首选项，可以帮助开发人员在云中构建更可靠、高性能的应用程序。在下一章中，您将了解MongoDB中的备份和还原操作。
