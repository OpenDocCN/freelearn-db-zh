- en: Harnessing Big Data with MongoDB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用MongoDB进行大数据处理
- en: MongoDB is often used in conjunction with big data pipelines because of its
    performance, flexibility, and lack of rigorous data schemas. This chapter will
    explore the big data landscape, and how MongoDB fits alongside message queuing,
    data warehousing, and extract, transform, load pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB通常与大数据管道一起使用，因为它具有高性能、灵活性和缺乏严格的数据模式。本章将探讨大数据领域以及MongoDB如何与消息队列、数据仓库和ETL管道配合使用。
- en: 'The topics that we will discuss in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章讨论以下主题：
- en: What is big data?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是大数据？
- en: Message queuing systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息队列系统
- en: Data warehousing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库
- en: A big data use case using Kafka, Spark on top of HDFS, and MongoDB
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka、Spark在HDFS上以及MongoDB的大数据用例
- en: What is big data?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是大数据？
- en: In the last five years, the number of people accessing and using the internet
    has almost doubled from a little under 2 billion to around 3.7 billion. Half of
    the global population are now online.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的五年里，访问和使用互联网的人数几乎翻了一番，从不到20亿增加到约37亿。全球一半的人口现在都在网上。
- en: With the number of internet users increasing, and with networks evolving, more
    data is being added to existing datasets each year. In 2016, global internet traffic
    was 1.2 zettabytes (which is 1.2 billion terabytes) and it is expected to grow
    to 3.3 zettabytes by 2021.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网用户数量的增加，以及网络的发展，每年都会向现有数据集中添加更多的数据。2016年，全球互联网流量为1.2泽字节（相当于1.2亿兆字节），预计到2021年将增长到3.3泽字节。
- en: This enormous amount of data that is generated every year means that it is imperative
    that databases and data stores in general can scale and process our data efficiently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每年产生的大量数据意味着数据库和数据存储通常必须能够高效扩展和处理我们的数据。
- en: The term **big data** was first coined in the 1980's by John Mashey ([http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf](http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf)),
    and mostly came into play in the past decade with the explosive growth of the
    internet. Big data typically refers to datasets that are too large and complex
    to be processed by traditional data processing systems, and so need some kind
    of specialized system architecture to be processed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**大数据**这个术语最早是由John Mashey在1980年代提出的（[http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf](http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf)），并且在过去的十年中随着互联网的爆炸性增长而开始流行起来。大数据通常指的是那些传统数据处理系统无法处理的过大和复杂的数据集，因此需要一些专门的系统架构来处理。'
- en: 'Big data''s defining characteristics are as follows, in general:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的定义特征通常如下：
- en: Volume
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量
- en: Variety
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样性
- en: Velocity
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度
- en: Veracity
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实性
- en: Variability
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变异性
- en: Variety and variability refer to the fact that our data comes in different forms
    and our datasets have internal inconsistencies. These need to be smoothed out
    by a data cleansing and normalization system before we can actually process our
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性和变异性指的是我们的数据以不同的形式出现，我们的数据集存在内部不一致性。这些需要通过数据清洗和规范化系统进行平滑处理，然后我们才能实际处理我们的数据。
- en: Veracity refers to the uncertainty of the quality of data. Data quality may
    vary, with perfect data for some dates and missing datasets for others. This affects
    our data pipeline and how much we can invest in our data platforms, since, even
    today, one out of three business leaders don't completely trust the information
    they use to make business decisions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性指的是数据质量的不确定性。数据质量可能会有所不同，对于某些日期来说是完美的数据，而对于其他日期来说则是缺失的数据集。这影响了我们的数据管道以及我们可以投入到数据平台中的数量，因为即使在今天，三分之一的商业领导人也不完全信任他们用来做出商业决策的信息。
- en: Finally, velocity is probably the most important defining characteristic of
    big data (other than the obvious volume attribute) and it refers to the fact that
    big datasets not only have large volumes of data, but also grow at an accelerated
    pace. This makes traditional storage using, for example, indexing a difficult
    task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，速度可能是大数据最重要的定义特征（除了明显的容量属性），它指的是大数据集不仅具有大量数据，而且增长速度加快。这使得传统的存储方式，比如索引，成为一项困难的任务。
- en: The big data landscape
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据领域
- en: Big data has evolved into a complex ecosystem affecting every sector of the
    economy. Going from hype to unrealistic expectations and back to reality, we now
    have big data systems implemented and deployed in most Fortune 1000 companies
    that deliver real value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据已经发展成一个影响经济各个领域的复杂生态系统。从炒作到不切实际的期望，再到现实，如今大多数财富1000强公司都实施和部署了大数据系统，为企业创造了真正的价值。
- en: 'If we segment the companies that participate in the big data landscape by industry,
    we would probably come up with the following sections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们按行业对参与大数据领域的公司进行分段，可能会得出以下几个部分：
- en: Infrastructure
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施
- en: Analytics
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析
- en: Applications-enterprise
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用-企业
- en: Applications-industry
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用-行业
- en: Cross-infrastructure analytics
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨基础设施分析
- en: Data sources and APIs
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来源和API
- en: Data resources
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据资源
- en: Open source
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源
- en: From an engineering point of view, we are probably more concerned about the
    underlying technologies than their applications in different industry sectors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程角度来看，我们可能更关心的是底层技术，而不是它们在不同行业领域的应用。
- en: 'Depending on our business domain, we may have data coming in from different
    sources, such as transactional databases, IoT sensors, application server logs,
    other websites via a web service API, or just plain web page content extraction:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的业务领域，我们可能会从不同的来源获取数据，比如事务性数据库、物联网传感器、应用服务器日志、通过Web服务API的其他网站，或者只是纯粹的网页内容提取：
- en: '![](img/810c5ec0-b3c4-469b-a7fd-c26d1bc1dadf.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/810c5ec0-b3c4-469b-a7fd-c26d1bc1dadf.png)'
- en: Message queuing systems
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息队列系统
- en: In most of the flows previously described, we have data being **extracted, transformed,
    loaded** (**ETL**) into an **enterprise data warehouse** (**EDW**). To extract
    and transform this data, we need a message queuing system to deal with spikes
    in traffic, endpoints being temporarily unavailable, and other issues that may
    affect the availability and scalability of this part of the system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前描述的大多数流程中，我们有数据被提取、转换、加载（ETL）到企业数据仓库（EDW）。为了提取和转换这些数据，我们需要一个消息队列系统来处理流量激增、临时不可用的端点以及可能影响系统可用性和可伸缩性的其他问题。
- en: Message queues also provide decoupling between producers and consumers of messages.
    This allows for better scalability by partitioning our messages into different
    topics/queues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列还可以在消息的生产者和消费者之间提供解耦。这通过将我们的消息分成不同的主题/队列来实现更好的可伸缩性。
- en: Finally, using message queues, we can have location-agnostic services that don't
    care where the message producers sit, which provide interoperability between different
    systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用消息队列，我们可以拥有不关心消息生产者所在位置的位置不可知服务，这提供了不同系统之间的互操作性。
- en: In the message queuing world, the most popular systems in production at the
    time of writing this book are RabbitMQ, ActiveMQ, and Kafka. We will provide a
    small overview of them before we dive into our use case to bring all of them together.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息队列世界中，目前在生产中最受欢迎的系统是RabbitMQ、ActiveMQ和Kafka。在我们深入研究使用案例之前，我们将对它们进行简要概述。
- en: Apache ActiveMQ
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache ActiveMQ
- en: Apache ActiveMQ is an open source message broker, written in Java, together
    with a full **Java Message Service** (**JMS**) client.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Apache ActiveMQ是一个用Java编写的开源消息代理，配有完整的Java消息服务（JMS）客户端。
- en: It is the most mature implementation out of the three that we examine here,
    and has a long history of successful production deployments. Commercial support
    is offered by many companies, including Red Hat.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 它是我们在这里检查的三种实现中最成熟的，有着成功的生产部署的悠久历史。许多公司提供商业支持，包括Red Hat。
- en: It is a fairly simple queuing system to set up and manage. It is based on the
    JMS client protocol and is the tool of choice for Java EE systems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的排队系统，可以轻松设置和管理。它基于JMS客户端协议，是Java EE系统的首选工具。
- en: RabbitMQ
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RabbitMQ
- en: RabbitMQ, on the other hand, is written in Erlang and is based on the **Advanced
    Message Queuing Protocol** (**AMQP**) protocol. AMQP is significantly more powerful
    and complicated than JMS, as it allows peer-to-peer messaging, request/reply,
    and publish/subscribe models for one-to-one or one-to-many message consumption.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，RabbitMQ是用Erlang编写的，基于高级消息队列协议（AMQP）协议。AMQP比JMS更强大和复杂，因为它允许点对点消息传递、请求/响应和发布/订阅模型，用于一对一或一对多的消息消费。
- en: RabbitMQ has gained popularity in the past 5 years, and is now the most searched-for
    queuing system.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的5年中，RabbitMQ变得越来越受欢迎，现在是搜索量最大的排队系统。
- en: 'RabbitMQ''s architecture is outlined as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ的架构概述如下：
- en: '![](img/b4473fae-e41f-43d5-b002-2a30e6ff17ba.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4473fae-e41f-43d5-b002-2a30e6ff17ba.png)'
- en: Scaling in RabbitMQ systems is performed by creating a cluster of RabbitMQ servers.
    Clusters share data and state, which are replicated, but message queues are distinct
    per node. To achieve high availability we can also replicate queues in different
    nodes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ系统的扩展是通过创建一组RabbitMQ服务器集群来完成的。集群共享数据和状态，这些数据和状态是复制的，但消息队列在每个节点上是独立的。为了实现高可用性，我们还可以在不同节点中复制队列。
- en: Apache Kafka
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Kafka, on the other hand, is a queuing system that was first developed by LinkedIn
    for its own internal purposes. It is written in Scala and is designed from the
    ground up for horizontal scalability and the best performance possible.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Kafka是由LinkedIn首先为其自身内部目的开发的排队系统。它是用Scala编写的，从根本上设计为水平可伸缩和尽可能高的性能。
- en: Focusing on performance is a key differentiator for Apache Kafka, but it means
    that in order to achieve performance, we need to sacrifice something. Messages
    in Kafka don't hold unique IDs, but are addressed by their offset in the log.
    Apache Kafka consumers are not tracked by the system; it is the responsibility
    of the application design to do so. Message ordering is implemented at the partition
    level and it is the responsibility of the consumer to identify if a message has
    been delivered already.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于性能是Apache Kafka的关键区别因素，但这意味着为了实现性能，我们需要牺牲一些东西。Kafka中的消息没有唯一的ID，而是通过它们在日志中的偏移量来寻址。Apache
    Kafka消费者不受系统跟踪；这是应用程序设计的责任。消息排序是在分区级别实现的，消费者有责任确定消息是否已经被传递。
- en: 'Semantics were introduced in version 0.11 and are part of the latest 1.0 release
    so that messages can now be both strictly ordered within a partition and always
    arrive exactly once for each consumer:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 语义学是在0.11版本中引入的，并且是最新的1.0版本的一部分，因此消息现在可以在分区内严格排序，并且每个消费者始终只能到达一次：
- en: '![](img/5c856121-fad0-42bb-a2fb-6c572334bbc6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c856121-fad0-42bb-a2fb-6c572334bbc6.png)'
- en: Data warehousing
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据仓库
- en: 'Using a message queuing system is just the first step in our data pipeline
    design. At the other end of message queuing, we would typically have a data warehouse
    to process the vast amount of data that arrives. There are numerous options there,
    and it is not the main focus of this book to go over these or compare them. However,
    we will skim through two of the most widely-used options from the Apache Software
    Foundation: Apache Hadoop and Apache Spark.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用消息队列系统只是我们数据管道设计的第一步。在消息队列的另一端，我们通常会有一个数据仓库来处理大量到达的数据。那里有很多选择，本书的重点不是讨论这些选择或进行比较。然而，我们将简要介绍Apache软件基金会中最广泛使用的两个选项：Apache
    Hadoop和Apache Spark。
- en: Apache Hadoop
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop
- en: The first, and probably still most widely used, framework for big data processing
    is Apache Hadoop. Its foundation is the **Hadoop Distributed File System** (**HDFS**).
    Developed at Yahoo! in the 2000s, it originally served as an open source alternative
    to **Google File System** (**GFS**), a distributed filesystem that was serving
    Google's needs for distributed storage of its search index.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个，也可能仍然是最广泛使用的大数据处理框架是Apache Hadoop。它的基础是**Hadoop分布式文件系统**（**HDFS**）。在2000年代由Yahoo!开发，最初是作为**Google文件系统**（**GFS**）的开源替代品，GFS是谷歌用于分布式存储其搜索索引的文件系统。
- en: Hadoop also implemented a MapReduce alternative to Google's proprietary system,
    Hadoop MapReduce. Together with HDFS, they constitute a framework for distributed
    storage and computations. Written in Java, with bindings for most programming
    languages and many projects that provide abstracted and simple functionality,
    and sometimes based on SQL querying, it is a system that can reliably be used
    to store and process terabytes, or even petabytes, of data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop还实现了一个MapReduce替代方案，用于谷歌专有系统的Hadoop MapReduce。与HDFS一起，它们构成了一个分布式存储和计算的框架。用Java编写，具有大多数编程语言的绑定和许多提供抽象和简单功能的项目，有时基于SQL查询，这是一个可靠地用于存储和处理几十亿甚至拍它字节数据的系统。
- en: In later versions, Hadoop became more modularized by introducing **Yet Another
    Resource Negotiator** (**YARN**), which provides the abstraction for applications
    to be developed on top of Hadoop. This has enabled several applications to be
    deployed on top of Hadoop, such as **Storm**, **Tez**, **OpenMPI**, **Giraph**,
    and, of course, **Apache Spark**, as we will see in the following sections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续版本中，Hadoop通过引入**Yet Another Resource Negotiator**（**YARN**）变得更加模块化，为应用程序提供了在Hadoop之上开发的抽象。这使得几个应用程序可以部署在Hadoop之上，例如**Storm**，**Tez**，**OpenMPI**，**Giraph**，当然还有**Apache
    Spark**，我们将在接下来的部分中看到。
- en: Hadoop MapReduce is a batch-oriented system, meaning that it relies on processing
    data in batches, and is not designed for real-time use cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce是一个面向批处理的系统，意味着它依赖于批量处理数据，并不适用于实时用例。
- en: Apache Spark
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is a cluster-computing framework from the University of California,
    Berkeley's AMPLab. Spark is not a substitute for the complete Hadoop ecosystem,
    but mostly for the MapReduce aspect of a Hadoop cluster. Whereas Hadoop MapReduce
    uses on-disk batch operations to process data, Spark uses both in-memory and on-disk
    operations. As expected, it is faster with datasets that fit in memory. This is
    why it is more useful for real-time streaming applications, but it can also be
    used with ease for datasets that don't fit in memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是加州大学伯克利分校AMPLab的集群计算框架。Spark并不是完整的Hadoop生态系统的替代品，而主要是Hadoop集群的MapReduce方面。而Hadoop
    MapReduce使用磁盘批处理操作来处理数据，Spark则同时使用内存和磁盘操作。预期地，对于适合内存的数据集，Spark更快。这就是为什么它对于实时流应用更有用，但也可以轻松处理不适合内存的数据集。
- en: 'Apache Spark can run on top of HDFS using YARN or in standalone mode, as shown
    in the following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark可以在HDFS上使用YARN或独立模式运行，如下图所示：
- en: '![](img/72713523-c529-41c1-850c-451605c31865.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72713523-c529-41c1-850c-451605c31865.png)'
- en: This means that in some cases (such as the one that we will use in our following
    use case) we can completely ditch Hadoop for Spark if our problem is really well
    defined and constrained within Spark's capabilities.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在某些情况下（例如我们将在下面的用例中使用的情况），如果我们的问题确实在Spark的能力范围内得到了很好的定义和限制，我们可以完全放弃Hadoop而选择Spark。
- en: Spark can be up to 100 times faster than Hadoop MapReduce for in-memory operations.
    Spark offers user-friendly APIs for Scala (its native language), Java, Python,
    and Spark SQL (a variation of the SQL92 specification). Both Spark and MapReduce
    are resilient to failure. Spark uses RDDs that are distributed across the whole
    cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存操作，Spark可能比Hadoop MapReduce快100倍。Spark为Scala（其本地语言），Java，Python和Spark SQL（SQL92规范的变体）提供了用户友好的API。Spark和MapReduce都具有容错性。Spark使用分布在整个集群中的RDD。
- en: As we can see from the overall Spark architecture, as follows, we can have several
    different modules of Spark working together for different needs, from SQL querying
    to streaming and machine learning libraries.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从总体上看，根据Spark的架构，我们可以有几个不同的Spark模块一起工作，满足不同的需求，从SQL查询到流处理和机器学习库。
- en: Comparing  Spark with Hadoop MapReduce
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Spark与Hadoop MapReduce进行比较
- en: 'The Hadoop MapReduce framework is more commonly compared to Apache Spark, a
    newer technology that aims to solve problems in a similar problem space. Some
    of their most important attributes are summarized in the following table:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce框架更常与Apache Spark进行比较，后者是一种旨在解决类似问题空间中问题的新技术。它们最重要的属性总结在下表中：
- en: '|  | **Hadoop MapReduce** | **Apache Spark** |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | **Hadoop MapReduce** | **Apache Spark** |'
- en: '| Written in | Java | Scala |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 编写语言 | Java | Scala |'
- en: '| Programming model | MapReduce | RDD |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 编程模型 | MapReduce | RDD |'
- en: '| Client bindings | Most high-level languages | Java, Scala, Python |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 客户端绑定 | 大多数高级语言 | Java，Scala，Python |'
- en: '| Ease of use | Moderate, with high-level abstractions (Pig, Hive, and so on)
    | Good |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 使用便捷性 | 中等，具有高级抽象（Pig，Hive等） | 良好 |'
- en: '| Performance | High throughput in batch | High throughput in streaming and
    batch mode |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 批处理高吞吐量 | 流处理和批处理模式高吞吐量 |'
- en: '| Uses | Disk (I/O bound) | Memory, degrading performance if disk is needed
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 使用 | 磁盘（I/O受限） | 内存，如果需要磁盘会降低性能 |'
- en: '| Typical node | Medium | Medium-large |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 典型节点 | 中等 | 中等大 |'
- en: As we can see from the preceding comparison, there are pros and cons for both
    technologies. Spark arguably has better performance, especially in problems that
    use fewer nodes. On the other hand, Hadoop is a mature framework with excellent
    tooling on top of it to cover almost every use case.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述比较可以看出，这两种技术都有优缺点。Spark在性能方面可能更好，特别是在使用较少节点的问题上。另一方面，Hadoop是一个成熟的框架，具有出色的工具，几乎可以覆盖每种用例。
- en: MongoDB as a data warehouse
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MongoDB作为数据仓库
- en: Apache Hadoop is often described as the 800 lb gorilla in the room of big data
    frameworks. Apache Spark, on the other hand, is more like a 200 lb cheetah for
    its speed, agility, and performance characteristics, which allow it to work well
    in a subset of the problems that Hadoop aims to solve.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop经常被描述为大数据框架中的800磅大猩猩。另一方面，Apache Spark更像是一只200磅的猎豹，因为它的速度、敏捷性和性能特点，使其能够很好地解决Hadoop旨在解决的一部分问题。
- en: MongoDB, on the other hand, can be described as the MySQL equivalent in the
    NoSQL world, because of its adoption and ease of use. MongoDB also offers an aggregation
    framework, MapReduce capabilities, and horizontal scaling using sharding, which
    is essentially data partitioning at the database level. So naturally, some people
    wonder why we don't use MongoDB as our data warehouse to simplify our architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MongoDB可以被描述为NoSQL世界中的MySQL等效物，因为它的采用和易用性。MongoDB还提供聚合框架、MapReduce功能和使用分片进行水平扩展，这实质上是在数据库级别进行数据分区。因此，一些人自然会想知道为什么我们不使用MongoDB作为我们的数据仓库来简化我们的架构。
- en: 'This is a pretty compelling argument, and it may or may not be the case that
    it makes sense to use MongoDB as a data warehouse. The advantages of such a decision
    are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当有说服力的论点，也许使用MongoDB作为数据仓库是有道理的，也可能不是。这样做的优势如下：
- en: Simpler architecture
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更简单的架构
- en: Less need for message queues, reducing latency in our system
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息队列的需求减少，减少了系统的延迟
- en: 'The disadvantages are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点如下：
- en: MongoDB's MapReduce framework is not a replacement for Hadoop's MapReduce. Even
    though they both follow the same philosophy, Hadoop can scale to accommodate larger
    workloads.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB的MapReduce框架不能替代Hadoop的MapReduce。尽管它们都遵循相同的理念，但Hadoop可以扩展以容纳更大的工作负载。
- en: Scaling MongoDB's document storage using sharding will hit a wall at some point.
    Whereas Yahoo! has reported using 42,000 servers in its largest Hadoop cluster,
    the largest MongoDB commercial deployments stand at 5 billion (Craigslist), compared
    to 600 nodes and petabytes of data for Baidu, the internet giant dominating, among
    others, the Chinese internet search market.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分片来扩展MongoDB的文档存储将在某个时候遇到瓶颈。尽管Yahoo!报告称其最大的Hadoop集群使用了42,000台服务器，但最大的MongoDB商业部署仅达到50亿（Craigslist），而百度的节点数和数据量达到了600个节点和PB级数据，这家互联网巨头主导着中国互联网搜索市场等领域。
- en: There is more than an order of magnitude of difference in terms of scaling.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展方面存在一个数量级的差异。
- en: MongoDB is mainly designed around being a real-time querying database based
    on stored data on disk, whereas MapReduce is designed around using batches, and
    Spark is designed around using streams of data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB主要设计为基于磁盘上存储数据的实时查询数据库，而MapReduce是围绕使用批处理设计的，Spark是围绕使用数据流设计的。
- en: A big data use case
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个大数据用例
- en: 'Putting all of this into action, we will develop a fully working system using
    a data source, a Kafka message broker, an Apache Spark cluster on top of HDFS
    feeding a Hive table, and a MongoDB database. Our Kafka message broker will ingest
    data from an API, streaming market data for an XMR/BTC currency pair. This data
    will be passed on to an Apache Spark algorithm on HDFS to calculate the price
    for the next ticker timestamp, based on the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些付诸实践，我们将开发一个完全工作的系统，使用数据源、Kafka消息代理、在HDFS上运行的Apache Spark集群，供应Hive表，以及MongoDB数据库。我们的Kafka消息代理将从API摄取数据，为XMR/BTC货币对流动市场数据。这些数据将传递给HDFS上的Apache
    Spark算法，以根据以下内容计算下一个ticker时间戳的价格：
- en: The corpus of historical prices already stored on HDFS
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经存储在HDFS上的历史价格语料库
- en: The streaming market data arriving from the API
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从API到达的流动市场数据
- en: This predicted price will then be stored in MongoDB using the MongoDB Connector
    for Hadoop. MongoDB will also receive data straight from the Kafka message broker,
    storing it in a special collection with the document expiration date set to one
    minute. This collection will hold the latest orders, with the goal of being used
    by our system to buy or sell, using the signal coming from the Spark ML system.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个预测的价格将使用MongoDB Connector for Hadoop存储在MongoDB中。MongoDB还将直接从Kafka消息代理接收数据，将其存储在一个特殊的集合中，文档过期日期设置为一分钟。这个集合将保存最新的订单，旨在被我们的系统用来购买或出售，使用来自Spark
    ML系统的信号。
- en: So, for example, if the price is currently 10 and we have a bid for 9.5, but
    we expect the price to go down at the next market tick, then the system would
    wait. If we expect the price to go up in the next market tick, then the system
    would increase the bid price to 10.01 to match the price in the next ticker.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果价格当前为10，我们出价为9.5，但我们预计下一个市场tick价格会下降，那么系统会等待。如果我们预计下一个市场tick价格会上涨，那么系统会将出价提高到10.01以匹配下一个ticker的价格。
- en: Similarly, if the price is 10 and we bid for 10.5, but expect the price to go
    down, we would adjust our bid to 9.99 to make sure we don't overpay for it. But,
    if the price is expected to go up, we would immediately buy to make a profit at
    the next market tick.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果价格为10，我们出价为10.5，但预计价格会下降，我们会调整我们的出价为9.99，以确保我们不会为此支付过多。但是，如果预计价格会上涨，我们会立即购买，以在下一个市场tick中获利。
- en: 'Schematically, our architecture looks like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表上，我们的架构如下：
- en: '![](img/b14f1dcd-50d9-44b7-9902-c3234bb6efa8.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b14f1dcd-50d9-44b7-9902-c3234bb6efa8.png)'
- en: The API is simulated by posting JSON messages to a Kafka topic named `xmr_btc`.
    On the other end, we have a Kafka consumer importing real-time data to MongoDB.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: API通过将JSON消息发布到名为`xmr_btc`的Kafka主题来模拟。另一方面，我们有一个Kafka消费者将实时数据导入MongoDB。
- en: We also have another Kafka consumer importing data to Hadoop to be picked up
    by our algorithms, which send recommendation data (signals) to a Hive table. Finally,
    we export data from the Hive table into MongoDB.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有另一个Kafka消费者将数据导入Hadoop，供我们的算法使用，发送推荐数据（信号）到Hive表。最后，我们将数据从Hive表导出到MongoDB。
- en: Setting up Kafka
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Kafka
- en: The first step in setting up the environment for our big data use case is to
    establish a Kafka node. Kafka is essentially a FIFO queue, so we will use the
    simplest single node (broker) setup. Kafka organizes data using topics, producers,
    consumers, and brokers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 建立大数据用例环境的第一步是建立一个Kafka节点。Kafka本质上是一个FIFO队列，因此我们将使用最简单的单节点（broker）设置。Kafka使用主题、生产者、消费者和代理来组织数据。
- en: 'The important Kafka terminologies are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的Kafka术语如下：
- en: A **broker** is essentially a node.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**本质上是一个节点。'
- en: A **producer** is a process that writes data to the message queue.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**本质上是一个写入数据到消息队列的过程。'
- en: A **consumer** is a process that reads data from the message queue.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**是从消息队列中读取数据的过程。'
- en: A **topic** is the specific queue that we write to and read data from.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**是我们写入和读取数据的特定队列。'
- en: A Kafka topic is further subdivided into a number of partitions. We can split
    data from a particular topic into multiple brokers (nodes), both when we write
    to the topic and also when we read our data at the other end of the queue.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka主题进一步分为多个分区。我们可以在写入主题时，以及在队列的另一端读取数据时，将特定主题的数据拆分为多个代理（节点）。
- en: 'After installing Kafka on our local machine, or any cloud provider of our choice
    (there are excellent tutorials for EC2 to be found just a search away), we can
    create a topic using this single command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的本地机器上安装Kafka，或者选择任何云提供商（有很好的EC2教程可以找到），我们可以使用以下单个命令创建一个主题：
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will create a new topic called `xmr-btc`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`xmr-btc`的新主题。
- en: 'Deleting the topic is similar to creating one, by using this command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 删除主题与创建主题类似，使用以下命令：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can then get a list of all topics by issuing the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过发出以下命令来获取所有主题的列表：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can then create a command-line producer for our topic, just to test that
    we can send messages to the queue, like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以为我们的主题创建一个命令行生产者，只是为了测试我们是否可以将消息发送到队列，就像这样：
- en: '[PRE3]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Data on every line will be sent as a string encoded message to our topic, and
    we can end the process by sending a `SIGINT` signal (typically *Ctrl* + *C*).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 每行的数据将作为字符串编码的消息发送到我们的主题，我们可以通过发送`SIGINT`信号（通常是*Ctrl* + *C*）来结束这个过程。
- en: 'Afterwards we can view the messages that are waiting in our queue by spinning
    up a consumer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以通过启动一个消费者来查看等待在我们队列中的消息：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This consumer will read all messages in our `xmr-btc` topic, starting from the
    beginning of history. This is useful for our test purposes, but we will change
    this configuration in real-world applications.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个消费者将从我们的`xmr-btc`主题中读取所有消息，从历史的开始。这对我们的测试目的很有用，但在实际应用中我们会更改这个配置。
- en: You will keep seeing `zookeeper`, in addition to `kafka`, mentioned in the commands.
    Apache Zookeeper comes together with Apache Kafka, and is a centralized service
    that is used internally by Kafka for maintaining configuration information, naming,
    providing distributed synchronization, and providing group services.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令中，除了提到`kafka`，您还会看到`zookeeper`。Apache Zookeeper与Apache Kafka一起使用，是一个集中式服务，由Kafka内部用于维护配置信息、命名、提供分布式同步和提供组服务。
- en: Now we have set up our broker, we can use the code at [https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9 ](https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9)to
    start reading (consuming) and writing (producing) messages to the queue. For our
    purposes, we are using the `ruby-kafka` gem, developed by Zendesk.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了我们的代理，我们可以使用[https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9](https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9)上的代码来开始读取消息并将消息写入队列。对于我们的目的，我们使用了由Zendesk开发的`ruby-kafka`
    gem。
- en: For simplicity, we are using a single class to read from a file stored on disk
    and to write to our Kafka queue.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们使用一个单一的类来从磁盘上存储的文件中读取数据，并将其写入我们的Kafka队列。
- en: 'Our `produce` method will be used to write messages to Kafka as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`produce`方法将用于将消息写入Kafka，如下所示：
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our `consume` method will read messages from Kafka as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`consume`方法将从Kafka中读取消息，如下所示：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that we are using the consumer group API feature (added in Kafka 0.9)
    to get multiple consumers to access a single topic by assigning each partition
    to a single consumer. In the event of a consumer failure, its partitions will
    be reallocated to the remaining members of the group.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了消费者组API功能（在Kafka 0.9中添加）来使多个消费者通过将每个分区分配给单个消费者来访问单个主题。在消费者故障的情况下，其分区将重新分配给组的其余成员。
- en: 'The next step is to write these messages to MongoDB, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些消息写入MongoDB，如下所示：
- en: 'First, we create our collection so that our documents expire after one minute.
    Enter the following in the `mongo` shell:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建我们的集合，以便我们的文档在一分钟后过期。在`mongo` shell中输入以下内容：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This way, we create a new database called `exchange_data` with a new collection
    called `xmr_btc` that has auto-expiration after one minute. For MongoDB to auto-expire
    documents, we need to provide a field with a `datetime` value to compare its value
    against the current server time. In our case, this is the `createdAt` field.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们创建了一个名为`exchange_data`的新数据库，其中包含一个名为`xmr_btc`的新集合，该集合在一分钟后自动过期。要使MongoDB自动过期文档，我们需要提供一个带有`datetime`值的字段，以将其值与当前服务器时间进行比较。在我们的情况下，这是`createdAt`字段。
- en: 'For our use case, we will use the low-level MongoDB Ruby driver. The code for
    `MongoExchangeClient` is as follows:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将使用低级别的MongoDB Ruby驱动程序。`MongoExchangeClient`的代码如下：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This client connects to our local database, sets the `createdAt` field for the
    TTL document expiration, and saves the message to our collection.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此客户端连接到我们的本地数据库，为TTL文档过期设置`createdAt`字段，并将消息保存到我们的集合中。
- en: With this setup, we can write messages to Kafka, read them at the other end
    of the queue, and write them into our MongoDB collection.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个设置，我们可以将消息写入Kafka，在队列的另一端读取它们，并将它们写入我们的MongoDB集合。
- en: Setting up Hadoop
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Hadoop
- en: We can install Hadoop and use a single node for the use case in this chapter
    using the instructions from Apache Hadoop's website at [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以安装Hadoop，并使用单个节点来完成本章的用例，使用Apache Hadoop网站上的说明[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)。
- en: After following these steps, we can browse the HDFS files in our local machine
    at `http://localhost:50070/explorer.html#/`. Assuming that our signals data is
    written in HDFS under the `/user/<username>/signals` directory, we will use the
    MongoDB Connector for Hadoop to export and import it into MongoDB.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤后，我们可以在本地机器上的`http://localhost:50070/explorer.html#/`上浏览HDFS文件。假设我们的信号数据写在HDFS的`/user/<username>/signals`目录下，我们将使用MongoDB
    Connector for Hadoop将其导出并导入到MongoDB中。
- en: MongoDB Connector for Hadoop is the officially supported library, allowing MongoDB
    data files or MongoDB backup files in BSON to be used as the source or destination
    for Hadoop MapReduce tasks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Connector for Hadoop是官方支持的库，允许将MongoDB数据文件或BSON格式的MongoDB备份文件用作Hadoop
    MapReduce任务的源或目的地。
- en: This means that we can also easily export to, and import data from, MongoDB
    when we are using higher-level Hadoop ecosystem tools such as Pig (a procedural
    high-level language), Hive (a SQL-like high-level language), and Spark (a cluster-computing
    framework).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当我们使用更高级别的Hadoop生态系统工具时，例如Pig（一种过程化高级语言）、Hive（一种类似SQL的高级语言）和Spark（一种集群计算框架）时，我们也可以轻松地导出和导入数据到MongoDB。
- en: Steps for Hadoop setup
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop设置步骤
- en: 'The different steps to set up Hadoop are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Hadoop的不同步骤如下：
- en: Download the JAR from the Maven repository at [http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/2.0.2/](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/2.0.2/).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[Maven库](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/2.0.2/)下载JAR。
- en: Download `mongo-java-driver` from [https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/](https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/](https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/)下载`mongo-java-driver`。
- en: 'Create a directory (in our case, named `mongo_lib`) and copy these two JARs
    in there with the following command:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个目录（在我们的情况下，命名为`mongo_lib`），并使用以下命令将这两个JAR复制到其中：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Alternatively, we can copy these JARs under the `share/hadoop/common/` directory.
    As these JARs will need to be available in every node, for clustered deployment,
    it's easier to use Hadoop's `DistributedCache` to distribute the JARs to all nodes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将这些JAR复制到`share/hadoop/common/`目录下。由于这些JAR需要在每个节点上都可用，对于集群部署，使用Hadoop的`DistributedCache`将JAR分发到所有节点更容易。
- en: The next step is to install Hive from [https://hive.apache.org/downloads.html](https://hive.apache.org/downloads.html).
    For this example, we used a MySQL server for Hive's metastore data. This can be
    a local MySQL server for development, but it is recommended that you use a remote
    server for production environments.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是从[https://hive.apache.org/downloads.html](https://hive.apache.org/downloads.html)安装Hive。在本例中，我们使用了MySQL服务器来存储Hive的元数据。这可以是用于开发的本地MySQL服务器，但建议在生产环境中使用远程服务器。
- en: 'Once we have Hive set up, we just run the following command:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们设置好了Hive，我们只需运行以下命令：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we add the three JARs (`mongo-hadoop-core`, `mongo-hadoop-driver`, and `mongo-hadoop-hive`)
    that we downloaded earlier:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们添加之前下载的三个JAR（`mongo-hadoop-core`、`mongo-hadoop-driver`和`mongo-hadoop-hive`）：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And then, assuming our data is in the table exchanges:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，假设我们的数据在表交换中：
- en: '| **customerid                                             ** | **int** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| **customerid                                             ** | **int** |'
- en: '| `pair` | `String` |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `pair` | `String` |'
- en: '| `time` | `TIMESTAMP` |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `time` | `TIMESTAMP` |'
- en: '| `recommendation` | `int` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `recommendation` | `int` |'
- en: We can also use Gradle or Maven to download the JARs in our local project. If
    we only need MapReduce, then we just download the `mongo-hadoop-core` JAR. For
    Pig, Hive, Streaming, and so on, we must download the appropriate JARs from
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Gradle或Maven在我们的本地项目中下载JAR。如果我们只需要MapReduce，那么我们只需下载`mongo-hadoop-core`
    JAR。对于Pig、Hive、Streaming等，我们必须从
- en: '[http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/)。'
- en: Some useful Hive commands include the following: `show databases;` and
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一些有用的Hive命令包括：`show databases;`和
- en: '`create table exchanges(customerid int, pair String, time TIMESTAMP, recommendation
    int);`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表交换（客户ID int，对 String，时间时间戳，建议 int）;
- en: 'Now that we are all set, we can create a MongoDB collection backed by our local
    Hive data:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了，我们可以创建一个由我们本地Hive数据支持的MongoDB集合：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can copy all data from the `exchanges` Hive table into MongoDB
    as follows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以按照以下方式将`exchanges` Hive表中的所有数据复制到MongoDB中：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This way, we have established a pipeline between Hadoop and MongoDB using Hive,
    without any external server.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经建立了Hadoop和MongoDB之间的管道，使用Hive，而不需要任何外部服务器。
- en: Using a Hadoop to MongoDB pipeline
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop到MongoDB的管道
- en: An alternative to using the MongoDB Connector for Hadoop is to use the programming
    language of our choice to export data from Hadoop, and then write into MongoDB
    using the low-level driver or an ODM, as described in previous chapters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MongoDB Connector for Hadoop的替代方法是使用我们选择的编程语言从Hadoop中导出数据，然后使用低级驱动程序或ODM将数据写入MongoDB，如前几章所述。
- en: 'For example, in Ruby, there are a few options:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Ruby中，有一些选项：
- en: '**WebHDFS** on GitHub, which uses the WebHDFS or the **HttpFS** Hadoop API
    to fetch data from HDFS'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GitHub上的**WebHDFS**，它使用WebHDFS或**HttpFS** Hadoop API从HDFS获取数据
- en: System calls, using the Hadoop command-line tool and Ruby's `system()` call
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统调用，使用Hadoop命令行工具和Ruby的`system()`调用
- en: 'Whereas in Python, we can use the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 而在Python中，我们可以使用以下命令：
- en: '**HdfsCLI**, which uses the WebHDFS or the HttpFS Hadoop API'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HdfsCLI**，它使用WebHDFS或HttpFS Hadoop API'
- en: '**libhdfs**, which uses a JNI-based native C wrapped around the HDFS Java client'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**libhdfs**，它使用基于JNI的本地C封装的HDFS Java客户端'
- en: All of these options require an intermediate server between our Hadoop infrastructure
    and our MongoDB server, but, on the other hand, allow for more flexibility in
    the ETL process of exporting/importing data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些选项都需要我们的Hadoop基础设施和MongoDB服务器之间的中间服务器，但另一方面，允许在导出/导入数据的ETL过程中更灵活。
- en: Setting up Spark to MongoDB
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Spark到MongoDB
- en: MongoDB also offers a tool to directly query Spark clusters and export data
    to MongoDB. Spark is a cluster computing framework that typically runs as a YARN
    module in Hadoop, but can also run independently on top of other filesystems.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB还提供了一个工具，可以直接查询Spark集群并将数据导出到MongoDB。Spark是一个集群计算框架，通常作为Hadoop中的YARN模块运行，但也可以独立在其他文件系统之上运行。
- en: MongoDB Spark Connector can read and write to MongoDB collections from Spark
    using Java, Scala, Python, and R. It can also use aggregation and run SQL queries
    on MongoDB data after creating a temporary view for the dataset backed by Spark.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Spark Connector可以使用Java、Scala、Python和R从Spark读取和写入MongoDB集合。它还可以在创建由Spark支持的数据集的临时视图后，对MongoDB数据进行聚合和运行SQL查询。
- en: Using Scala, we can also use Spark Streaming, the Spark framework for data-streaming
    applications built on top of Apache Spark.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scala，我们还可以使用Spark Streaming，这是构建在Apache Spark之上的数据流应用程序的Spark框架。
- en: Further reading
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following references for further information:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下参考资料获取更多信息：
- en: '[https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html)'
- en: '[http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)'
- en: '[https://spreadstreet.io/database/](https://spreadstreet.io/database/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spreadstreet.io/database/](https://spreadstreet.io/database/)'
- en: '[http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png](http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png](http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png)'
- en: '[http://mattturck.com/bigdata2017/](http://mattturck.com/bigdata2017/)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://mattturck.com/bigdata2017/](http://mattturck.com/bigdata2017/)'
- en: '[https://dzone.com/articles/hadoop-t-etl](https://dzone.com/articles/hadoop-t-etl)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://dzone.com/articles/hadoop-t-etl](https://dzone.com/articles/hadoop-t-etl)'
- en: '[https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html](https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html](https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html)'
- en: '[https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham](https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham](https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham)'
- en: '[https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html](https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html](https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html)'
- en: '[https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ](https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ](https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ)'
- en: '[https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG](https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG](https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG)'
- en: '[https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
- en: '[https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture](https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture](https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture)'
- en: '[https://wiki.apache.org/hadoop/PoweredByYarn](https://wiki.apache.org/hadoop/PoweredByYarn)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://wiki.apache.org/hadoop/PoweredByYarn](https://wiki.apache.org/hadoop/PoweredByYarn)'
- en: '[https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1](https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1](https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1)'
- en: '[https://www.mongodb.com/blog/post/mongodb-live-at-craigslist](https://www.mongodb.com/blog/post/mongodb-live-at-craigslist)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/blog/post/mongodb-live-at-craigslist](https://www.mongodb.com/blog/post/mongodb-live-at-craigslist)'
- en: '[https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)'
- en: '[http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html](http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html](http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html)'
- en: '[https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript](https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript](https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript)'
- en: '[https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics](https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics](https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics)'
- en: '[https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html](https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html](https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html)'
- en: '[https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark](https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark](https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark)'
- en: '[https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true](https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true](https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true)'
- en: '[https://www.infoq.com/articles/apache-kafka](https://www.infoq.com/articles/apache-kafka)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.infoq.com/articles/apache-kafka](https://www.infoq.com/articles/apache-kafka)'
- en: '[https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka](https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka](https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka)'
- en: '[https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f](https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f](https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f)'
- en: '[https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree](https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree](https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree)'
- en: '[https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it](https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it](https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it)'
- en: '[https://github.com/zendesk/ruby-kafka](https://github.com/zendesk/ruby-kafka#producing-messages-to-kafka)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/zendesk/ruby-kafka](https://github.com/zendesk/ruby-kafka#producing-messages-to-kafka)'
- en: '[http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html](http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html](http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html)'
- en: '[https://github.com/mtth/hdfs](https://github.com/mtth/hdfs)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/mtth/hdfs](https://github.com/mtth/hdfs)'
- en: '[http://wesmckinney.com/blog/outlook-for-2017/](http://wesmckinney.com/blog/outlook-for-2017/)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://wesmckinney.com/blog/outlook-for-2017/](http://wesmckinney.com/blog/outlook-for-2017/)'
- en: '[http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/)'
- en: '[https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/](https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/](https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/)'
- en: '[https://sookocheff.com/post/kafka/kafka-in-a-nutshell/](https://sookocheff.com/post/kafka/kafka-in-a-nutshell/)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://sookocheff.com/post/kafka/kafka-in-a-nutshell/](https://sookocheff.com/post/kafka/kafka-in-a-nutshell/)'
- en: '[https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto](https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto](https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto)'
- en: '[http://ondra-m.github.io/ruby-spark/](http://ondra-m.github.io/ruby-spark/)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://ondra-m.github.io/ruby-spark/](http://ondra-m.github.io/ruby-spark/)'
- en: '[https://amodernstory.com/2015/03/29/installing-hive-on-mac/](https://amodernstory.com/2015/03/29/installing-hive-on-mac/)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://amodernstory.com/2015/03/29/installing-hive-on-mac/](https://amodernstory.com/2015/03/29/installing-hive-on-mac/)'
- en: '[https://www.infoq.com/articles/apache-spark-introduction](https://www.infoq.com/articles/apache-spark-introduction)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.infoq.com/articles/apache-spark-introduction](https://www.infoq.com/articles/apache-spark-introduction)'
- en: '[https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf](https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf](https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf)'
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the big data landscape and how MongoDB compares
    with, and fares against, message-queuing systems and data warehousing technologies.
    Using a big data use case, we learned how to integrate MongoDB with Kafka and
    Hadoop from a practical perspective.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了大数据领域以及MongoDB与消息队列系统和数据仓库技术的比较和对比。通过一个大数据用例，我们从实际角度学习了如何将MongoDB与Kafka和Hadoop集成。
- en: In the next chapter, we will turn to replication and cluster operations, and
    discuss replica sets, the internals of elections, and the setup and administration
    of our MongoDB cluster.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向复制和集群操作，并讨论副本集、选举的内部情况以及我们的MongoDB集群的设置和管理。
