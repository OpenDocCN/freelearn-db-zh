# 第十三章：分片

分片是通过将数据集分区到不同服务器（分片）上来横向扩展我们的数据库的能力。这是 MongoDB 自 2010 年 8 月发布 1.6 版本以来的一个特性。Foursquare 和 Bitly 是 MongoDB 最著名的早期客户之一，从其推出一直到其正式发布都使用了分片功能。

在本章中，我们将学习以下主题：

+   如何设计分片集群以及如何做出关于其使用的最重要决定——选择分片键

+   不同的分片技术以及如何监视和管理分片集群

+   `mongos`路由器及其用于在不同分片之间路由我们的查询的方式

+   我们如何从分片中恢复错误

# 为什么要使用分片？

在数据库系统和计算系统中，我们有两种方法来提高性能。第一种方法是简单地用更强大的服务器替换我们的服务器，保持相同的网络拓扑和系统架构。这被称为**垂直扩展**。

垂直扩展的优点是从操作的角度来看很简单，特别是像亚马逊这样的云服务提供商只需点击几下就可以用**m2.extralarge**服务器实例替换**m2.medium**。另一个优点是我们不需要进行任何代码更改，因此几乎没有什么东西会出现灾难性的错误。

垂直扩展的主要缺点是存在限制；我们只能获得与云服务提供商提供给我们的服务器一样强大的服务器。

相关的缺点是获得更强大的服务器通常会带来成本的增加，这种增加不是线性的而是指数级的。因此，即使我们的云服务提供商提供更强大的实例，我们在部门信用卡的成本效益限制之前就会遇到成本效益的障碍。

提高性能的第二种方法是使用相同容量的相同服务器并增加它们的数量。这被称为**水平扩展**。

水平扩展的优点在于理论上能够呈指数级扩展，同时对于现实世界的应用来说仍然足够实用。主要缺点是在操作上可能更加复杂，需要进行代码更改并在系统设计上进行仔细设计。水平扩展在系统方面也更加复杂，因为它需要在不太可靠的网络链接上的不同服务器之间进行通信，而不是在单个服务器上进行进程间通信。以下图表显示了水平和垂直扩展之间的区别：

![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)

要理解扩展，重要的是要了解单服务器系统的限制。服务器通常受以下一个或多个特征的限制：

+   **CPU**：CPU 受限系统是受 CPU 速度限制的系统。例如，可以放入 RAM 的矩阵相乘任务将受到 CPU 限制，因为 CPU 必须执行特定数量的步骤，而不需要进行任何磁盘或内存访问即可完成任务。在这种情况下，CPU 使用率是我们需要跟踪的指标。

+   **I/O**：输入输出受限系统同样受到存储系统（HDD 或 SSD）速度的限制。例如，从磁盘读取大文件加载到内存中的任务将受到 I/O 限制，因为在 CPU 处理方面几乎没有什么要做的；大部分时间都花在从磁盘读取文件上。需要跟踪的重要指标是与磁盘访问相关的所有指标，每秒读取次数和每秒写入次数，与我们存储系统的实际限制相比。

+   **内存和缓存**：受内存限制和缓存限制的系统受到可用 RAM 内存和/或我们分配给它们的缓存大小的限制。一个将矩阵乘以大于我们 RAM 大小的任务将受到内存限制，因为它将需要从磁盘中分页数据来执行乘法。要跟踪的重要指标是已使用的内存。在 MongoDB MMAPv1 中，这可能会产生误导，因为存储引擎将通过文件系统缓存分配尽可能多的内存。

另一方面，在 WiredTiger 存储引擎中，如果我们没有为核心 MongoDB 进程分配足够的内存，内存不足的错误可能会导致其崩溃，这是我们要尽一切努力避免的。

监控内存使用量必须通过操作系统直接进行，并间接地通过跟踪分页数据来进行。增加的内存分页数通常表明我们的内存不足，操作系统正在使用虚拟地址空间来跟上。

作为数据库系统的 MongoDB 通常受到内存和 I/O 的限制。为我们的节点投资 SSD 和更多内存几乎总是一个不错的投资。大多数系统是前述限制的一个或多个组合。一旦我们增加了更多内存，我们的系统可能会变得 CPU 受限，因为复杂的操作几乎总是 CPU、I/O 和内存使用的组合。

MongoDB 的分片设置和操作非常简单，这也是它多年来取得巨大成功的原因，因为它提供了横向扩展的优势，而不需要大量的工程和运营资源。

也就是说，从一开始就正确地进行分片非常重要，因为一旦设置好了，从操作的角度来看，要更改配置是非常困难的。分片不应该是一个事后想法，而应该是设计过程早期的一个关键架构设计决策。

# 架构概述

一个分片集群由以下元素组成：

+   两个或更多分片。每个分片必须是一个副本集。

+   一个或多个查询路由器（`mongos`）。`mongos`提供了我们的应用程序和数据库之间的接口。

+   一个副本集的配置服务器。配置服务器存储整个集群的元数据和配置设置。

这些元素之间的关系如下图所示：

![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)

从 MongoDB 3.6 开始，分片必须实现为副本集。

# 开发、持续部署和暂存环境

在预生产环境中，使用完整服务器集可能是过度的。出于效率原因，我们可能选择使用更简化的架构。

我们可以为分片部署的最简单配置如下：

+   一个`mongos`路由器

+   一个分片副本集，有一个 MongoDB 服务器和两个仲裁者

+   一个配置服务器的副本集，有一个 MongoDB 服务器和两个仲裁者

这应严格用于开发和测试，因为这种架构违背了副本集提供的大多数优势，如高可用性、可扩展性和数据复制。

强烈建议在暂存环境中镜像我们的生产环境，包括服务器、配置和（如果可能）数据集要求，以避免在部署时出现意外。

# 提前计划分片

正如我们将在接下来的部分中看到的，分片在操作上是复杂且昂贵的。重要的是要提前计划，并确保我们在达到系统极限之前很久就开始分片过程。

一些关于何时需要开始分片的大致指导原则如下：

+   当平均 CPU 利用率低于 70%时

+   当 I/O（尤其是写入）容量低于 80%时

+   当平均内存利用率低于 70%时

由于分片有助于写入性能，重要的是要关注我们的 I/O 写入容量和应用程序的要求。

不要等到最后一刻才开始在已经忙碌到极致的 MongoDB 系统中进行分片，因为这可能会产生意想不到的后果。

# 分片设置

分片是在集合级别执行的。我们可以有一些我们不想或不需要分片的集合，有几个原因。我们可以将这些集合保持为未分片状态。

这些集合将存储在主分片中。在 MongoDB 中，每个数据库的主分片都不同。在分片环境中创建新数据库时，MongoDB 会自动选择主分片。MongoDB 将选择在创建时存储数据最少的分片。

如果我们想在任何其他时间更改主分片，我们可以发出以下命令：

```sql
> db.runCommand( { movePrimary : "mongo_books", to : "UK_based" } )
```

有了这个，我们将名为`mongo_books`的数据库移动到名为`UK_based`的分片中。

# 选择分片键

选择我们的分片键是我们需要做出的最重要的决定：一旦我们分片我们的数据并部署我们的集群，更改分片键就变得非常困难。首先，我们将经历更改分片键的过程。

# 更改分片键

在 MongoDB 中，没有命令或简单的程序可以更改分片键。更改分片键的唯一方法涉及备份和恢复所有数据，这在高负载生产环境中可能从极其困难到不可能。

以下是我们需要经历的步骤，以更改分片键：

1.  从 MongoDB 导出所有数据

1.  删除原始的分片集合

1.  使用新键配置分片

1.  预先拆分新的分片键范围

1.  将我们的数据恢复到 MongoDB 中

在这些步骤中，步骤 4 是需要进一步解释的步骤。

MongoDB 使用块来分割分片集合中的数据。如果我们从头开始引导 MongoDB 分片集群，MongoDB 将自动计算块。然后，MongoDB 将这些块分布到不同的分片上，以确保每个分片中有相等数量的块。

唯一不能真正做到这一点的时候是当我们想要将数据加载到新的分片集合中。

这样做的原因有三个：

+   MongoDB 仅在`insert`操作后创建拆分。

+   块迁移将从一个分片复制该块中的所有数据到另一个分片。

+   `floor(n/2)`块迁移可以在任何时间发生，其中`n`是我们拥有的分片数量。即使有三个分片，这也只是一次`floor(1.5)=1`块迁移。

这三个限制意味着让 MongoDB 自行解决这个问题肯定会花费更长时间，并且可能最终导致失败。这就是为什么我们希望预先拆分我们的数据，并为 MongoDB 提供一些关于我们的块应该放在哪里的指导。

在我们的示例中，`mongo_books`数据库和`books`集合如下：

```sql
> db.runCommand( { split : "mongo_books.books", middle : { id : 50 } } )
```

`middle`命令参数将在我们的键空间中拆分文档，这些文档的`id`小于或等于`50`，以及`id`大于`50`的文档。我们的集合中没有必要存在`id`等于`50`的文档，因为这只会作为我们分区的指导值。

在这个例子中，我们选择了`50`，因为我们假设我们的键在值范围从`0`到`100`中遵循均匀分布（即，每个值的键数量相同）。

我们应该努力创建至少 20-30 个块，以赋予 MongoDB 在潜在迁移中的灵活性。如果我们想手动定义分区键，我们也可以使用`bounds`和`find`而不是`middle`，但是这两个参数在应用它们之前需要数据存在于我们的集合中。

# 选择正确的分片键

在前面的部分之后，现在很明显我们需要考虑我们的分片键的选择，因为这是一个我们必须坚持的决定。

一个很好的分片键具有三个特点：

+   高基数

+   低频率

+   值的非单调变化

我们将首先介绍这三个属性的定义，以了解它们的含义：

+   **高基数**：这意味着分片键必须具有尽可能多的不同值。布尔值只能取`true`/`false`，因此不是一个好的分片键选择。一个可以取从*−(2⁶³)*到*2⁶³−1*的任何值的 64 位长值字段在基数方面是一个好的选择。

+   **低频率**：它直接关系到高基数的论点。低频率的分片键将具有接近完全随机/均匀分布的值分布。以我们 64 位长值的例子，如果我们一直观察到零和一这样的值，那么它对我们几乎没有用处。事实上，这和使用布尔字段一样糟糕，因为布尔字段也只能取两个值。如果我们有一个高频率值的分片键，我们最终会得到不可分割的块。这些块无法进一步分割，并且会增长，负面影响包含它们的分片的性能。

+   **非单调变化的值**：这意味着我们的分片键不应该是一个每次新插入都增加的整数，例如。如果我们选择一个单调递增的值作为我们的分片键，这将导致所有写入最终都进入我们所有分片中的最后一个，从而限制我们的写入性能。

如果我们想要使用单调变化的值作为分片键，我们应该考虑使用基于哈希的分片。

在下一节中，我们将描述不同的分片策略，包括它们的优点和缺点。

# 基于范围的分片

默认和最广泛使用的分片策略是基于范围的分片。这种策略将把我们集合的数据分成块，将具有相邻值的文档分组到同一个分片中。

对于我们的示例数据库和集合，分别是`mongo_books`和`books`，我们有以下内容：

```sql
> sh.shardCollection("mongo_books.books", { id: 1 } )
```

这将在`id`上创建一个基于范围的分片键，并且是升序的。我们的分片键的方向将决定哪些文档将最终出现在第一个分片中，哪些文档出现在随后的分片中。

如果我们计划进行基于范围的查询，这是一个很好的策略，因为这些查询将被定向到保存结果集的分片，而不必查询所有分片。

# 基于哈希的分片

如果我们没有一个达到前面提到的三个目标的分片键（或者无法创建一个），我们可以使用替代策略，即使用基于哈希的分片。在这种情况下，我们正在用数据分布来交换查询隔离。

基于哈希的分片将获取我们的分片键的值，并以一种接近均匀分布的方式进行哈希。这样，我们可以确保我们的数据将均匀分布在分片中。缺点是只有精确匹配查询将被路由到持有该值的确切分片。任何范围查询都必须从所有分片中获取数据。

对于我们的示例数据库和集合（分别是`mongo_books`和`books`），我们有以下内容：

```sql
> sh.shardCollection("mongo_books.books", { id: "hashed" } )
```

与前面的示例类似，我们现在将`id`字段作为我们的哈希分片键。

假设我们使用浮点值字段进行基于哈希的分片。如果我们的浮点数的精度超过*2⁵³*，那么我们将会遇到碰撞。在可能的情况下，应该避免使用这些字段。

# 提出我们自己的键

基于范围的分片不需要局限于单个键。事实上，在大多数情况下，我们希望结合多个键来实现高基数和低频率。

一个常见的模式是将低基数的第一部分（但仍具有两倍于我们拥有的分片数量的不同值的数量）与高基数键作为其第二字段组合。这既从分片键的第一部分实现了读取和写入分布，又从第二部分实现了基数和读取局部性。

另一方面，如果我们没有范围查询，那么我们可以在主键上使用基于哈希的分片，因为这将精确地定位我们要查找的分片和文档。

使事情变得更加复杂的是，这些考虑因我们的工作负载而改变。几乎完全由读取（比如 99.5%）组成的工作负载不会关心写入分布。我们可以使用内置的`_id`字段作为我们的分片键，这只会给最后一个分片增加 0.5%的负载。我们的读取仍然会分布在各个分片上。不幸的是，在大多数情况下，情况并不简单。

# 基于位置的数据

由于政府法规和希望将数据尽可能靠近用户，通常存在对特定数据中心中的数据进行限制和需要的约束。通过将不同的分片放置在不同的数据中心，我们可以满足这一要求。

每个分片本质上都是一个副本集。我们可以像连接副本集一样连接到它进行管理和维护操作。我们可以直接查询一个分片的数据，但结果只会是完整分片结果集的子集。

# 分片管理和监控

与单服务器或副本集部署相比，分片的 MongoDB 环境具有一些独特的挑战和限制。在本节中，我们将探讨 MongoDB 如何使用 chunks 平衡我们的数据跨分片，并在需要时如何调整它们。我们将一起探讨一些分片设计的限制。

# 平衡数据-如何跟踪和保持我们的数据平衡

在 MongoDB 中分片的一个优点是，它对应用程序基本上是透明的，并且需要最少的管理和运营工作。

MongoDB 需要不断执行的核心任务之一是在分片之间平衡数据。无论我们实现基于范围还是基于哈希的分片，MongoDB 都需要计算哈希字段的边界，以便确定将每个新文档插入或更新到哪个分片。随着数据的增长，这些边界可能需要重新调整，以避免出现大部分数据都集中在一个热分片上的情况。

为了举例说明，假设有一种名为`extra_tiny_int`的数据类型，其整数值范围为`-12`, `12`)。如果我们在这个`extra_tiny_int`字段上启用分片，那么我们数据的初始边界将是由`$minKey: -12`和`$maxKey: 11`表示的整个值范围。 

在我们插入一些初始数据后，MongoDB 将生成 chunks 并重新计算每个 chunk 的边界，以尝试平衡我们的数据。

默认情况下，MongoDB 创建的初始 chunk 数量是*2 × 分片数量*。

在我们的情况下，有两个分片和四个初始 chunk，初始边界将如下计算：

*Chunk1: [-12..-6)*

*Chunk2:  [-6..0)*

*Chunk3:  [0..6)*

*Chunk4:  [6,12)*其中*'['*是包含的，*')'*是不包含的

以下图表说明了前面的解释：

![在我们插入一些数据后，我们的 chunks 将如下所示：+   *ShardA*：+   *Chunk1*:* -12,-8,-7*+   *Chunk2*:*  -6*+   *ShardB*:+   *Chunk3*:* 0, 2      *+   *Chunk4*: *7,8,9,10,11,11,11,11*以下图表说明了前面的解释：![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)

在这种情况下，我们观察到 c*hunk4*的项比任何其他 chunk 都多。MongoDB 将首先将*chunk4*分成两个新的 chunk，试图保持每个 chunk 的大小在一定的阈值以下（默认为 64 MB）。

现在，我们有*chunk4A*：*7,8,9,10*和*chunk4B*：*11,11,11,11*，而不是 c*hunk4*。

以下图表说明了先前的解释：

![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)

其新边界如下：

+   *chunk4A*: *6,11)*

+   *chunk4B*: *[11,12)*

请注意，*chunk4B*只能容纳一个值。这现在是一个不可分割的分片，无法再分割成更小的分片，并且将无限增长，可能会导致性能问题。

这解释了为什么我们需要使用高基数字段作为我们的分片键，以及为什么像布尔值这样只有`true`/`false`值的字段是分片键的不良选择。

在我们的情况下，*ShardA*现在有两个分片，*ShardB*有三个分片。让我们看看下表：

| **分片数量** | **迁移阈值** |
| --- | --- |
| *≤19* | *2* |
| *20-79* | *4* |
| *≥80* | *8* |

我们还没有达到迁移阈值，因为*3-2 = 1*。

迁移阈值是根据拥有最多分片的分片和拥有最少分片的分片数量计算得出的，如下所示：

+   *Shard1 -> 85 chunks*

+   *Shard2 -> 86 chunks*

+   *Shard3 -> 92 chunks*

在上面的例子中，直到*Shard3*（或*Shard2*）达到*93*个分片之前，平衡都不会发生，因为迁移阈值对于*≥80*个分片是*8*，而*Shard1*和*Shard3*之间的差距仍然是*7*个分片（*92-85*）。

如果我们继续在*chunk4A*中添加数据，它最终将被分割成*chunk4A1*和*chunk4A2*。

现在*ShardB*有四个分片（*chunk3*，*chunk4A1*，*chunk4A2*和*chunk4B*），*ShardA*有两个分片（*chunk1*和*chunk2*）。

以下图表说明了分片与分片之间的关系：

![MongoDB 平衡器现在将从*ShardB*迁移一个分片到*ShardA*，因为*4-2 = 2*，达到了少于*20*个分片的迁移阈值。平衡器将调整两个分片之间的边界，以便能够更有效地查询（有针对性的查询）。以下图表说明了先前的解释：![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)

从上图表中可以看出，MongoDB 将尝试将*>64* MB 的分片一分为二。如果我们的数据分布不均匀，那么两个结果分片之间的边界可能会完全不均匀。MongoDB 可以将分片分割成更小的分片，但不能自动合并它们。我们需要手动合并分片，这是一个复杂且操作成本高昂的过程。

# 分片管理

大多数情况下，我们应该让 MongoDB 来管理分片。我们应该在开始时手动管理分片，在接收到初始数据负载时，当我们将配置从副本集更改为分片时。

# 移动分片

要手动移动一个分片，我们需要在连接到`mongos`和`admin`数据库后发出以下命令：

```sql
> db.runCommand( { moveChunk : 'mongo_books.books' ,
 find : {id: 50},
 to : 'shard1.packtdb.com' } )
```

使用上述命令，我们将包含`id: 50`的文档（这必须是分片键）从`mongo_books`数据库的`books`集合移动到名为`shard1.packtdb.com`的新分片。

我们还可以更明确地定义我们要移动的分片的边界。现在的语法如下：

```sql
> db.runCommand( { moveChunk : 'mongo_books.books' ,
 bounds :[ { id : <minValue> } ,
 { id : <maxValue> } ],
 to : 'shard1.packtdb.com' } )
```

在这里，`minValue`和`maxValue`是我们从`db.printShardingStatus()`中获取的值。

在先前的示例中，对于*chunk2*，`minValue`将是`-6`，`maxValue`将是`0`。

在基于哈希的分片中不要使用`find`。使用`bounds`代替。

# 更改默认的分片大小

要更改默认的分片大小，我们需要连接到`mongos`路由器，因此连接到`config`数据库。

然后我们发出以下命令将我们的全局`chunksize`更改为`16` MB：

```sql
> db.settings.save( { _id:"chunksize", value: 16 } )
```

更改`chunksize`的主要原因来自于默认的 64 MB `chunksize`可能会导致比我们的硬件处理能力更多的 I/O。在这种情况下，定义较小的`chunksize`将导致更频繁但数据密度较小的迁移。

更改默认块大小有以下缺点：

+   通过定义较小的块大小创建更多的拆分无法自动撤消。

+   增加块大小不会强制进行任何块迁移；相反，块将通过插入和更新而增长，直到达到新的大小。

+   降低块大小可能需要相当长的时间才能完成。

+   如果较低的块大小需要遵守新的块大小，那么只有在插入或更新时才会自动拆分。我们可能有一些块不会进行任何写操作，因此大小不会改变。

块大小可以为 1 到 1024 MB。

# 巨型块

在罕见情况下，我们可能会遇到巨型块，即大于块大小且无法由 MongoDB 拆分的块。如果我们的块中的文档数量超过最大文档限制，也可能遇到相同的情况。

这些块将启用`jumbo`标志。理想情况下，MongoDB 将跟踪它是否可以拆分块，并且一旦可以，它将被拆分；但是，我们可能决定在 MongoDB 之前手动触发拆分。

这样做的方法如下：

1.  通过 shell 连接到您的`mongos`路由器并运行以下命令：

```sql
> sh.status(true)
```

1.  使用以下代码标识具有`jumbo`的块：

```sql
databases:
…
mongo_books.books
...
chunks:
…
 shardB  2
 shardA  2
 { "id" : 7 } -->> { "id" : 9 } on : shardA Timestamp(2, 2) jumbo
```

1.  调用`splitAt()`或`splitFind()`手动在`mongo_books`数据库的`books`集合上拆分`id`等于`8`的块，使用以下代码：

```sql
> sh.splitAt( "mongo_books.books", { id: 8 })
```

`splitAt()`函数将根据我们定义的拆分点进行拆分。两个新的拆分可能平衡也可能不平衡。

或者，如果我们想让 MongoDB 找到拆分块的位置，我们可以使用`splitFind`，如下所示：

```sql
> sh.splitFind("mongo_books.books", {id: 7})
```

`splitFind`短语将尝试找到`id:7`查询所属的块，并自动定义拆分块的新边界，使它们大致平衡。

在这两种情况下，MongoDB 将尝试拆分块，如果成功，它将从中删除`jumbo`标志。

1.  如果前面的操作不成功，那么只有在这种情况下，我们应该首先尝试停止平衡器，同时验证输出并等待任何待处理的迁移完成，如下所示：

```sql
> sh.stopBalancer()
> sh.getBalancerState() > use config
while( sh.isBalancerRunning() ) {
 print("waiting...");
 sleep(1000);
} 
```

这应该返回`false`

1.  等待任何`waiting…`消息停止打印，然后以与之前相同的方式找到带有`jumbo`标志的块。

1.  然后在`mongos`路由器的`config`数据库中更新`chunks`集合，如下所示：

```sql
> db.getSiblingDB("config").chunks.update(
 { ns: "mongo_books.books", min: { id: 7 }, jumbo: true },
 { $unset: { jumbo: "" } }
)
```

前面的命令是一个常规的`update()`命令，第一个参数是`find()`部分，用于查找要更新的文档，第二个参数是要应用于它的操作（`$unset: jumbo flag`）。

1.  完成所有这些操作后，我们重新启用平衡器，如下所示：

```sql
> sh.setBalancerState(true)
```

1.  然后，我们连接到`admin`数据库，将新配置刷新到所有节点，如下所示：

```sql
> db.adminCommand({ flushRouterConfig: 1 } )
```

在手动修改任何状态之前，始终备份`config`数据库。

# 合并块

正如我们之前所看到的，通常情况下，MongoDB 将调整每个分片的块边界，以确保我们的数据均匀分布。在某些情况下，这可能不起作用，特别是当我们手动定义块时，如果我们的数据分布出奇地不平衡，或者我们的分片中有许多`delete`操作。

拥有空块将引发不必要的块迁移，并使 MongoDB 对需要迁移的块产生错误印象。正如我们之前解释的那样，块迁移的阈值取决于每个分片持有的块的数量。拥有空块可能会触发平衡器，也可能不会在需要时触发平衡器。

只有当至少有一个块为空时，块合并才会发生，并且只会发生在相邻块之间。

要找到空块，我们需要连接到要检查的数据库（在我们的情况下是`mongo_books`），并使用`runCommand`，设置`dataSize`如下：

```sql
> use mongo_books
> db.runCommand({
 "dataSize": "mongo_books.books",
 "keyPattern": { id: 1 },
 "min": { "id": -6 },
 "max": { "id": 0 }
})
```

`dataSize`短语遵循`database_name.collection_name`模式，而`keyPattern`是我们为这个集合定义的分片键。

`min`和`max`值应该由我们在这个集合中拥有的数据块计算得出。在我们的情况下，我们已经在本章前面的示例中输入了*chunkB*的详细信息。

如果我们的查询边界（在我们的情况下是*chunkB*的边界）没有返回任何文档，结果将类似于以下内容：

```sql
{ "size" : 0, "numObjects" : 0, "millis" : 0, "ok" : 1 }
```

现在我们知道*chunkB*没有数据，我们可以像这样将它与另一个数据块（在我们的情况下，只能是*chunkA*）合并：

```sql
> db.runCommand( { mergeChunks: "mongo_books.books",
 bounds: [ { "id": -12 },
 { id: 0 } ]
 } )
```

成功后，这将返回 MongoDB 的默认`ok`状态消息，如下所示：

```sql
{ "ok" : 1 }
```

然后，我们可以通过再次调用`sh.status()`来验证*ShardA*上只有一个数据块。

# 添加和移除分片

向我们的集群添加一个新的分片就像连接到`mongos`，连接到`admin`数据库，并使用以下命令调用`runCommand`一样简单：

```sql
> db.runCommand( {
addShard: "mongo_books_replica_set/rs01.packtdb.com:27017", maxSize: 18000, name: "packt_mongo_shard_UK"
} )
```

这将从`rs01.packtdb.com`主机的端口`27017`上运行的`mongo_books_replica_set`复制集中添加一个新的分片。我们还将为这个分片定义数据的`maxSize`为`18000` MB（或者我们可以将其设置为`0`以不设限），新分片的名称为`packt_mongo_shard_UK`。

这个操作将需要相当长的时间来完成，因为数据块将需要重新平衡和迁移到新的分片。

另一方面，移除一个分片需要更多的参与，因为我们必须确保在这个过程中不会丢失任何数据。我们按照以下步骤进行：

1.  首先，我们需要确保负载均衡器已启用，使用`sh.getBalancerState()`。然后，在使用`sh.status()`、`db.printShardingStatus()`或`listShards admin`命令中任何一个来识别我们想要移除的分片后，我们连接到`admin`数据库并按以下方式调用`removeShard`：

```sql
> use admin
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )
```

输出应该包含以下内容：

```sql
...
 "msg" : "draining started successfully",
 "state" : "started",
...
```

1.  然后，如果我们再次调用相同的命令，我们会得到以下结果：

```sql
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )
…
"msg" : "draining ongoing",
 "state" : "ongoing",
 "remaining" : {
 "chunks" : NumberLong(2),
 "dbs" : NumberLong(3)
 },
…
```

结果中剩下的文档包含仍在传输的`chunks`和`dbs`的数量。在我们的情况下，分别是`2`和`3`。

所有命令都需要在`admin`数据库中执行。

移除分片时可能会出现额外的复杂情况，如果我们要移除的分片作为它包含的一个或多个数据库的主分片。主分片是在我们启用分片时由 MongoDB 分配的，因此当我们移除分片时，我们需要手动将这些数据库移动到一个新的分片。

1.  我们可以通过查看`removeShard()`结果中的以下部分来确定是否需要执行此操作：

```sql
...
"note" : "you need to drop or movePrimary these databases",
 "dbsToMove" : [
 "mongo_books"
 ],
...
```

我们需要删除或`movePrimary`我们的`mongo_books`数据库。首先要确保我们连接到`admin`数据库。

在运行此命令之前，我们需要等待所有数据块完成迁移。

1.  在继续之前，请确保结果包含以下内容：

```sql
 ..."remaining" : {
 "chunks" : NumberLong(0) }...
```

1.  只有在我们确保要移动的数据块已经减少到零后，我们才能安全地运行以下命令：

```sql
> db.runCommand( { movePrimary: "mongo_books", to: "packt_mongo_shard_EU" })
```

1.  这个命令将调用一个阻塞操作，当它返回时，应该有以下结果：

```sql
{ "primary" : "packt_mongo_shard_EU", "ok" : 1 }
```

1.  在我们完成所有操作后再次调用相同的`removeShard()`命令应该返回以下结果：

```sql
> db.runCommand( { removeShard: "packt_mongo_shard_UK" } )

... "msg" : "removeshard completed successfully",
 "state" : "completed",
 "shard" : "packt_mongo_shard_UK"
 "ok" : 1
...
```

1.  一旦`state`为`completed`，`ok`为`1`，就可以安全地移除我们的`packt_mongo_shard_UK`分片。

移除分片自然比添加分片更复杂。在对我们的实时集群执行潜在破坏性操作时，我们需要留出一些时间，希望一切顺利，并为最坏的情况做好准备。

# 分片限制

分片提供了很大的灵活性。不幸的是，在执行一些操作的方式上存在一些限制。

我们将在以下列表中突出显示最重要的部分：

+   `group()`数据库命令不起作用。无论如何都不应该使用`group()`命令；而是使用`aggregate()`和聚合框架，或者`mapreduce()`。

+   `db.eval()`命令不起作用，出于安全原因，在大多数情况下应禁用。

+   更新的`$isolated`选项不起作用。这是分片环境中缺少的功能。`update()`的`$isolated`选项提供了保证，即如果我们一次更新多个文档，其他读者和写入者将不会看到一些文档被更新为新值，而其他文档仍将保留旧值。在非分片环境中实现这一点的方式是通过保持全局写锁和/或将操作序列化到单个线程，以确保`update()`受影响的文档的每个请求不会被其他线程/操作访问。这种实现意味着它不具备性能，并且不支持任何并发，这使得在分片环境中允许`$isolated`运算符成本过高。

+   不支持查询的`$snapshot`运算符。`find()`游标中的`$snapshot`运算符防止文档在更新后由于移动到磁盘上的不同位置而出现多次在结果中。`$snapshot`运算符在操作上是昂贵的，通常不是硬性要求。替代它的方法是在查询中使用一个字段的索引，这个字段的键在查询期间不会改变。

+   如果我们的查询不包含分片键，索引将无法覆盖我们的查询。在分片环境中，结果将来自磁盘，而不仅仅来自索引。唯一的例外是如果我们仅在内置的`_id`字段上进行查询，并且仅返回`_id`字段，那么 MongoDB 仍然可以使用内置索引来覆盖查询。

+   `update()`和`remove()`操作的工作方式不同。在分片环境中，所有`update()`和`remove()`操作必须包括要受影响的文档的`_id`或分片键；否则，`mongos`路由器将不得不在所有集合、数据库和分片上进行全表扫描，这在操作上将非常昂贵。

+   跨分片的唯一索引需要包含分片键作为索引的前缀。换句话说，为了实现跨分片的文档唯一性，我们需要遵循 MongoDB 为分片所遵循的数据分布。

+   分片键的大小必须达到 512 字节。分片键索引必须按照分片的关键字段以及可选的其他字段的升序排列，或者对其进行哈希索引。

文档中的分片键值也是不可变的。如果我们的`User`集合的分片键是`email`，那么在设置后我们就不能更新任何用户的`email`值。

# 查询分片数据

使用 MongoDB 分片查询我们的数据与单服务器部署或副本集不同。我们不是连接到单个服务器或副本集的主服务器，而是连接到决定要请求我们的数据的分片的`mongos`路由器。在本节中，我们将探讨查询路由器的操作方式，并使用 Ruby 来说明对开发人员来说这与副本集有多相似。

# 查询路由器

查询路由器，也称为`mongos`进程，充当我们 MongoDB 集群的接口和入口。应用程序连接到它，而不是连接到底层的分片和副本集；`mongos`执行查询，收集结果并将其传递给我们的应用程序。

`mongos`进程不持有任何持久状态，通常对系统资源要求较低。

`mongos`进程通常托管在与应用程序服务器相同的实例中。

它充当请求的代理。当查询进来时，`mongos`将检查并决定哪些分片需要执行查询，并在每个分片中建立一个游标。

# 查找

如果我们的查询包括分片键或分片键的前缀，`mongos`将执行有针对性的操作，只查询持有我们寻找的键的分片。

例如，在我们的`User`集合上使用`{ _id，email，address }`的复合分片键，我们可以使用以下任何查询进行有针对性的操作：

```sql
> db.User.find({_id: 1})
> db.User.find({_id: 1, email: 'alex@packt.com'})
> db.User.find({_id: 1, email: 'janluc@packt.com', address: 'Linwood Dunn'})
```

这些查询由前缀（与前两个情况相同）或完整的分片键组成。

另一方面，对`{email，address}`或`{address}`的查询将无法定位正确的分片，导致广播操作。广播操作是不包括分片键或分片键前缀的任何操作，并且它们导致`mongos`查询每个分片并从中收集结果。它们也被称为**分散和聚集操作**或**扇出查询**。

这种行为是索引组织方式的直接结果，并且类似于我们在索引章节中确定的行为。

# 排序/限制/跳过

如果我们想对结果进行排序，我们有以下两个选项：

+   如果我们在排序标准中使用分片键，那么`mongos`可以确定查询分片的顺序。这将导致高效且再次是有针对性的操作。

+   如果我们在排序标准中不使用分片键，那么与没有任何排序标准的查询一样，它将成为一个扇出查询。在不使用分片键时对结果进行排序时，主分片在将排序结果集传递给`mongos`之前在本地执行分布式合并排序。

对每个单独的分片强制执行查询的限制，然后再次在`mongos`级别进行，因为可能来自多个分片的结果。另一方面，`skip`操作符无法传递给各个分片，并且在本地检索所有结果后由`mongos`应用。

如果我们结合`skip`和`limit`操作符，`mongos`将通过将两个值传递给各个分片来优化查询。这在分页等情况下特别有用。如果我们在没有`sort`的情况下查询，而结果来自多个分片，`mongos`将在分片之间进行轮询以获取结果。

# 更新/删除

在文档修改操作中，例如`update`和`remove`，我们与`find`中看到的情况类似。如果我们在修改器的`find`部分中有分片键，那么`mongos`可以将查询定向到相关的分片。

如果我们在`find`部分没有分片键，那么它将再次成为一个扇出操作。

`UpdateOne`，`replaceOne`和`removeOne`操作必须具有分片键或`_id`值。

以下表总结了我们可以在分片中使用的操作：

| **操作类型** | **查询拓扑** |
| --- | --- |
| 插入 | 必须具有分片键 |
| 更新 | 可以具有分片键 |
| 具有分片键的查询 | 目标操作 |
| 没有分片键的查询 | 分散和聚集操作/扇出查询 |
| 具有分片键的索引、排序查询 | 目标操作 |
| 具有分片键的索引、排序查询而没有分片键 | 分布式排序合并 |

# 使用 Ruby 进行查询

使用 Ruby 连接到分片集群与连接到副本集没有区别。使用官方的 Ruby 驱动程序，我们必须配置`client`对象以定义一组`mongos`服务器，如下面的代码所示：

```sql
client = Mongo::Client.new('mongodb://key:password@mongos-server1-host:mongos-server1-port,mongos-server2-host:mongos-server2-port/admin?ssl=true&authSource=admin')
```

然后`mongo-ruby-driver`将返回一个`client`对象，这与从 Mongo Ruby 客户端连接到副本集没有区别。然后我们可以像在之前的章节中一样使用`client`对象，包括关于分片行为与独立服务器或具有查询和性能方面的副本集不同的所有注意事项。

# 与副本集的性能比较

开发人员和架构师总是在寻找比较副本集和分片配置性能的方法。

MongoDB 实现分片的方式是基于副本集。生产中的每个分片都应该是一个副本集。性能上的主要区别来自于扇出查询。当我们在没有分片键的情况下进行查询时，MongoDB 的执行时间受到最差表现的副本集的限制。此外，当使用没有分片键的排序时，主服务器必须在整个数据集上实现分布式归并排序。这意味着它必须收集来自不同分片的所有数据，对它们进行归并排序，并将它们作为排序后的数据传递给`mongos`。在这两种情况下，网络延迟和带宽限制可能会减慢操作的速度，而在副本集中则不会出现这种情况。

另一方面，通过拥有三个分片，我们可以将工作集需求分布在不同的节点上，从而从 RAM 中提供结果，而不是访问底层存储，HDD 或 SSD。

另一方面，写操作可以显著加快，因为我们不再受限于单个节点的 I/O 容量，而且我们可以在所有分片中进行写操作。总而言之，在大多数情况下，特别是在使用分片键的情况下，查询和修改操作都将因分片而显著加快。

分片键是分片中最重要的决定，并且应反映和应用于我们最常见的应用程序用例。

# 分片恢复

在本节中，我们将探讨不同的故障类型以及在分片环境中如何进行恢复。在分布式系统中，故障可能以多种形式出现。在本节中，我们将涵盖所有可能的情况，从像`mongos`这样的无状态组件失败到整个分片宕机的最简单情况。

# mongos

`mongos`进程是一个相对轻量级的进程，不保存状态。如果该进程失败，我们只需重新启动它或在不同的服务器上启动一个新进程。建议将`mongos`进程放置在与我们的应用程序相同的服务器上，因此从我们的应用程序使用我们在应用程序服务器中共同放置的一组`mongos`服务器连接是有意义的，以确保`mongos`进程的高可用性。

# mongod

在分片环境中，`mongod`进程失败与在副本集中失败没有区别。如果是一个 secondary，primary 和其他 secondary（假设是三节点副本集）将继续正常运行。

如果是一个`mongod`进程充当 primary，那么选举将开始选举该分片（实际上是一个副本集）中的新 primary。

在这两种情况下，我们应该积极监视并尽快修复节点，因为我们的可用性可能会受到影响。

# 配置服务器

从 MongoDB 3.4 开始，配置服务器也被配置为副本集。配置服务器的故障与常规的`mongod`进程故障没有区别。我们应该监视、记录和修复该进程。

# 一个分片宕机

失去整个分片是非常罕见的，在许多情况下可以归因于网络分区而不是失败的进程。当一个分片宕机时，所有会发送到该分片的操作都将失败。我们可以（而且应该）在应用程序级别实现容错，使我们的应用程序能够恢复完成的操作。

选择一个可以轻松映射到我们操作方面的分片键也可以帮助；例如，如果我们的分片键是基于位置的，我们可能会失去 EU 分片，但仍然能够通过我们的 US 分片写入和读取关于美国客户的数据。

# 整个集群宕机

如果我们失去整个集群，除了尽快恢复运行之外，我们无法做任何其他事情。重要的是要进行监视，并制定适当的流程，以了解如果发生这种情况，需要在何时以及由谁来完成。

当整个集群崩溃时，恢复基本上涉及从备份中恢复并设置新的分片，这很复杂并且需要时间。在测试环境中进行干测试也是明智的选择，另外，通过 MongoDB Ops Manager 或任何其他备份解决方案进行定期备份也是明智的选择。

为了灾难恢复目的，每个分片的副本集成员可能位于不同的位置。

# 进一步阅读

以下资源建议您深入学习分片：

+   《扩展 MongoDB》由 Kristina Chodorow

+   《MongoDB：权威指南》由 Kristina Chodorow 和 Michael Dirolf 编写

+   [`docs.mongodb.com/manual/sharding/`](https://docs.mongodb.com/manual/sharding/)

+   [`www.mongodb.com/blog/post/mongodb-16-released`](https://www.mongodb.com/blog/post/mongodb-16-released)

+   [`github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469`](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)

+   [`www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and`](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)

+   [`plusnconsulting.com/post/mongodb-sharding-and-chunks/`](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)

+   [`github.com/mongodb/mongo/wiki/Sharding-Internals`](https://github.com/mongodb/mongo/wiki/Sharding-Internals)

+   [`learnmongodbthehardway.com/schema/sharding`](http://learnmongodbthehardway.com/schema/sharding)

+   [`learnmongodbthehardway.com/schema/sharding/`](http://learnmongodbthehardway.com/schema/sharding/)

+   [`www.infoq.com/news/2010/08/MongoDB-1.6`](https://www.infoq.com/news/2010/08/MongoDB-1.6)

+   [`www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png`](http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)

# 摘要

在本章中，我们探讨了 MongoDB 最有趣的功能之一，即分片。我们从分片的架构概述开始，然后讨论了如何设计分片并选择正确的分片键。

我们学习了监控、管理和分片带来的限制。我们还学习了`mongos`，MongoDB 的分片路由器，它将我们的查询定向到正确的分片。最后，我们讨论了在 MongoDB 分片环境中从常见故障类型中恢复。

下一章关于容错和高可用性将提供一些有用的技巧和窍门，这些内容在其他 11 章中没有涉及。
